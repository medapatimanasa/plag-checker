{"title": "How to Get the Best Out of Your Fingerprint Database: Hierarchical Fingerprint Indoor Positioning for Databases With Variable Density", "number": "8825770", "authors": "[{'preferredName': 'Qiang Chang', 'normalizedName': 'Q. Chang', 'firstName': 'Qiang', 'lastName': 'Chang', 'searchablePreferredName': 'Qiang Chang', 'id': 37089599264}, {'preferredName': 'Samuel Van De Velde', 'normalizedName': 'S. Van De Velde', 'firstName': 'Samuel', 'lastName': 'Van De Velde', 'searchablePreferredName': 'Samuel Van De Velde', 'id': 38247504700}, {'preferredName': 'Heidi Steendam', 'normalizedName': 'H. Steendam', 'firstName': 'Heidi', 'lastName': 'Steendam', 'searchablePreferredName': 'Heidi Steendam', 'id': 37269092200}]", "abstract": "In this paper, we consider wireless positioning using Received Signal Strength (RSS) fingerprinting. To obtain good accuracy, this technique requires a database containing a high density of up-to-date fingerprints. However, as acquiring fingerprints through training is labor intensive and the indoor topology is subject to changes, a high density fingerprint database cannot always be obtained. On t...", "text": "In this paper we consider wireless positioning using Received Signal Strength RSS fingerprinting To obtain good accuracy this technique requires a database containing a high density of up to date fingerprints However as acquiring fingerprints through training is labor intensive and the indoor topology is subject to changes a high density fingerprint database cannot always be obtained On t The existence of an accurate indoor positioning system is a prerequisite for Location Based Services LBS which are implemented in an increasingly number of applications However no accurate large scale positioning system is available yet due to the lack of infrastructure the high hardware cost or the low accuracy of the solutions One of the most promising low cost solutions is based on RSS fingerprinting In this approach the position is estimated by comparing the Received Signal Strength RSS with fingerprints in a database A fingerprint indoor positioning system consists of two phases In the first off line training phase a database of position fingerprints is constructed by inserting for a number of reference points RPs with the received signal strength RSS from different signal sources such as WiFi access points FM UWB ultra wide band and geomagnetic fields In the second on line positioning phase the RSS values from different sources measured by the user are compared with the data in the database and based on the best match the user s position is determined Because of the widespread availability of different signal sources the deployment cost is low Further the algorithms can easily be implemented in mobile devices and result in a reasonable accuracy Currently some commercial fingerprinting implementations are available such as Google Map Indoor WiFiSlam or Rtmap The main issue in fingerprint positioning is the requirement of a high density up to date database as the accuracy of the fingerprinting technique highly depends on the density and accuracy of the fingerprints in the database Firstly as constructing and maintaining a high density database is labor intensive expensive and sometimes practically impossible because of the complex local environments the available databases show diverse training data densities and sometimes insufficient data for accurate positioning As point by point measurements of the fingerprints is too labor intensive and expensive alternative solutions to construct and maintain a high density database were proposed in the literature The first solution is crowdsourcing In crowdsourcing the user upload their fingerprints to update the database However encouraging the user to upload their data is a challenge The second solution is creating a virtual database using mathematical model such as linear and exponential model MotleyKeenan model Log Distance Path Loss LDPL model Gaussian processes GP model and so on The third way is ray tracing In ray tracing a detailed description of the environment is required to build the fingerprint database The fourth method is simultaneous localization and mapping SLAM In SLAM the users are equipped with a receiver and an IMU and the database is populated on the fly As the developement of deep learning the researchers are trying to present the use of deep neural networks DNN for WiFi fingerprinting The fingerprint database is replaced by a deep neural network However training the deep neural network requires a lot of labeled data The second issue in fingerprint positioning is the time required to search the database which is related to the centralized nature of the fingerprint position algorithms Because of the memory required for a dense database the database is generally stored in a central location server Every time a user wants to estimate its position the user first needs to collect accurate measurements of the RSS and submit the measured values to the location server The server searches the database to find the most likely match from which the user s position is derived As data retrieval from a large database is time consuming the users might not receive their results in real time To cope with the issues in fingerprint positioning we introduce the Hierarchical Positioning Algorithm HPA which is based on the Discrete Level of Detail DLOD algorithm from computer graphics This HPA algorithm however requires the construction of a hierarchical database for the different levels of the algorithm containing different densities of fingerprints In order to have the same accuracy over the whole area the fingerprints in each sub database of the hierarchical database must have a spatial distribution that is as uniform as possible For constructing the hierarchical database from the variable density database we use the Minimum Distance Algorithm MDA in order to select the best RPs and the Local Gaussian Process LGP algorithm to estimate the RSS values at the selected RPs Compared with GP LGP has lower complexity at the cost of a slight deterioration of the accuracy The HPA starts with a coarse estimate at the highest level and gradually improves the accuracy in going to the lowest level In this paper we show that the proposed HPA algorithm with the hierarchical database constructed using LGP and MDA improves the accuracy in areas with sparse training data and reduces the time consumption of the position estimation in dense data areas To the best knowledge of the authors this article is the first presenting the use of DLOD for fingerprint positioning The rest of the paper is organized as follows Section II introduces the hierarchical positioning algorithm and reviews the MDA and LGP algorithms Section III provides the simulation results and the conclusions are given in Section IV In this paper we concentrate on 2D positioning Assume the fingerprint database covers an area of and contains fingerprints from signal sources such as FM WiFi DVB and DTMB During the training phase we measure signal strengths at reference points RPs and store the resulting RSS values together with the coordinates Based on the resulting training database we will estimate a user s position during the on line phase In order to reduce the time for searching the database we propose the Hierarchical Positioning Algorithm HPA This algorithm is based on the Discrete Level of Detail DLOD algorithm used in computer vision to decrease the complexity of representation of a 3D object when it moves away from the viewer Figure 1 illustrates the basic idea of DLOD The basic idea of DLOD Show All In if we are far from the object we just need to render level 1 which contains fewer details When we move closer then we render one of the four tiles in Level 2 that contains more details according to our view Using DLOD is a way of decrease the complexity of a 3D object representation In the HPA algorithm we use a hierarchical database containing different levels In a high level sub database the area is sparsely covered with a low density of fingerprints resulting in a coarse estimate At a low level sub database the data is dense and results in a fine estimate Figure 2 illustrates a hierarchical database with two levels Let us denote the database at level as We assume database contains reference points with coordinates and RSS values where is the number of signal sources within s visibility is the measured signal strength corresponding to signal source and is the uncertainty on the RSS value In order to obtain the wanted accuracy over the whole area the RPs at each level have to be spatially uniformly distributed However the database constructed in the training phase will in practice have variable density such that extracting the hierarchical database from the training database is not straightforward In order to build the hierarchical database we consider the minimum distance algorithm MDA to select the RPs as uniformly as possible over the area and the local Gaussian process LGP algorithm to estimate the RSS values for the selected RPs After the hierarchical database is constructed we propose an altered version of the K weighted nearest neighbors KWNN algorithm to extract the user s position from the hierarchical database A hierarchical database with two levels Show All The hierarchical fingerprint database consists of several sub databases with different densities of fingerprints As stated earlier the fingerprints in each sub database should be selected as uniformly as possible over the area Define as the number of reference points to be selected for the database at level However for general values of it is not straightforward to uniformly distribute the RPs over the area Therefore we propose a low complexity algorithm to select the positions of the RPs the Minimum Distance Algorithm MDA In this algorithm the selection of the positions of the RPs of the sub database at level is based on a virtual sample database which is constructed by placing a square grid in the area with grid size where the positions of the virtual RPs are selected as the corners of the squares in the grid Assuming the room has size the number of virtual positions equals The positions of the RPs for sub database are selected out of the virtual sample database We initialize the algorithm by randomly choosing one virtual position from The other positions are picked from the virtual database based on the measure function where is the coordinate of the candidate position and are the coordinates of the RP positions already present in the sub database The virtual position that minimizes is selected and added to the database Because the measure function is inversely proportional to the Euclidean distances between the candidate RF and the RPs in the database candidate positions that are far from the already selected RP positions are favored while candidate positions near already selected RP positions are filtered out As a result the distances between the RPs will be maximized and the RPs in will be distributed uniformly and expand to the very edges of the area The algorithm is shown in Algorithm 1 the area the distance between neighbor virtual RPs in the number of RPs we want to select select RPs every meters in to build randomly select from while do for all do Calculate 1 end for end while To illustrate MDA we consider an area of and Figure 3 shows the positions of the RPs in when and 40 RPs are selected out of the virtual database Further the figure shows the positions of the RPs when the RPs are selected randomly from As can be observed the proposed algorithm is able to select the RPs spatially uniform over the area Positions of the RPs a MDA b MDA c randomly d randomly Show All After the positions of the RPs in database are selected with MDA the RSS values for these RPs need to be determined To this end we compare the positions of the RPs in with those in the training database Whenever one or more RPs in are within a distance of a RP in we will replace the position of the RP in with the position of the nearest RP in together with its RSS values and the uncertainty on the measured RSS values If no RPs in are within a distance of a RP in the Local Gaussian Process LGP algorithm will be used to estimate the RSS values and their uncertainty in The resulting sub database is determined by three parameters the number of RPs the distance between RPs in the virtual sample database and the radius within which is looked for nearby training RPs The number of RPs is defined by the accuracy that is targeted at level of the hierarchical positioning algorithm The distance determines not only the spatial uniformity of the resulting RPs but also the complexity of the algorithm by reducing the RPs will be placed more uniformly over the area but the complexity of MDA increases as the number of virtual RPs to be searched increases in an inverse proportion to the quadrate of Finally the radius will have an influence on the performance of the HPA When the radius is small the resulting database will have a more uniform placement of the RPs but the probability of finding a nearby training RP decreases such that the RSS of more RPs need to be determined using the LGP algorithm On the other hand when the radius is selected large the resulting database will be less spatially uniform but more training RPs will be present in In the numerical results we will evaluate the effect of the parameters and on the performance of the positioning The Local Gaussian Process LGP algorithm is used to reduce the computational complexity of the Gaussian Process GP algorithm which is used to predict unknown RSS values at positions that are not in the training database In various GP sparsification methods are introduced LGP is proposed from physical point of view while various GP sparsification methods in are introduced from a mathematical point of view In this section we first review the GP algorithm This algorithm starts from the property that RSS values at surrounding positions are correlated Because of this correlation it is possible to describe the RSS at positions where the RSS is not known as function of the RSS at positions where the RSS value is measured The GP algorithm uses the Gaussian kernel to describe this correlation As a result the correlation matrix between the noisy RSS values at positions measured during the training phase can be written as where and is the diagonal matrix of the variances of the measured RSS values Further is the Gaussian kernel function where and are the signal variance and length scale respectively determining the correlation with the RSS values at surrounding positions The parameters and can be estimated using hyper parameter estimation This covariance matrix can be used to predict the RSS value at an arbitrary position The posterior distribution of the RSS value at any position is modeled as a Gaussian random variable i e where and are given by where is the measurement variance and The estimate of the RSS value at position equals and the uncertainty on the estimated RSS is For a large area containing several hundreds of RPs computing the RSS values with equation 4 and 5 are computationally demanding because of the inversion of the large covariance matrix 2 However in an indoor environment we may assume that RPs at a large distance from the position where we want to estimate the RSS value are blocked by several walls and other objects Hence the covariance between the RSS value of those far away RPs and the RSS value at the considered position will be approximately zero As a result it is a reasonable assumption that only training RPs close to the considered position will contribute to the RSS value at considered position The LGP algorithm restricts the training RPs that contribute to the RSS value at position to a training set setting if Assuming the number of RPs in equals the LGP algorithm simplifies equation 4 and 5 by only considering the nearest RPs i e and reduce to a vector and 2 to a matrix Compared to the complexity when all RPs in the training database are used the LGP algorithm has complexity to select the nearest RPs and to invert the reduced size covariance matrix 2 To illustrate the LGP algorithm we consider the RSS radio map of a WiFi access point in an indoor environment The true radio map is created using the WinProp tool from AWE Communications The area is a rectangle containing 18 rooms in the same floor Figure 4 shows the true radio maps including all RPs True radio map created by WinProp Show All Figure 5 shows the radio maps created by GP LGP and LDPL respectively In this simulation we randomly select 80 nodes from the database as training data And use different algorithms to estimate the other nodes RSS values The parameters of the GP are estimated based on the training data considering all RPs In LGP algorithm is set to 4 And the Log Distance Path Model LDPL where the parameters of the LDPL model are estimated based on the training data The uncertainty of RP is defined as follows where and are estimated and true RSS values at RP respectively The uncertainty on the RSS value a radio map using LGP b LGP RSS uncertainty c radio map using GP d GP RSS uncertainty e radio map using LDPL b LDPL RSS uncertainty Show All As can be observed the radio maps for GP and LGP are similar to the true radio map The LDPL model which is known to fail at positions far from the signal source resembles less the true radio map We also compute the average uncertainty over all positions The average uncertainty is defined as GP has the lowest average uncertainty which is about 8 00 followed by LGP with an average of 9 42 The highest average comes from LDPL which is 34 86 For more accurate result we use and to evaluate these three algorithms and are defined as where is the number of virtual RPs refers to the time for calculating all the RPs RSS values in the database And we also define a parameter to indicate the density of the database where is the number of RPs in the database and refers the density of the training database and virtual database respectively In the following simulation we set varies from 0 02 to 1 the training RPs are selected randomly For each we simulate 2000 times with and Fig 6 is the result The of GP and LGP Show All In Fig 6 We can see that GP has a smaller than LGP but the differences between them is slightly small We compare GP LGP and LDPL in RMSE and the time complexity in Fig 7 and Time complexity vary with different a is and b is the time spend in creating the virtual database using different algorithms Show All In Fig 7 a GP performs the best followed by LGP and LDPL performs the worst But the differences between GP and LGP is small In Fig 7 b LDPL has the lowest time complexity followed by LGP and GP In summary LGP keeps a good balance between RMSE and time complexity Using the two previous algorithms we are able to generate the hierarchical database required for our HPA positioning algorithm With this hierarchical database we will successively estimate the position at the different levels starting at the highest level At each level we use the weighted nearest neighbors WKNN algorithm which searches in the database for the RPs with RSS values closest to the user and takes a weighted sum of the positions of these nearest RPs to estimate the position However as the uncertainty on the RSS value of some RPs in the database can be high the accuracy of the standard WKNN algorithm can be low In order to improve the accuracy we change the algorithm to take into account the uncertainty on the RSS values in the database Let us assume we measure the RSS values times Assume the user measures the RSS values during the measurement where is the number of signal sources The signal distance to RP in the database at level is defined as where is the RSS value at RP for signal source and is the uncertainty on that RSS value This distance measure favors more reliable RPs having a small and will disfavor unreliable RPs with high uncertainty Taking into account that in general the uncertainty of a training RP will be lower than that of a virtual RP the training data is more likely to be selected with this altered algorithm improving the accuracy After the selection of the nearest neighbors the WKNN algorithm estimates the coordinates of the user by weighting the coordinates of the selected RPs where are the coordinates of RP and is the normalized weight The variance is defined as follows If we want more accurate result we can continue our estimation using lower level sub database which contains more RPs For reducing the time for searching in database to find the most likely match RPs We can use the previous result to reduce the searching space In the previous estimation we calculate the location based on nearest RPs in We put these RPs in set Based on we can estimate the user s most possible located area denoted by shows how the area is created where is set to 3 Creating based on Show All The area is the user s most likely located place We select all the node in from for fine estimation using the algorithm illustrated in this section In this experiments we apply the WiFi fingerprint to test our algorithm We first compare our proposed virtual database creation algorithm LGP with GP and the widely used Log Distance Path Loss model We also evaluate our positioning algorithm with both high and low density training database Our test environment consists of 18 room in one floor with the area of And 8 APs are placed in this area The floor plan of the corridor and the position of the APs are shown in Floor plan of the office corridor and the position of the APs Show All We build a WiFi fingerprint radio map for our environment by means of 3D ray tracing We use WinProp from AWE communications to create the signal fingerprint database The database contains 3318 RPs denoted as We consider the radio map as the ground truth In the following simulation we only use 800 random distributed RPs for training The others are used for testing For evaluating different algorithms we define and is defined as where are the true and estimated coordinates respectively is the number of positioning instance refers to the mean time for locating one single RP There are some parameters in HPA including in MDA in WKNN and in LGP All of these parameters determines the complexity and positioning accuracy of HPA We first evaluate these parameters to get the optimized values determines the density of Different result in different as a result the positioning accuracy might be affected In this simulation we test different We apply the LGP for estimating the virtual RPs RSS values The stardand WKNN algorithm is applied for positioning The other parameters are set as follows and are estimated using hyper parameter estimation We build different based on different for positioning Results comes from 20000 positioning instance is the positioning result from different CDF and Mean error vary with different The training curve means using the training database for positioning and random means using 80 randomly selected RPs for positioning Show All In this simulation is set to 0 25 0 5 0 75 1 1 25 1 5 1 75 2 2 25 2 5 2 75 3 and 3 25 respectively We also use the training database containing 800 training RPs and randomly selected 80 RPs for positioning Fig 10 a only contains the result when is set to 0 25 1 25 2 25 and 3 25 In Fig 10 positioning using the training database performs the best while the random database worst The difference between variant is slightly small In Fig 10 b has almost no effect on the performance But building the database with different cost different time Fig 11 gives the time for constructing the virtual database with different Time for building the virtual database Show All Fig 11 shows that the time decrease when increase The result from Fig 10 and Fig 11 tell us that we can use as large as possible to reduce the time for constructing the database determines the distance that we can use the training data directly A big might break the distribution of the RPs but will introduce more reliable RSS values In this section we will evaluate the performance with different Similar with the previous setting we apply the LGP for estimating the virtual RPs RSS values and the traditional WKNN algorithm for positioning The other parameters are set as follows and are estimated using hyper parameter estimation We build with different based on for positioning We first look at the percentage of training data in A large result in more training data be used Fig 12 shows the result Results comes from 20000 positioning instance percentage of training data in varies with Show All The result in Fig 12 shows what we expected The percentage increase as the grows of More training data introduces more reliable data but the breaks the distribution of the RPs in We explore the positioning performance with different Fig 13 gives the result CDF and Mean error vary with different The training curve means using the training database for positioning and random means using 80 randomly selected RPs for positioning Show All In this simulation is set to 0 0 1 0 15 0 2 0 25 0 3 0 35 0 4 0 45 0 5 0 55 0 6 0 65 0 7 0 75 0 8 0 85 0 9 0 95 and 1 respectively We also use the training database containing 800 training RPs and a randomly selected 80 RPs for positioning Fig 13 a only contains the result when is set to 0 0 25 0 5 0 75 and 1 In Fig 13 positioning using the training database performs the best different doesn t result in significant difference in performance When which means all the RPs are training nodes the performance is not the best When which means the least training RPs are included the performance is not the worst Fig 13 b shows that the best result is achieved when the is set to a middle value is the number of training RPs used for estimating the RSS values for a given virtual nodes in LGP A large introduces more training data and more accurate result obtained But the time for estimating the RSS values will be increased In this section we explore to find a good balance between the accuracy and time for building the virtual database Similar with the previous setting we apply the LGP for estimating the virtual RPs RSS values and the traditional WKNN algorithm for positioning The other parameters are set as follows and are estimated using hyper parameter estimation We build based on with different for positioning Results comes from 20000 positioning instance Fig 14 shows the result CDF and Mean error vary with different The training curve means using the training database for positioning and random means using 80 randomly selected RPs for positioning Show All In this simulation is set from 2 to 15 Positioning using training database and randomly selected database are included In a we can see that different CDF curve looks similar to each other When we find that the positioning accuracy is not as good as others Fig 14 b proves our observation The reason is that using a small means fewer training data used for estimating the virtual RP s RSS values As a result the positioning result is not as good as using more training data But a large does not leads to better performance This result proves assumption 1 We needn t to using all the training data for estimation is the number of nodes used for positioning In this simulation we want to find the best for estimation We apply the traditional WKNN algorithm for positioning based on the training database We set increase from 1 to 10 Results comes from 20000 positioning instance Fig 15 shows the result CDF and Mean error vary with different Show All The result from Fig 15 a shows that a smaller or larger is not good choice Fig 15 b tells us that using 3 nodes for positioning performs the best The difference between standard WKNN and improved WKNN algorithm is the distance definition In this simulation we want to evaluate the improved WKNN algorithm both in sparse and dense database We create two virtual databases using MDA and LGP based on 40 randomly selected RPs One is sparse virtual database containing 40 virtual points denoted as while the other one contains 400 virtual points denoted as In MDA algorithm In LGP algorithm In standard and improved WKNN algorithm Fig 16 shows the CDF and Mean Error for these two algorithms using different databases Evaluating improved WKNN using different database a is using a database containing 40 virtual RPs b is using a database containing 400 virtual RPs Show All From Fig 16 we can see that the improved algorithm perform better when we use the sparse database In sparse database the average distance from the testing node to the nearby RPs is large than that in dense nodes But the number of training RPs is limited and we set As a result we have more possibility to select the more reliable data in sparse database When we use the sparse database for positioning the mean error for the standard algorithm is 2 2479m while the improved algorithm is 2 1471m The positioning accuracy has been improved by the proposed algorithm When we use the dense database the positioning accuracy drops down as illustrated in b Training and updating a low density of training database is much easier than a dense one Here we evaluate HPA using the low density of training database In the Simulation we set increases from 0 01 8 RPs in area to 0 1 80 RPs in area and the RPs in are selected randomly from In HPA we create the database with only two levels using GP LGP and LDPL respectivily In LGP In the standard WKNN we use for positioning And we set both in standard and improved WKNN For each we simulate 800 times In each simulation we test 2000 positioning instances In each positioning instances we use to generate signal strength measurement by adding Gaussian noise We compare their RMSE which defined in Equation 16 and average positioning time for one node gives the simulation results RMSE of WKNN and HPA using sparse training database Show All In there are four curves WKNN means using the standard WKNN algorithm and the database is the training database The average RMSE is 4 59 HPA GP HPA LGP and HPA LDPL mean using the HPA algorithm for positioning and the virtual databases are built using GP LGP and LDPL algorithm respectively As we can observed from this Figure HPA LDPL performs the worst The average RMSE is 7 05 HPA GP and HPA LGP perform nearly the same the average RMSE is 3 49 and 3 48 respectively Compared with standard WKNN the proposed algorithm improves the RMSE for about 24 2 Fig 18 gives the time for locating one node Time of WKNN and HPA using sparse training database Show All In Fig 18 we find that WKNN cost least time this is because there are less RPs for positioning HPA LDPL cost the most time this is because in the first stage the possible region is very large As a result in the second stage the algorithm has to search in a large database HPA GP and HPA LGP cost nearly the same When the training database s density is about 0 1 WKNN HPA GP and HPA LGP cost the same time But in Fig 17 we can see that HPA GP and HPA LDPL have less RMSE And in Fig 7 b we see that GP cost far more time for building the virtual database These result means the proposed algorithm performs the best when the training database is sparse Assuming we have a dense training database For comparing WKNN and HPA we assumes varies from 0 1 to 1 The RPs in the training database are selected randomly from All the other parameters are set the same as the previous section gives the simulation results RMSE of WKNN and HPA using dense training database Show All In all the four curves mean the same with Fig 17 but here we use more dense training database As illustrated in Fig 19 HPA LDPL performs the worst the average RMSE is 4 68 Followed by the standard WKNN algorithm while the average RMSE is 3 22 HPA GP and HPA LGP performs nearly the same the average RMSE is 2 874 and 2 873 respectively The performance has been improved for about 10 8 Fig 20 is the time for locating one node Time of WKNN and HPA using dense training database Show All In Fig 20 the time for WKNN increase with this is because the algorithm has to search in a increasing scale database But the time for HPA LDPL decrease with this is because more training RPs are introduced as the increase of where Fig 12 gives the result HPA GP and HPA LGP cost the same But Fig 7 b tells us that GP cost far more time for building the virtual database than LGP These result also means the proposed algorithm performs the best when the training database is dense In this subsection we prove that the proposed algorithm performs better both using sparse and dense training database Wireless fingerprint technique has the advantage of low deployment cost supply for reasonable accuracy and easily to be applied to mobile devices As a result fingerprinting has attracted a lot of attentions The positioning accuracy of fingerprint technique is highly dependent on the density of RPs in the fingerprint database However constructing a fingerprint database with high density fingerprint samples is labor intensive or impossible in some cases And even if it was possible to get a high density of fingerprint database data retrieve from the large database is time consuming and the database has to be updated as the environment changes For these problems in fingerprinting we introduce the DLOD Discrete Level of Detail from computer graphics to the fingerprint localization problem to propose HPA This algorithm can be applied to the both high and low density of training database In HPA we first propose MDA to transform the different density of training database into uniformly distributed databases which contain different density of sub databases And then we propose LGP to estimate the RSS values for all the virtual RPs The higher level built with low density of fingerprints gives coarse estimation while the lower level contains dense data to provide fine estimation There are two advantages in HPA Firstly the time for positioning is reduced This is because in HPA we only search the matched reference points in a small database instead of a large database Secondly the positioning accuracy is increased This is because LGP will supply more reference points for positioning Simulation results show that when we have a dense training database the positioning accuracy can be improved for about 10 8 And when the training database contains low density fingerprints the positioning accurate can be improved for about 24 2 The results also imply us that we can select the RPs for training database based on MDA This algorithm can be applied to both high and low density training database need no more specialized hardware and can reuse the existing Wi Fi infrastructure and fingerprint database implying it s robust for daily used Our future work is concentrate on building a larger experiment environment to test our algorithm and more levels will be tested"}
{"title": "Analysis of the Effects Power-Inversion (PI) Adaptive Algorithm Have on GNSS Received Pseudorange Measurement", "number": "8896038", "authors": "[{'preferredName': 'Xin Yang', 'normalizedName': 'X. Yang', 'firstName': 'Xin', 'lastName': 'Yang', 'searchablePreferredName': 'Xin Yang', 'id': 37087242057}, {'preferredName': 'Wenxiang Liu', 'normalizedName': 'W. Liu', 'firstName': 'Wenxiang', 'lastName': 'Liu', 'searchablePreferredName': 'Wenxiang Liu', 'id': 37085791131}, {'preferredName': 'Feiqiang Chen', 'normalizedName': 'F. Chen', 'firstName': 'Feiqiang', 'lastName': 'Chen', 'searchablePreferredName': 'Feiqiang Chen', 'id': 37085616406}, {'preferredName': 'Zukun Lu', 'normalizedName': 'Z. Lu', 'firstName': 'Zukun', 'lastName': 'Lu', 'searchablePreferredName': 'Zukun Lu', 'id': 37086024873}, {'preferredName': 'Feixue Wang', 'normalizedName': 'F. Wang', 'firstName': 'Feixue', 'lastName': 'Wang', 'searchablePreferredName': 'Feixue Wang', 'id': 37066613500}]", "abstract": "Power inversion algorithms (PI) are widely used by ant-jamming receivers in harsh environment, but it also induces pseudo-range measurement error which is fatal to high precision receivers. In this paper, it proposes a more accurate method based on the S Curve Bias (SCB) to evaluate the pseudo-range bias which can actually affect the positioning results, as it excludes a fixed time delay contained...", "text": "Power inversion algorithms PI are widely used by ant jamming receivers in harsh environment but it also induces pseudo range measurement error which is fatal to high precision receivers In this paper it proposes a more accurate method based on the S Curve Bias SCB to evaluate the pseudo range bias which can actually affect the positioning results as it excludes a fixed time delay contained Global Navigation Satellite System GNSS is now widely used in almost all infrastructure livelihood facilities for navigation positioning and timing But weakness of signal makes receivers susceptible to external interference especially radio frequency interference RFI signal which has been universally accepted as a real threat to GNSS applications Therefore countermeasures must to be adopted to ensure the robustness and reliability of navigation system Among all alternative solutions of anti jammer devices equipped with the adaptive antenna architecture are considered most effective particularly for the high grade receivers with high demands of reliability and robustness For now many classical space time adaptive processing STAP algorithms have been extensively applied to antenna arrays for instance the least mean square LMS adaptive filter algorithm Widrow et al Maximum Signal to Interference plus Noise Ratio MSINR adaptive filter algorithm Applebaum et al Power inversion PI adaptive filter algorithm and et cetera Instead of requiring an estimation to the direction of arrival DOA PI has simple structure and broad application scenario so it is applied frequently The principle of PI is to minimize the output power for antenna array PI adaptive algorithm could be regarded as a space filter which does not provide gain at the DOA of the jammer to eliminate the signal with stronger power unconditionally Nonetheless when the direction of jammer is adjacent to the direction of desired signal PI has a potential to reduce the signal to noise ratio SNR of the signal as well which indicates that PI may not be effective to do positioning in these conditions PI could be split into different ranges depending on the domain involved the Spatial Only Processing Power Inversion SOP PI as well as Space Time Adaptive Processing Power Inversion STAP PI Compared with SOP PI STAP PI has been validated to have a better performance in mitigating interference effects However as revealed in STAP PI algorithm will cause distortion of cross correlation function CCF and time delay Especially in the specific reasons why STAP PI causes distortion and delay of correlation function are given in detail and the experimental results in this article prove its views as well It is worth noting that distortion of CCF and time delay are possible to give rise to the pseudo range measurement bias As described in this bias has impact on the final output positioning results of some precise applications such as precision point positioning and real time kinematics RTK In respect to SOP PI has demonstrated that SOP PI is free from bias in nature as it only weights in amplitude and phase In addition it has also been confirmed in that SOP PI makes a flat frequency response within the signal bandwidth which prevents the CCF of the ranging codes from distortion However in our research it is discovered that the conclusion drawn in may not be sufficiently accurate In order to explain the problem more clearly we establish the signal models at the beginning and by analyzing the models we find out that both SOP PI and STAP PI can bring errors and many factors particularly the type of interference signal can have impacts on the biases Both of views above are rarely mentioned by previous articles According to the analysis in this paper we advocate both SOP PI and STAP PI can induce pseudo range measurement errors to receivers and besides this discovery in order to assess the pseudo range errors more accurately we propose a new method based on SCB to evaluate the errors Generally when calculating the pseudo range error the pseudo range of the output signal of antenna is directly compared with pseudo range of the reference signal But the accuracy of the result can be influenced by many factors for example noise disturbance and the sampling rate Especially some precise positioning systems require the pseudo range bias of 8 cms which places high demand on the sampling rate when analyzing the problem Moreover it is discovered that not all error calculated by differing can affect the positioning results so a new method is proposed to evaluate the bias which can really affect the final positioning And this method is used in the simulation and experiment to verify consistency of theory and reality The remaining sections of this paper are structured as follows Firstly Section II establishes models to represent the signal received by adaptive antenna as well as the output signal Then a method is suggested in Section III to perform the calculation of the induced bias that can actually affect the positioning results Meanwhile the impact factors on pseudo range bias are analyzed respectively Secondly simulation and experiment based on a 4 element antenna are carried out in Section IV to verify theoretical analysis in Section III Finally the conclusion is given in the final section During the first processing stage of receiver the received signal makes cross correlation with locally generated reference signal in order to make prediction of the code delay of the signal relative to the receiver s local clock Considering a signal enters the receiver as where stands for the desired signal while and represent interference signal and noise respectively Therefore the received signal of antenna can be expressed alternatively as follows where represents transpose And there also has denote the signal and jammer data received by the th element respectively and refers to noise Besides represent the time delay of desired signal and jammer of the th element comparing to the reference element Among them is the three dimensional coordiantes of th element which is decided by the space between elements the number of elements contained in antenna and also the distribution of antenna and refer to the azimuth angle and pitch angle of desired signal and jammer of the th element respectively And the output signal of array is is the weights of the anti jamming algorithm in this paper it represents the weights of PI is conjugate transposition So the output desired signal could be expressed as The major cause of giving rise to the bias by PI is that comparing to cross correlation function of with generated by receiver which is the pseudorandom spreading code cross correlation function of with leads to distortion and delay The CCF can be expressed as As desired signal interference signal and noise are irrelevant with each other and the power of desired signal is significantly weaker than interference signal there has represent CCF performed by the received signal jammer and noise respectively The purpose of Power inversion adaptive filter algorithm is to reduce the output power of the array to the minimum PI does not have gain at DOA of interference signal which consequently mitigates interference Its weights are calculated by In reference to 15 denotes the weights The PI algorithm can be divided into SOP PI and STAP PI depending on whether PI uses time domain to filter which means when there is SOP PI the weight vector are scalars and when there is STAP PI are vectors represents time taps After the array antenna obtains the weights according to the PI algorithm it synthesizes the multiplex signals into the output signal by using the weights And in this process pseudo range biases could be brought into the output signal thereby affecting the tracking and positioning results As showed in figure 1 Relation between PI and pseudo range biases affecting positioning results Show All The following sections will analyze the pseudorange errors introduced by SOP PI and STAP PI in detail and describe a method to evaluate such biases SOP PI is limited to space domain according to 14 the CCF of received signal is mainly composed of the CCF of the jammer and the noise The CCF of jammer is And also CCF of noise is represent the power of received jammer and noise respectively On the basis of 16 and 17 it can calculate the CCF of the received signal and then using 15 to get weights The output desired signal of antenna can be shown as And the CCF of with which is used to evaluate the pseudo range bias can be expressed as As revealed in the equation above the distortion of the CCF after SOP PI could be viewed as multipath effects It undermined the symmetry of the CCF Besides the distortion of CCF is possible to reduce the precision of pseudorange due to the delay lock loop DLL as it works for the symmetrical CCF accurately As is the function of and also associated with the relevant characteristic of the jammer the distortion of CCF could be impacted by the direction of desired signal and interference signal the power of desired and interference signal as well as the number and distribution of the elements In some studies it is believed that SOP PI is incapable of impacting on the symmetry of the cross correlation This is premised on the assumption that the signal is narrowband signal As indicated in the frequency of GNSS such as B3I is 1268 520M and its bandwidth is no excess of 20M for which it could be treated as a narrowband signal In this case the received signal could be presented as is the steering vector of desired signal is the reference desired signal Therefore based on equation 12 it has As and are vectors with is a complex number It is assumed that So It is clear that when compared with the reference element the peak of is found to be at the same point of So ideally SOP PI is unlikely to result in code delay bias However as 20 is premised on the assumption of narrowband signal and from this hypothesis it can not be clear whether it changes the symmetry of CCF after SOP PI so it is insufficient to explain the problem accurately Therefore the expression showed by equation 18 is relatively precise and faithful to the reality In this paper the study is based on equation 18 and believes that SOP PI can lead to bias of pseudorange measurement STAP PI contains two domains for which it is assumed that the time tap is then represents the number of elements denotes the sample period So the CCF after STAP PI is Alternatively the equation above can be regarded as taking through a FIR filter in some cases It is uncovered that the symmetry of is undermined substantially and it causes time delay to CCF But in this paper it is found that not all time delay can bring errors to the final positioning result so it proposes a method to estimate the bias really affect the positioning result the details are showed as follows Through the Section above it can be known that both SOP PI and STAP PI can bring errors to the pseudo range measurement and the error can be affected by many factors So to measure the error more accurately an evaluating method is proposed below In order to make assessment of the CCF distortion caused by SOP PI and STAP PI SCB S Curve Bias is brought into this paper as it can effectively represent code phase bias caused by CCF distortion or delay The S curve can be expressed as The S curve is a phase detection curve and it is the result of the early code correlation value minus the late code correlation value The zero crossing offset of S curve is the SCB which is a common indicator for measuring the navigation measurement error The definition of CCF used to appraise the bias can be represented in detail as is the correlation integration time and it usually is one code period The zero crossing point of S curve satisfies that Combined with 26 27 28 it can calculate the zero crossing point of the S curve And the difference between the zero crossing point of the output desired signal and the reference signal can express the pseudo range bias More details about S curve bias is showed by Figure 2 represents interval of correlation SCB schematic diagram Show All So the can be indicated as When using SOP PI there has as it does not have time tap in 29 all caused by the distrotion of the CCF and can affect the positioning So it can make the assessment of pseudo range bias induced by SOP PI There has But not all SCB of STAP PI can have effects on the positioning result By transforming the output signal expressed by 24 into the frequency domain it can be derived as Among this In the above is rational Thus And Therefore from this equation It can be seen that using STAP PI can lead to fixed time delay As this kind of delay is only connected to the time tap and has nothing to do with the direction of the signal etc this part of bias will be attributed to the clock difference Based on the discussion above to assess the bias which actually affects the positioning results caused by STAP PI there establishes a new evaluation method which can be expressed as In order to illustrate the problem more clealy an array antenna with four elements forming as a square matrix is used in this paper as an example which is also used frequently in real environment The coordinate of these four elements are listed as follows represents the half of wavelength is RF frequency is the speed of light It is assumed that array antenna received desired signal could be defined as indicates the data received by th element in period In order to verify the analysis in the previous part there has a simulation The simulation generates a B3 signal of Beidou navigation system at first It is modulated by BPSK Binary Phase Shift Keying Then noise and a jammer are added to the signal At last it is received by the antenna modeled in the simulation Subsequently the direction of desired signal and interference signal and the power of interference signal to do SOP PI and STAP PI are changed The results are presented in Figures 3 and 4 with the 15dB signal to noise ratio SNR and 62MHz sample frequency The time tap of STAP PI is Code delay bias after SOP PI Show All Code delay bias after STAP PI Show All In the following parts the code delay biases are counted as showed by Figure 3 and 4 with pitch angle ranging from 10 degree to 90 degree and azimuth angle varying from 1 degree to 360 degree The simulation has three kinds of conditions involved The first one is 1 absence of interference signal The second one is 2 that the azimuth and pitch angle of jammer are at 300 5 300 is azimuth angle and 5 is pitch angle with jammer to noise ratio JNR being 30 dB The third one is that 3 the azimuth and pitch angle of jammer are at 121 22 with jammer to noise ratio being 75 dB Figure 3 reveals the code delay biases after SOP PI which are largely attributed to SCB Figure a b c are results under condition 1 2 3 respectively Figure 3 a confirms that the distortion would cause biases to pseudorange at certain directions by using SOP PI even in the environment without jammer Besides when compared to b c in the condition with jammer this sort of bias is insignificant Furthermore the presence of jammer also adds the difficulty to the receiver to keep track of the signal as shown in b c where the bright yellow parts within the red circle in figures indicates that when the desired signal is near the direction of jammer the signal after PI can not be stable tracked by receivers Additionally according to when the direction of desired signal is close to the direction of jammer anti jammer algorithm SOP PI and STAP PI can serve to reduce the SNR which means that in this case PI not only reduces the power of jammer but also the desired signal So it can be hard for the receivers to keep track of signal In the condition 1 the code delay bias is below 1m for most directions of which the maximum is 1 0091m The minimum is about zero which is And the percentage of stable tracking is 1 in accordance with Table 1 Also owning to the presence of jammer for example when the JNR is 30 dB the percentage of stable tracking is lowered to approximately 0 97 the maximum of the bias is about 1 00m expect the directions from which the signal can not be tracked and the minimum is also around zero which is In case of changing the direction of desired signal the bias changes on a random basis And also as shown in Figures b c the different directions of jammer lead to different bias results As indicated in Figure 4 It shows the code delay bias after using STAP PI The conditions shown in Figure 4 are identical to the conditions in Figure 3 condition 1 2 3 respectively Based on the results a conclusion can be drawn that in the direction close to the jammer the desired signal could also be unlikely to be tracked accurately by receivers The code delay biases resulting from STAP PI are below 10m except the DOA from which signal can not be tracked When the power of jammer increases the part of bias around 10m becomes larger and the part that can not be stably tracked has also increased Comparing with the stable tracking percentage of SOP PI the percentage of STAP PI is larger which means that signal after STAP PI is more likely to be tracked in the same interference scenario From the results below it also can be drawn out that when the direction of desired signal is changed the bias is changed as well The results obtained above leads to a conclusion that using SOP PI and STAP PI both can induce bias to pseudorange measurement and affect the positioning results Although using STAP PI signal is more likely to be tracked by receiver the bias which can finally affect the positioning result is also bigger than using SOP PI And in the region where the directions of signal and jammer are close the biases become bigger As analyzed in Section III above the relevant characteristic of the jammer can affect the code measurement bias which means that the type of jammer can affect the code measurement this is a point that was previously ignored so we do the following simulation to proves this point In this simulation it uses four kinds of jammer They are single frequency jammer wideband Gaussian jammer narrowband Gaussian jammer and satellite signal jammer which is transmitting a satellite signal at the same frequency of the desired signal with different PRN pseudo random noise code And the result is showed as Figure 5 Code delay bias after STAP PI Show All These two figures show the pseu dorange bias to the receiver with different jammers After SOP PI the biases introduced by the four types of interference are about the same and the bias line which is introduced by satellite interference has greater fluctuation than the other three types of interference indicating that at some time the bias caused by satellite signal jammer is even bigger comparing with other three kinds of jammers In the case of STAP PI when the interference signal power is relatively small the pseu dorange biases caused by these four types of interference are mostly consistent but when the interference power is relatively large the four curves have a certain degree of separation That means the biases caused by the different kinds of interference are different Especially in the case of single frequency interference after the JSR is greater than 20 dB the error value has a significant decrease compared with the other three kinds of interferences and the maximum difference value is about 0 7 m The common method which is also the most frequently used method to evaluate the pseudo range errors previously is comparing the code phase of and from DLL Delay Lock Loop which also means differencing the pseudo range directly It can be showed as expresses the code phase of the output signal and is the code phase of the desired signal received by the reference element For analyzing the performance of the method proposed in this paper it compares the estimation results in the same conditions one result is evaluated by the method proposed in this paper and the other is directly evaluated by differing the pseudo range using equation 38 The JNR is 75 dB DOA of interference signal is 121 22 other conditions in this Section are as same as the Section above The result can be showed by table 2 From the results it reveals that the supposed method has better performance when directly differing the pseudo range of the output signal and reference signal the evaluation results are larger than the proposed method This is because that when directly comparing the pseudo range the result will be affected by the noise disturbance on the tracking loop On the other hand the accuracy of directly comparison result is also influenced by the sampling rate The larger the sampling rate the more accurate the result will be and it also will be closer to the estimation result of the proposed method Furthermore for STAP PI directly comparison result contains a fixed delay so in this case the result is somewhat larger than the result of the proposed evaluation result To make assessment of the performance of precise pseudorange in the practical interference environment a simulated reality interference environment is created in this paper by using an antenna like figure 6 Initially it applied NSS8000 navigation signal source to generate the GNSS signal an array signal source fabricated by our search center and an array antenna to create a simulated reality environment the power of signal is about 120dBm and antenna is a four elements with half wavelength interval array antenna A multi channel signal record playback system the sample rate is 62MHz is used to collect data after analog to digital conversion In this experiment the direction of desired signal is at 181 24 and the jammer signal is at 121 22 In the formal environment without jammer every channel collected data from every element Subsequently in the environment with jammer wideband interference at frequency of B3I signal which is transmitted by Beidou system with power of 30dBm is transmitted Meanwhile the same procession is implemented to collect data in the identical normal environment without interference Antenna schematic diagram patch antenna with red circle receive B3I signal Show All To reveal the detail about how PI could have impacts on the performance of precise pseudorange it applied the collected data to conduct PI filtering acquisition tracking in computer For a more comprehensive analysis it comes out a comparison of the CCF between the environment without interference and with interference using SOP PI STAP PI the tap of time is 5 or none PI it means just processing the data output by the reference array element The schematic diagram of whole experiment process is now displayed in figure 7 Diagram of experiment process Show All Figure 8 demonstrates the CCF in the environment without interference and with interference As explained in a the peak of none PI and SOP PI is at one point However this figure also shows the distortion of CCF which could affect the precision of pseudo range by DLL forward Figure 8 b reveals the peak of STAP PI that moves backwards about 5 samples which could result in code delay bias In addition it confirms that the CCF exhibits more visible distortion These two sides could both give rise to bias to pseudo range The result is discovered to be in line with the analysis in the method Section forward The cross correlation function Show All Under the circumstance without jammer figure 9 shows the bias after SOP PI and STAP PI The SOP PI bias is about 0m while only in some samples the bias is up to 10m which may be caused by the non ideal characteristics of the channel and the mutual coupling characteristics of the array elements Regarding STAP PI it causes more significant bias than SOP PI that could reach up to 20m in some samples which mostly is about 9m In the simulation above the bias at this direction was found to be about 5 8m which means the bias in experiment result is similar with the simulation result The reason why bias from experiment is larger than in simulation at some points is attributed to the non ideal characteristics of the channel and the mutual coupling when data is collected in laboratory as the channel is ideal in the simulation The code delay bias in the environment without interference Show All Figure 10 presents the results obtained from the environment without interference signal and with interference signal it shows as the difference between the results of STAP PI and SOP PI in normal environment and interference environment because in the interference environment there does not have satellites that can be tracked without PI As indicated by the result the bottom of this two lines are as same as each other but in the interference environment it appears more glitch than it without interference The code delay bias in the environment with interference Show All When compared with the result in ideal conditions as discussed previously the results in this Section serve to evidence that the adaptive anti jammer algorithm could cause distortion of the cross relation function Moreover the bias in the real scenario using anti jammer antenna is more significant than in the simulated ideal environment This paper proposes a method to assess the pseudo range bias comparing to the method which directly appraises the bias by differing the pseudo range the method proposed in this paper seems more accurate to evaluate the pseudo range bias that can actually affect the positioning as the result of the proposed method does not contain a fixed time delay and the accuracy of the proposed method is not related to the noise disturbance and the sampling rate This paper also uses the proposed method to estimate the pseudo range bias which can actually affect the positioning in the simulation and the laboratory experiment The results demonstrate that SOP PI and STAP PI could both cause code delay bias It has been also confirmed that all the parameters including the direction of desired signal direction of interference signal the power and the type of interference signal could make impacts on the bias The pseudo range measurement error caused by the SOP PI does not exceed 1 m and the pseudo range measurement error caused by STAP PI is close to 10 m at some directions It is indicated that bias caused by STAP PI is more significant than it caused by SOP PI Furthermore the bias in the real scenario is also more notable than that in the ideal environment which is largely attributed to the non ideal characteristics of the channel The conclusions drawn above are shown to be coherent with the theoretical analysis conducted in the past"}
{"title": "The Dispersion Reduction Frequency Upconversion System at 1550 nm With Tightly Focused Beam", "number": "8968354", "authors": "[{'preferredName': 'Yuqi Jiang', 'normalizedName': 'Y. Jiang', 'firstName': 'Yuqi', 'lastName': 'Jiang', 'searchablePreferredName': 'Yuqi Jiang', 'id': 37089305718}, {'preferredName': 'Fang Dai', 'normalizedName': 'F. Dai', 'firstName': 'Fang', 'lastName': 'Dai', 'searchablePreferredName': 'Fang Dai', 'id': 37089307454}, {'preferredName': 'Guohua Gu', 'normalizedName': 'G. Gu', 'firstName': 'Guohua', 'lastName': 'Gu', 'searchablePreferredName': 'Guohua Gu', 'id': 37085571062}, {'preferredName': 'Weiji He', 'normalizedName': 'W. He', 'firstName': 'Weiji', 'lastName': 'He', 'searchablePreferredName': 'Weiji He', 'id': 37085807230}]", "abstract": "The upconversion detection is a promising method to detect infrared radiation. It is proved that the focused beam makes a contribution to improving the upconversion efficiency. A near-infrared detector at room-temperature based on sum frequency generation(SFG) with focused beams is demonstrated. To enhance the upconversion efficiency, the signal and pump beams are tightly focused by the same focal...", "text": "The upconversion detection is a promising method to detect infrared radiation It is proved that the focused beam makes a contribution to improving the upconversion efficiency A near infrared detector at room temperature based on sum frequency generation SFG with focused beams is demonstrated To enhance the upconversion efficiency the signal and pump beams are tightly focused by the same focal Nowadays the infrared IR detection technology plays an important role in remote sensing With the advantage of eye safety the laser draws much attention Especially due to the highest maximum permissible exposure in the wavelength ranging from to and suffering lower atmospheric attenuation the laser is a promising wavelength in active remote sensing such as LiDAR Normally the signal can be detected by InGaAs detectors directly Unfortunately the application of commercial InGaAs detectors are limited by the dark noise and detection efficiency In contrast the visible near infrared detectors such as Si based detector perform better on overcoming the dark noise at Currently the frequency upconversion has became an alternative to infrared detection technology According to the noise in the frequency upconversion process can be reduce to photon level Especially the thermal noise of upconversion process can be neglected at To avoid the dark noise in InGaAs detectors it is available to use a Si based detector detect signal indirectly after the signal is converted by the frequency upconversion system The frequency upconversion can be described as a process that the IR signal mixed with a pump field is transferred into corresponding visible near IR signal inside a material The upconverted signal is detected by visible near infrared detectors In the past decade year there are many researches about frequency upconversion Taking advantage of high peak power of pulse laser the upconversion efficiency of 64 has been achieved with the average pump power of at within the pulse duration of Benefited from the quasi phasematch QPM technology and the period poled nonlinear crystal such as PPLN the upconversion efficiency in single photon counting system can reach approximately 90 With the application of the waveguide the PPLN bulk can be replaced by the PPLN waveguide because these devices have a higher upconversion efficiency Several researches shows the upconversion efficiency of the PPLN waveguide is beyond 50 Unfortunately a drawback of PPLN waveguides is more intense parasitic second harmonic generation SHG of the pump wavelength can t be separated easily Normally the signal beam is focused but the pump beam is kept collimated in an upconversion system Then a dichroic mirror DM is applied to combine the signal beam and the pump beam before two beams incident the nonlinear crystal Besides it is proved that the signal beam and the pump beam are focused before two beams combined by a dichroic mirror is more effective Unfortunately it is hard to apply tightly focusing to enhance the upconversion efficiency because the dichroic mirror is placed between the lens and the nonlinear crystal To our knowledge the focal length of the lens that is used to focus the signal beam is more than in terms of existing upconversion systems Inspired by a polarization maintaining wavelength division multiplexer connected with a collimator is applied to combine the signal pulse and the pump continue wave beam instead of a dichroic mirror in our system which contributes to ensuring the signal pulse and the pump beam are collinear However with the light transmitting in the fiber the power of pump light is limited To overcome that the upconversion efficiency suffers the limited pump power we use a lens with short focal length focus the mixed beam into the PPLN To optimize our system the relation between the focused beam and the upconversion efficiency is investigated A classic theory with neglecting the dispersion is proposed by Boyd and Kleinman and the theory really contributes to optimizing upconversion systems Because the phase mismatch and the interaction between the signal beam and the pump beam in PPLN are not independent on the dispersion it is still necessary to investigate the influence of dispersion on the relation between the focused beams and the upconversion efficiency With the help of proposed model the negative influences of the dispersion is reduced and the result shows the upconversion efficiency can reaches with the pump power In this section the experimental setup is introduced and the schematic drawing is given in Fig 1 To mix the signal pulse with the pump beam a pulse laser and a continue wave laser are connected with a polarization maintaining wavelength division multiplexer The mixed beam is output by a collimator connected with the polarization maintaining wavelength division multiplexer and it has a divergence 0 075 with the diameter of Then the collimator and two lenses L1 L2 are installed on a cage system to ensure they are coaxial We use the lens L1 to focus the mixed beam and the lens L2 to collimate the converted light The parameter of L2 is the same as L1 The size of L1 and L2 is A 5 MgO doped PPLN bulk is placed in a temperature controlled device and located on the center of the L1 and L2 The size of PPLN bulk is long width and high The poling period is One side of the PPLN bulk is coated with an antireflection AR film for high transmission at and Another side is coated with an antireflection AR film for high transmission at The temperature of PPLN is kept at A long pass filter at prevents the noise from background Finally the generated upconverted light is detected by a PMT Schematic view of the experimental setup PMWDM polarization maintaining wavelength division multiplexer ID iris diaphragm L1 and L2 lens PPLN periodically poled lithium niobate LF long pass filter PMT photomultiplier tube with infrared extended multialkali photocathode Show All To investigate the relation of the mixed beam size and the influence of dispersion an iris diaphragm is inserted between the collimator and L1 to control the radius of the mixed beam output from the collimator The radius of aperture ranges from to The continue wave laser is the source of pump beam and The pulse laser is the signal source The pulse duration is and the repetition rate is In terms of the PMT the spectral response of PMT is the rise time is the sensitivity is and the gain is set to With the effective area diameter of the PMT is all the converted light can be detected In our system the IR signal is converted to near infrared via sum frequency generation SFG In SFG the energy of the pump photon with frequency is transferred to the signal photon of frequency in the PPLN bulk Because of the conservation of energy in the process the energy of converted photon comes from the signal photon and the pump photon which means the converted photon frequency equals The wave vectors of signal photons pump photons and converted photons are defined as and separately Because of the conservation of momentum the wave vector of the converted signal is shown as Figure 2 which can be expressed as where stands for the contribution of the nonlinear medium with a quasi phase matching periodicity of and can be written as stands for so called phase mismatch to describe the upconversion efficiency To analyze the converted signal it is necessary to consider the minimum phase mismatch in our system According to 2 it is known that since only varies with the temperature the direction of and depends on the and From we can deduce that and are a group of collinear vectors with opposite direction In our system with the signal beam and the pump beam focused into PPLN there is a possibility that the signal photon interacts with a pump photon while corresponding and are not on the same plane Using spherical coordinates the phase mismatch for any depends on Wave vector diagram of the SFG Assuming all the vectors is on a plane the signal vector and the pump vector don t have to be collinear Show All In frequency upconversion producing an output signal efficiently need a certain phase matching condition which indicates the is smaller than On this condition the is calculated as where is the refraction index of wavelength in nonlinear crystals and is the light speed in vacuum  we can deduce that all the vectors are on the same plane is one of the condition that reaches the minimum When all the vectors are on a plane can be simplified to Equation 5 shows a smaller makes a contribution to a certain phase matching condition Especially is another condition that makes reach the minimum Reference to the is given by where is the intensity of is the interaction length of input waves of frequency and in the PPLN is the effective nonlinear coefficient of the PPLN is the vacuum permeability Here the upconversion efficiency depends on when the pump power and the nonlinear crystal are set up Consequently the total upconversion efficiency can be written as a function about and To enhance the upconversion efficiency with limited pump power the tightly focusing is applied Hence it is necessary to investigate the relation between the focused beams and the upconversion efficiency A simplified model is proposed to describe it Especially the focal length shift caused by the dispersion of lens L1 should not be neglected in this model The model is based on the assumption that the beam size of both the signal and the pump can be approximated to and the signal beam is focused into the center of nonlinear medium With spherical coordinates the total power of converted light can be expressed by a double integral over the incident angles The is where stands for the upconversion efficiency corresponding to a pair of wave vectors and and can be written to a function is the maximum incident angle and is the intensity of incident light in the nonlinear crystal According to 7 the is In 8 depends on 6 With the upconversion process occurring when the phase mismatch is the minimum we can deduce from 5 In ideal conditions As a result 5 can be described as 9 shows the expression exists when is minimum Considering the focal length shift caused by the dispersion of the lens L1 the mixed beam consisting of the signal and pump beams separates in PPLN As a result the phase mismatch depends on the propagation of two beams According to the coordinate shown as Figure 3 is the central symmetry on the plane and it is independent on and The propagation of the signal beam can be described as where is the distance from the lens L1 to the front face of PPLN bulk Especially is the given focal length of lens L1 and is the focal length shift of the lens L1 at relative to stands for the signal beam size in the PPLN with varying from to Then the maximum incident angle at is Once is determined the propagation of the pump beam can be expressed as where is the distance from the front face of the PPLN bulk to the minimum size of the pump beam in the PPLN is the focal length shift of the lens L1 at relative to Similar to stands for the pump beam size in the PPLN with varying from to To reduce the phase mismatch can be rewritten as The origin of coordinates is located on the center of PPLN front face The focused beam propagates alone the z axis Show All Since the focal length shift leads that does not equal for every it is necessary to use a coefficient to describe the interaction between the signal beam and the pump beam When both the signal beam and the pump beam enter the nonlinear crystal depends on the area of the signal beam overlapping the pump beam and can be expressed as limits the power of the signal beam and the pump beam in upconversion process What s more and vary with the propagation distance in the nonlinear crystal is an integer alone axis from Then 7 is rewritten as According to 5 the phase match becomes independent on 12 shows is relevant to can be simplified to Additional the power mainly concentrates on the facula center nearby and the converted light is generated in the area So the corresponding beam size is smaller than the sport sizes On this condition the area within beam size can be approximated to average distribution Neglecting the loss in propagation the intensity of the signal beam and the pump beam are which is irrelevant to The integral reduces to In 15 depends on 5 and 12 According to the definition of upconversion efficiency the total upconversion efficiency is Here stands for the available power of the pump beam for upconversion process It is essential to remind that 16 is based on conditions as follows Firstly the signal beam is focused into the center of the nonlinear medium Secondly the size of the signal beam and the pump beam on the nonlinear crystal front face are not larger than the size of nonlinear crystal which means where is the thickness of the nonlinear crystal At the end all the converted light should be recorded by the detector With the dispersion existing the focal length of lens L1 shifts according to the wavelength To describe the difference of the focal length between the signal beam and the pump beam the focal length shift of lens L1 is defined as In proposed model influences the size of both the signal beam and the pump beam in the nonlinear crystal The interaction of two beams depends on in some degree In terms of the size of two beams because it is hard to measure the beam size in the PPLN bulk a simulation is used to analyze on instead of measuring directly To show the influence of the dispersion another group of lenses are used to display when the dispersion becomes serious The simulation was finished in ZEMAX and the result is shown as fig 4 The beam size in PPLN the solid line and broken line are theory beam size calculated by proposed model at and separately The cross and square are simulation in ZEMAX at and separately a A group of achromatic lenses group A are chose as L1 and L2 The focal length shift of lens1 between and is and The focal length shift of lens2 between and is and The focal length shift of lens3 between and is and The focal length shift of lens4 between and is and b A group of bi convex lenses group B are chose as L1 and L2 The focal length shift of lens1 between and is and The focal length shift of lens2 between and is and The focal length shift of lens3 between and is and The focal length shift of lens4 between and is and Show All Fig 4 shows that the beam size that is calculated by the proposed model is agreement with the simulation It is noticeable that with the focal length of lens L1 increasing the also increases but the decreases According to 16 for any position the available pump power makes a contribution to the upconversion efficiency in length equally without considering phase match Here stands for the available power of the signal beam and the pump beam in the upconversion process is defined to show the total influence of the dispersion in the PPLN where is defined to describe the dispersion In fig 5a is easy to be influenced by the dispersion The focal length shift rising leads decreasing for the same focal length Especially decreases obviously when makes on condition that the focal length is small enough In fig 5b with increasing 34 42 25 27 31 2 and 35 18 decreases 8 89 7 66 8 47 and 6 9 corresponding to and which shows that the loose focusing helps reduce the influence of dispersion when incident beams are focused by lens of group A and group B Show All On the other hand the phase match plays an important role in upconversion According to 5 and 16 the phase match depends on the incident angle but the existing of the dispersion makes the phase match varies alone To minimize phase mismatch the and should be collinear which means the incident angle equals zero and the maximum value of the phase match term remain unchanged Correspondingly the minimum value of the phase match term appears when the incident angle is maximum The Fig 6 shows the phase match term with two groups of lenses The phase match term when incident beams are focused by lens from group A and group B separately a L1 and L2 is chose from group A b L1 and L2 is chose from group B c The positive difference between maximum phase match term and minimum phase match term Show All Fig 6 shows the phase mismatch when the mixed beam is focused by lens with the same but different In fig 6a and fig 6b What should be explained is the color bar index stands for It seems that the color bar index does not change due to the change is too small to be shown in this figure Here is defined to describe this change and it is shown in fig 6c The point in fig 6c stands for of each subfigure correspondingly in fig 6a and fig 6b In fig 6c is about 10 11 level while the color bar index is about 10 4 level Because the is included by the integral shown in 16 it should not be approximated to a constant According to fig 6a and fig 6b although paraxial ray is hard to be influenced with the incident angle rising the phase mismatch increasing Moreover a large incident angle increases phase mismatch easily To evaluate the phase mismatch with different focal length the maximum and the minimum values of phase match term are shown in fig 6c Fig 6c shows that with increasing 34 42 25 27 31 2 and 35 18 the maximum decreases 9 48 8 95 6 83 and 9 corresponding to and Comparing and in fig 6c although when is less than when when is larger than when which means the tightly focusing leads to more phase mismatch and the loosely focusing helps reduce the influence of the dispersion Combining the effect of the dispersion on and phase match term we can draw a conclusion that although the tightly focusing is beneficial for enhance the intensity of incident beam and improve the upconversion efficiency it is affected by the dispersion easily On the contrast it is useful to restrain the influence of the dispersion that the signal beam and the pump beam are focused by lens with long focal length An important aspect of an upconversion system is the upconversion efficiency Here to show the influence of the dispersion on the upconversion efficiency two groups of lenses group A and group B inferred from fig 4 are applied in the experiment The the result are shown as fig 7 The upconversion efficiency of theoretical values and experimental data when L1 is chose from group A and group B Show All In fig 7 the focal length shift of lenses in group B are larger than group A but the loss of upconversion with loosely focusing is less than tightly focusing Comparing the data of two groups with increasing 34 42 25 27 31 2 and 35 18 the decreases 24 62 17 68 13 96 and 11 46 corresponding to and in the experiment Here are two reasons for the result Firstly the dispersion makes the signal beam and the pump beam separate and decreases the interaction area of two beams which limits the available power of the signal beam and the pump beam in the upconversion process especially when Secondly with and larger than the smaller angle between and makes a contribution to reducing phase mismatch Although the method that both the signal beam and the pump beam are focused into the PPLN can reduce the angle between and the tightly focusing increases the incident angle When and are not too large to make the phase mismatch can be approximated to a constant However on the one hand the tightly focusing makes that the incident angle and are larger On the other hand it increases the angle which makes the loss of upconversion efficiency caused by the tightly focusing is more serious Furthermore as the tightly focusing structure has an effect on the incident angle of the signal beam and the pump beam becoming large it is necessary to take the beam size into consideration The upconversion efficiency with different beam size is measured and the experimental data are shown as fig 8 The upconversion efficiency of theoretical values and experimental data with different beam size The broken lines stands for the function of upconversion efficiency about focal length when the focal length shift remain unchanged The square is the theoretical values according to the parameters of lens and the cross is the experimental data The simulation and measurement condition are and correspondingly Show All In terms of fig 8 it is necessary to remind that because the beam size is limited by the iris diaphragm the power of mixed beam depends on the beam size In fig 8 with the beam size increasing from fig 8a to fig 8d the tightly focusing does not always contributes to improving upconversion efficiency It is noted that because the incident beam focused down to the diffraction limited spot sizes due to the divergence of the collimator in 10 and 11 increases and the intensity of beams decreases As a result the experimental data is less than the theory What s more with the dispersion also existing in the collimator the experimental data deviates the theory more seriously even thought the tend of both are the same Especially the tightly focusing means the is smaller than the with the loosely focusing When is smaller the difference is more obvious However the tend of curves in fig 8 still shows that the tightly focusing and the incident beams with a large angle is easy to be influenced by the dispersion With the help of proposed model the setup is designed and shown as fig 1 To measure the upconversion efficiency of our system the iris diaphragm is removed Because the mixed beam that is output by a collimator connected with PMWDM has a divergence within 0 075 with a diameter of our system reaches maximum upconversion efficiency on condition that L1 and L2 are achromatic lenses with and according to fig 8d To avoid the damage of the PMWDM the pump power is limited Measured by the thermal power sensors the maximum available pump power is We record data on condition that the output of 1550nm laser is attenuated and the peak power of the signal pulse is at 10KHz with pulse duration of The result is shown as fig 9 and the maximum upconversion efficiency reaches The upconversion efficiency curve of our system The red points are the experimental data and the blue curve is the fitting curve Show All In this paper a dispersion reduction frequency upconversion setup with a tightly focusing is presented The system transfers light at to and records the converted signal by a multialkali photocathode PMT With the help of the polarization selective of single mode polarization maintaining fiber the noise is reduced To enhance the upconversion efficiency with low pump power the tightly focusing is applied Benefited from the tightly focusing by a lens with the focal length of the signal is converted by a pump beam with only With the help of proposed model it is revealed that the relation between the focused beam and the upconversion efficiency which contributes to reduce the influence of the dispersion The model shows although the tightly focusing makes a contribution to upconversion efficiency the dispersion has more negative influence on upconversion efficiency when the incident beams are tightly focused extremely Moreover the experiment shows the incident beam size increasing makes the influence of dispersion more serious With the help of the model the best focal length can be selected to improve the tightly focused upconversion system and our system to reach the maximum upconversion efficiency with the pump power Compared to the existing upconversion system whose upconversion efficiency can reach 10 1 level the upconversion efficiency of our system can only reach 10 4 level mainly due to the limited of the pump power Normally the pump power is more than dozens of watt level while the pump power is only in our system To avoid combining the signal beam and the pump beam without keep both beams collinear accurately we have to use a PMWDM to avoid that there is a angle between both beams However due to the fiber of the PMWDM cannot suffer the input power more than the output of pump source should be less than Besides considering the loss of fiber coupling between the pump laser and the PMWDM is 40 the pump power is less than Fortunately with the limited pump power the back conversion is reduced To improve our system a dichroic mirror can be used to replace PMWDM to combine the signal beam and the pump beam on condition that both beams is kept collinear accurately In this way the higher pump power can be set without considering the damage threshold of the PMWDM In the end the proposed model can make a contribution to reducing the influence of the dispersion in tightly focusing upconversion systems by choosing a reasonable focal length and beam size"}
{"title": "Spatial Enhanced Rotation Aware Network for Breast Mass Segmentation in Digital Mammogram", "number": "9023481", "authors": "[{'preferredName': 'Yulin Cheng', 'normalizedName': 'Y. Cheng', 'firstName': 'Yulin', 'lastName': 'Cheng', 'searchablePreferredName': 'Yulin Cheng', 'id': 37089524834}, {'preferredName': 'Ying Gao', 'normalizedName': 'Y. Gao', 'firstName': 'Ying', 'lastName': 'Gao', 'searchablePreferredName': 'Ying Gao', 'id': 37086476091}, {'preferredName': 'Linsen Xie', 'normalizedName': 'L. Xie', 'firstName': 'Linsen', 'lastName': 'Xie', 'searchablePreferredName': 'Linsen Xie', 'id': 37088649114}, {'preferredName': 'Xinyan Xie', 'normalizedName': 'X. Xie', 'firstName': 'Xinyan', 'lastName': 'Xie', 'searchablePreferredName': 'Xinyan Xie', 'id': 37089524474}, {'preferredName': 'Wengen Lin', 'normalizedName': 'W. Lin', 'firstName': 'Wengen', 'lastName': 'Lin', 'searchablePreferredName': 'Wengen Lin', 'id': 37089524446}]", "abstract": "Breast cancer is the most common cancer with highest mortality risk among the female worldwide and breast mass is the most effective sign for cancer identification. Thus, accurate segmentation of breast mass is regarded as a key step to reduce the death rate. Traditional segmentation methods require prior knowledge and manually set parameters, while recent studies prefer to construct neural networ...", "text": "Breast cancer is the most common cancer with highest mortality risk among the female worldwide and breast mass is the most effective sign for cancer identification Thus accurate segmentation of breast mass is regarded as a key step to reduce the death rate Traditional segmentation methods require prior knowledge and manually set parameters while recent studies prefer to construct neural networ Breast cancer is one of the most harmful diseases among the female worldwide According to 2018 Global Cancer Statistics breast cancer sufferers account for a quarter of all female cancer patients Even more shocking about 3 of early stage patients die from it Therefore early diagnosis is highly suggested for reducing death rate of breast cancer As is known to all breast mass is one of the most effective signs for cancer identification Thus breast mass segmentation on medical image is regarded as the first step of early diagnosis and the key step prior to classification of benign and malignant Traditional approaches for breast mass segmentation are manually time consuming and heavily dependent on radiologist s experience To reduce processing time and improve the accuracy of segmentation result computer aided detection CADe technology has been rapidly developed since the late 1980s and digital mammogram is the most reliable technique which widely used in breast mass segmentation However breast masses are varied in a wide range in shape size and texture which make the segmentation remain a challenging task Various machine learning algorithms were utilized to establish traditional CADe systems for disease detection and segmentation especially for breast mass segmentation Region growing and thresholding were the two most widely used methods With respect to region growing Mencattini et al introduced an effective region growing algorithm for breast mass segmentation on Digital Database for Screening Mammography DDSM dataset They followed the typical processing flow which consists of artifacts removal contrast enhancement and segmentation Moreover an iterative post processing step was designed to remove peninsulas on the boundary of mass A precision of 93 38 and a recall of 88 34 were achieved on a subset of DDSM with 200 images Regarding to thresholding technique Kom et al reported promising results for detecting mass using a local adaptive thresholding method combined with linear transformation enhancement The adaptive threshold value was calculated based on two context windows of distinct sizes Their method achieved great performance on a dataset of 61 mammograms images and got a sensitive of 95 91 Besides energy function based methods and clustering based methods were also adapted as the detectors To integrate the advantages of different techniques some studies combined several techniques in a processing flow and achieved great improvement in breast mass segmentation Min et al used multi scale morphological filtering and simple linear iterative clustering segmentation to detect suspicious areas of different sizes Multiple cascaded random forests were employed to make the final decision based on the features extracted from suspicious areas Two sensitives of 94 and 77 were achieved by their approach for INbreast and DDSM BCRP respectively Chakraborty et al presented a multilevel method controlled by thresholding and region growing in a coarse to fine manner At each step a thresholding method was used to find the focal region of a mass and then a region growing algorithm was employed to detect the accurate mass using gradient and intensity information With a mixed dataset of 107 images from the mini MIAS database and 158 digital radiography images from a local database two sensitives of 95 and 98 8 were achieved by their approach respectively Sharma et al combined watershed segmentation and k means clustering to detect breast mass The texture features extracted from suspicious areas were used to identify the true breast mass Although these methods achieved good performances they were relatively simple and sensitive to some initialization parameters such as seed threshold value and number of cluster centroids In most cases the parameters or the initialization strategies were decided by experience Otherwise these methods highly rely on image enhancement and lack the abilities to extract required pattern from raw image data directly This made them may ineffective in complex scenes and large scale dataset In recent years convolutional neural network CNN has developed rapidly and has become the most commonly selected deep learning structure in image processing domain CNN can extract features by abundant kernels automatically and fuse them in a non linear manner All the parameters of CNN can be initialized randomly adjusted by back propagation and trained in an end to end manner Based on these characteristics CNN has achieved great improvement in most computer vision tasks such as image classification object detection and semantic segmentation Regarding to semantic segmentation using traditional CNN the input image is divided into small patches using sliding window technique and the label of the central pixel of each patch is predicted by CNN However small image patch only contains local context information about the patch itself and losses global context information about the whole image Because of these the performance of traditional CNN is limited To address these problems Long et al proposed fully convolutional networks FCN for segmentation FCN replaces all the fully connected layers by convolution layers For this reason the input of FCN is allowed in arbitrarily size As the size of segmentation result should be the same as the input FCN upsamples the feature maps from high level layers and introduces skip connection to fuse the outputs from different layers The experimental results showed that the spatial information supplied by skip connection and gradually upsampling can improve the performance of FCN However the segmentation details are still far from expectation Inspired by FCN many successful neural networks were proposed for image segmentation task such as U Net DeepLab and PSPNet Among these works the encoder decoder structure of U Net is the easiest to implement and extend The encoder part of U Net employs multi stage convolutions connected by maxpooling layers to extract features and expand receptive fields while the decoder part contains multi stage convolutions connected by upsampling layers to fuse the features and expand the size Skip connection is applied in every decoder stage to concatenate the feature map from upsampling layers with the feature maps from the corresponding stage in encoder Benefit from small stride upsampling to expand the size gradually and skip connection to make up for spatial information U Net achieves great improvement in image segmentation Afterwards various U Net based architectures have been proposed to handle different application scenes especially in breast mass segmentation Li et al proposed an Attention Dense U net for breast mass segmentation on a subset of DDSM They used dense connected block to remould the encoder and applied Attention Gates at skip connection step for spatial information enhancement An Area under the Receiver Operating Characteristic Curve AUC of 0 8605 was reported Hai et al remoulded both the encoder and decoder by dense connected block Moreover they held the opinion that the multi scale context was useful in segmentation task Thus they used atrous spatial pyramid pooling ASPP mechanism for multi scale context captation Their method achieved the best performance on a self collected dataset with 380 mammograms in total Different from the approaches mentioned above Sun et al proposed an attention guided dense upsampling block to decrease the information loss in upsampling operation The original upsampling operation was replaced by a dense upsampling block and a channel wise attention was equipped for information enhancement An average Dice coefficient of 81 8 was achieved on CBIS DDSM dataset To improve the encoder part of U Net most of the studies mentioned above held the opinion that feature reuse which can be modeled by residual connection is the most important factor Unlike them we think that the feature extraction can be further improved by spatial attention Spatial attention which is mentioned in can adjust the focus to the region of interest and filter out interference information Thus it is possible to improve the feature extraction by explicitly modeling the spatial attention For this concern spatial attention block SA Block with a mask attention branch is designed in this paper to provide soft spatial attention about suspicious regions Moreover breast masses can display in every orientations in complex scenes which can be summarized as rotation Most studies employed data augment method to improve the perception to mass rotation Different from them we design multi stream rotation aware block MSRA Block which contains two extra asymmetric convolutions in parallel to enhance the rotation invariance and refine the prediction in decoder part As a summary of the foregoing a U type Spatial Enhanced Rotation Aware Network SERAN is designed for breast mass segmentation in digital mammogram in this paper The proposed SERAN consists of a residual spatial attention encoder and a multi stream rotation aware decoder The encoder contains five stages and each stage is made up of a residual convolution block and a SA Block Maxpooling layer is applied at the end of each encoder stage to expand the receptive field The decoder is made up of four stages and each stage consists of two MSRA Blocks To expand the size deconvolution layer is used after each decoder stage Moreover skip connection is employed in each decoder stage for spatial information supplement To train a segmentation neural network binary cross entropy loss or Dice loss is the most common choice However both of them are not efficient for model optimization due to data imbalance and the implicitly constraint of background area respectively To address this problem a novel regulation item named Inside outside Loss IOL is used to maximize the probabilities inside the mass and minimize the probabilities outside the mass The rest of this paper is organized as follows In Section II we describe the details about dataset and pre processing as well as the architecture of proposed SERAN To verify the ability of proposed method extensive experiments are summarized in Section III Finally a conclusion is drawn in Section IV This paper aims to build a Spatial Enhanced Rotation Aware Network SERAN for breast mass segmentation in digital mammogram The overview of SERAN is illustrated as The materials used in this study and the details of SERAN are described in following sections The architecture of Spatial Enhanced Rotation Aware Network SERAN Show All DDSM database from the University of Florida is used in this study Approximately 2 500 cases are collected in DDSM and each of them contains two X ray mammograms about each breast The database can be downloaded online and all the image sizes are larger than pixels The location and type of suspicious area in each image has been annotated by experienced radiologists as ground truth The images are encoded in LJPEG format and the corresponding ground truths are saved in OVERLAY format To transform the images and the ground truths to PNG format which can be easily used during programing a public tool named DDSM LJPEG Converter is used In this study 400 representative images are selected from DDSM by an experience radiologist The standard of the selection can be described as following 1 the mass is relative clear in the image and 2 the annotated area can cover the mass but not much bigger than the mass The images are randomly divided into three nonoverlapping parts in a ratio of 4 1 1 which are used as training set validation set and test set Each image in DDSM is composed of breast region and background However some artifacts exist in the background These artifacts can be regarded as noise and affect the ability of neural network badly Thus we design several steps to remove the artifacts in the background for denoising Firstly we employ gamma transformation to correct image and enhance the details about the breast region Secondly the image is binarized by OTSU Then the maximum connected area in the binary image is obtained as the breast region With these the pixel value of the breast region is recorded as 1 and the background is recorded as 0 Finally the original image is multiplied by the breast region binary image to get the unlabeled image The results are shown as Fig 2 a is the original image with artifacts and noises in the background b is the result image after artifact removal Show All After that all the images are resized to based on bilinear interpolation for reducing the computation cost and all the pixel values in an image are scaled into a range from 0 to 255 by Min Max normalization The sizes of masses are varied in a wide range and the distribution in each data subset is similar to the others as shown in Fig 3 The distributions of mass size in each data subset Show All For training a deep neural network for segmentation a large number of annotated images are required to avoid overfitting To enlarge the training set several data augmentation methods are applied Firstly each image is mirrored in horizontal direction Then all the images are rotated according to following principles 1 take the central point of the image as the origin of coordinate system 2 generate an angle in a range from 10 to 10 degree randomly 3 rotate the image according to the origin point and the degree 4 fill the extra area by 0 to form a rectangle image and 5 crop the image to according to origin point With these the size of training set is quadruple and the data diversity is increased The proposed SERAN consists of four parts a residual spatial attention encoder a multi stream rotation aware decoder skip connections and a final prediction layer The descriptions of all the modules are summarized in following subsections As shown in Fig 3 the shapes and sizes of different breast masses are varied in a wide range Thus it is required to build a powerful encoder for effective feature extraction Typical encoder is made up of convolution layers for feature extraction and pooling layers for receptive field expanding All the layers are formed in a series connection style and the input of layer is only the output of layer 1 However the ability of encoder may degrade with gradient vanishing To address this problem He et al proposed ResNet with residual connection Using residual connection the input of convolution layer is summed with the convolution result to form the output which can be expressed as where and are inputs of layer and respectively is a learning function and is a mapping function When the number of channels of is equal to the channel number of is an identity mapping function shown as Fig 4 a Otherwise convolution is used in to adjust the channel number shown as Fig 4 b refers to the ReLu function for activation Two variants of residual connected block a basic residual block b residual block with channel number adjustment The numbers in the boxes are kernel size filter number stride and padding respectively Show All Residual connection can avoid gradient vanishing and provide feature reuse for better convergence For these reasons we employ the residual learning structure as a basic module in encoder part In addition in our view spatial information is the most important factor to boost the performance in segmentation task Thus it will be helpful to model the attention on spatial information explicitly Inspired by a spatial attention block SA Block is designed to enhance the spatial context information at each pixel position As shown in Fig 5 the SA Block consists of a feature extraction branch and a mask attention branch The feature extraction branch utilizes two convolution layers to capture features from complex image context The mask attention branch is implemented as a U Net liked structure A single channel attention map is generated by the mask attention branch Each value in the attention map represents the weight of the corresponding pixel position in spatial dimension Because the values in the attention map range from 0 to 1 the output of the feature extraction branch is used twice to avoid the degradation of feature values in the deep layers The output of the feature extraction branch is weighted by the attention map firstly and then summed with the weighted result The summed result is then activated by ReLu The formulation of SA Block can be summarized as where and are pixel locations in width height and channel dimension respectively and are the learning functions of mask attention branch and feature extraction branch respectively is the ReLu activation function and Batch Normalization BN is equipped after each convolution operation The structure of SA Block Show All As well known the low level layers in a deep CNN model capture shallow features such as shape size and texture while the high level layers contain more abstract semantic information From this point of view the low level layers need stronger spatial attention Thus two similar mask attention branches with different depths are proposed which are shown in Fig 6 The deeper mask attention branch is equipped in the shallow layers of the encoder and the shallower one is embedded into the deeper layers Structures of Mask Attention Branches in SA Block of different depths Show All Composed of residual convolution blocks and SA Blocks the encoder can focus on extracting features from suspicious places as shown in Fig 7 Moreover the responses of regions with similar features to the target in one image can be restrained gradually The architecture of encoder is shown as the left part of Fig 1 The top row shows the comparative prediction result c of original image a compared to ground truth b The bottom row shows the attention maps from each encoder stages Show All In complex scenes the masses can display in every orientations which can be summarized as rotation In order to assist doctors for precise diagnosis the neural network must be aware to mass rotation and predict robust results in different rotations The goal of each decoder stage is to fuse the features and refine the prediction of mass location and contour details Thus it is possible to boost the rotation robustness of the neural network by building a rotation aware decoder According to different asymmetric convolutions are robust to different rotation scenes As shown in Fig 8 kernel is much more robust to horizontal flipping than kernel Comparison between kernel red and kernel yellow The kernel obtains the same results on horizontal flipped input but kernel gets different result Show All In order to extract more effective features and boost the rotation robustness we design a multi stream rotation aware block MSRA Block to fuse features with different receptive fields as shown in Fig 9 The MSRA Block contains a convolution and two asymmetric convolutions whose kernels are and respectively The three convolutions are used to get different purposes The convolution extracts features in a relatively larger receptive field The and convolutions are used to boost the rotation robustness to vertical and horizontal flipping respectively Moreover the width of the decoder is expanded resulting in multi scale feature fusion The feature maps from the three streams are added up as a fusion result BN and ReLu are used to activate the result in a nonlinear manner The formulation of MSRA Block can be described as where and are inputs of layer and respectively and are the learning functions of the three feature extraction streams with and kernels respectively is the ReLu function used for activation The structure of MSRA Block Show All The decoder consists of four stages and each stage is composed of two MSRA Blocks shown as the right part of Fig 1 The input of each stage is the concatenate feature maps which is formed by the corresponding skip connection and the deconvolution result of previous stage As a common view the detail information lost in pooling upsampling structure can be supplied through skip connection Thus skip connection is used to connect each encoder stage with corresponding decoder stage The output of each SA Block is treated as a part of the input of corresponding MSRA Block through skip connection The output of the last MSRA Block is used for final prediction A convolution is employed to fuse the features reduce the number of channels and keep the spatial size Two feature maps are generated and a channel wise softmax operation is used to transform the pixel values to probabilities One of the feature maps is regarded as background and another one predicts the mass To train a segmentation network binary cross entropy BCE loss was the most common choice due to its smooth derivative and the stable optimization procedure The formulation of BCE can be denoted as where is the cross entropy loss of one pixel is the predicted label and denotes the actual label BCE is the average cross entropy loss of all the pixels It is obvious that BCE regards segmentation task as pixel wise classification task and treats each pixel equally However the number of pixels representing breast mass is only a small proportion of the entire image In other words there exists heavy data imbalance when applying BCE as the objective function To address the problem Dice loss was presented to measure the similarity between the predicted area and the ground truth which can be denoted as where is the predicted probability map and is the ground truth is used to calculate the summation of pixel values represents the intersection area of prediction and ground truth which is named True Positive TP Dice loss offers a hidden linked constraint about the intersection area within mass region and the area outside mass region which is indicated by The change of TP is the main factor affecting Dice loss In our point of view explicitly constrain the two parts can be more efficient for model optimization Thus a novel regulation item named Inside outside Loss IOL is designed to explicitly constrain both the intersection area and the area outside the mass for better optimization The goal of IOL is to maximize the probabilities inside the ground truth area and minimize the outside probabilities The formulation of IOL is shown as following where is the predicted probability map and is the ground truth and are the height and width of image respectively is used to calculate the summation of pixel values represents the intersection area of prediction and ground truth which is named True Positive TP And represents the misclassification in background area which is named False Positive FP The InsideLoss indicates the probability loss inside the ground truth mass while the OutsideLoss indicates the loss outside the mass In other words the Insideloss focus on increasing TP indictor and the Outsideloss aims to reduce the FP indictor To avoid overfitting we treat IOL as a regulation item and set a small weight for IOL The final objective function is composed by Dice loss and IOL which is defined as following where is the weight of IOL Using IOL the network can avoid misclassification in background area and make a better prediction as shown in Fig 10 Segmentation results produced by SERAN trained by different loss function Show All To estimate the performance of proposed approach five commonly used evaluation metrics in image segmentation task are adopted namely Sensitive Specificity Accuracy Intersection Over Union IOU and Dice coefficient All of the metrics are denoted as following where TP is the number of pixels correctly predicted in mass region and TN is the number of correct predicted pixels in background FP are the pixels predicted wrong as mass and FN are the wrong pixels as background The confusion matrix is showed in Table 1 to make a clear explanation about TP TN FP and FN Moreover an average value MEAN of the results of 5 independent experiments is recorded as the final result to preclude statistical error and result bias Furthermore standard deviation STD is computed to reflect performance stability Adam method is employed as an optimizer to optimize the model The parameters of Adam are set as and The learning rate is initialized to 0 001 and halved every 20 epochs after the 70th epoch The batch size is set to 16 and the weight of IOL is set to 0 01 in training phase Several early stopping strategies are used to avoid overfitting If the loss of validation set keeps increasing in 5 consecutive epochs or changes within a small range in 10 consecutive epochs the training procedure will stop and the model with the lowest validation loss will be saved All the preprocessing steps and data augment methods are implemented on Python interface of OpenCV library The proposed neural network is implemented on Python interface of Pytorch platform while related experimentations are tested using Intel R Core TM i7 6850K 3 60GHz CPU and two GeForce GTX 1080p GPUs on Ubuntu16 04 1 The versions of Python OpenCV and Pytorch are 3 6 6 3 4 3 and 1 0 1 respectively In this section the investigation of each proposed module and strategy is performed to analysis the contributions And the comprehensive comparison with 5 state of the art segmentation methods is presented at the end of this Section to show the advancement of proposed SERAN U Net is implemented as a baseline model All the results are achieved in test set to show the generalization ability of each methods Since the values of Specificity and Accuracy are close we record them in more detail To analyze the contribution of residual spatial attention encoder a spatial enhanced network SEN is constructed after replacing the encoder of U Net by proposed encoder The performances of SEN and U Net are showed in the top 2 rows in Table 2 for evaluation In addition both SEN and U Net are trained by Dice loss In summary SEN wins 9 out of 10 comparisons and most of the metrics are improved significantly Due to the explicitly modeling of spatial attention SEN can focus more on the region of interest and make the feature extraction more effective As shown in Fig 7 the number of focuses is obviously decreased and the response values in mass region are relatively bigger in the attention map of deeper layer That means the network owns the ability to identify the mass from multiple regions with high response values Moreover the performance of SEN is more stable than U Net which is indicated by STDs As shown in the line 2 and line 3 of Table 2 the performances of SERAN and SEN are showed to investigate the effect of multi stream rotation aware decoder mentioned in Section II C 2 SERAN transforms the decoder part of SEN by the proposed one For the sake of fairness Dice loss is used to train SERAN SERAN wins all the competitions about average performance Due to the multi stream structure the width of the decoder is expanded and features from different aspects are fused Thus the prediction result can be refined to a certain extent The Accuracy and Specificity of SERAN is better and more stable than U Net and SEN That means the location of suspicious area is easier to detect by using SERAN Although SERAN seemed more unstable than U Net and SEN in Sensitive IOU and Dice coefficient the overall performance of SERAN is relatively more outstanding In order to analyze the effectiveness of multi stream rotation aware mechanism we expand the test set using the data augment method mentioned in Section II B The masses in the expanded test set may display in different orientations due to the mirror operation and rotation operation The experimental results are summarized in Table 3 It is obvious that both SEN and SERAN achieve much better performances than U Net due to the effective encoder we designed Benefit from the multi stream rotation aware mechanism SERAN wins 8 out of 10 comparisons facing mass rotation Although the performance of SERAN has declined to some extent compared to the performance on original test set it is still at a relatively high level In this part effectiveness of IOL is investigated The line 4 of Table 2 shows the results achieved by SERAN which is trained by Dice loss with IOL Compared to the performances of SERAN trained without IOL as shown in the line 3 of Table 2 IOL can obviously improve the performances of SERAN It achieves 8 comparable results among 10 comparisons Accuracy IOU and Dice coefficient of SERAN are significantly improved According to the visual results shown in Fig 10 IOL can avoid misclassification in background area and optimize the network for better prediction which are the leading causes for the improvement For the purpose to determine the weight of IOL we test several values of which are 1 0 1 0 01 and 0 001 The experimental results are showed in Table 4 and the visual results are showed in Fig 11 The loss function used in this work is a combination of Dice loss and IOL which is described in 7 According to 4 6 the change of TP is the main factor affecting the loss function The Outsideloss in IOL aims to reduce the FP indictor Thus it can avoid the prediction of multiple regions The Insideloss in IOL focus on the perceptron about the region of mass When the is small such as 0 001 IOL has little effect on the loss function Thus the performance of SERAN trained by is similar to the one trained by Dice loss and multiple region prediction occurs as shown in Fig 11 d When the gets larger IOL affects the loss function a lot Thus SERAN trends to predict the mass with larger size than ground truth to get the highest TP when trained by as shown in Fig 11 a That is the reason to explain why it gets the highest Sensitive but falls in other metrics To make a balance about constraints of the mass and the background we determined the value of to 0 01 in this work based on the experimental results Performances achieved by SERAN trained by different values of Show All For the purpose to show the advancement of proposed SERAN 5 state of the art methods are implemented for comprehensive comparison Three of them are neural networks and the other two are traditional artificial intelligence methods We implement the three neural networks on Pytorch platform The two traditional artificial intelligence methods are implemented mainly based on OpenCV library and Sklearn library Table 5 shows the comparison between the 5 methods and SERAN And Table 6 displays the computation cost and model complexity of each method In this section SERAN is trained by Dice loss with IOL The method proposed in has achieved the same results every time Thus the STDs of the method are 0 The visual results of these methods are showed in Fig 12 It can be concluded that SERAN outperform all the state of the arts in a certain extent The performance of SERAN is more stable than other neural networks and competitive to traditional artificial intelligence methods The comparison result of sensitive shows that all the methods can predict masses with accurate locations Indicated by IOU and Dice coefficient the mass predicted by SERAN owns the biggest overlapping area with ground truth The Accuracy indicator shows that SERAN can make better prediction about both mass and background than other methods Segmentation results produced by state of the arts and SERAN Show All The most competitive performance is achieved by the method proposed by Sun et al It can be attributed to the dense upsampling and channel attention The two mechanisms are effective to maintain most useful features However the channel attention is in a fully connected style which is the leading cause for the high time cost The method proposed by Li et al employs spatial attention to guide the prediction made by decoder and achieves competitive results in all the metrics The method proposed in owns the minimal complexity among all the neural networks and gets a good performance as well The multi scale context is effective to extract features under different receptive fields All the neural networks run faster than the traditional artificial intelligence methods This may be caused by the great ability of Pytorch framework and the end to end style of neural network The method proposed in has used a complex processing flow to detect and identify the masses in each scale which consists of morphological filtering simple linear iterative clustering segmentation feature extraction and classification The method proposed in has used watershed segmentation twice to find out all the regions of interest A k means clustering algorithm is used to reduce the number of regions found out by the first watershed segmentation algorithm and provide the maker to the second watershed segmentation method Finally the features extracted from each region are used to identify the true mass Most of the components are time consuming which result in the high time costs However the performances of these methods are poorer than neural networks mentioned above In the real world it is easy for specialists to discover the masses with large size while the small size masses are difficult to recognize To evaluate the performance on small size masses we select all the masses smaller than 200 pixels and recorded the Dice coefficient achieved by state of the arts and proposed SERAN As shown in Fig 13 SERAN has achieved much better performance than other methods in most cases It can be concluded that SERAN owns the ability to work in complex conditions Performances achieved by all the state of the arts and proposed SERAN on masses smaller than 200 pixels Show All Moreover the visual results of all the cases in test set have been reviewed by an experienced radiologist In summary our work in this paper has achieved great improvement in breast mass segmentation and can assist the radiologists to a certain extent In this paper a Spatial Enhanced Rotation Aware Network SERAN is developed for breast mass segmentation using digital mammogram Two main critical components are proposed for effective feature extraction and prediction refinement An encoder with spatial attention enhancement under residual learning paradigm is designed for effective feature extraction Spatial attention maps are explicitly modeled to adjust the focus in every encoder stage Moreover residual connection is utilized to avoid gradient vanishing and achieve better convergence To boost the robustness of SERAN to masses displayed in different orientations a decoder using multi stream rotation aware mechanism for feature fusion and prediction refinement is designed A convolution and two asymmetric convolutions whose kernels are and respectively are combined in parallel to boost the rotation robustness To avoid misclassification in background area and optimize SERAN for better prediction a novel regulation item named Inside outside Loss is applied in training procedure Comparing with state of the arts SERAN has achieved significant performance improvement for breast mass segmentation A sensitive of 87 7 an IOU of 73 95 and a Dice coefficient of 84 3 are achieved by SERAN on a representative subset of DDSM dataset In future work we will focus on developing SERAN to fit different types of medical image such as ultrasound CT and MRI Moreover we will try to transform 2D convolution by 3D convolution to adapt 3D scenes such as 3D CT and 3D MRI Besides we will update the current system to fit different application scenes such as brain cancer segmentation and prostate cancer segmentation"}
{"title": "Active Collision Avoidance for Human-Manipulator Safety", "number": "9031397", "authors": "[{'preferredName': 'Guanglong Du', 'normalizedName': 'G. Du', 'firstName': 'Guanglong', 'lastName': 'Du', 'searchablePreferredName': 'Guanglong Du', 'id': 37999836800}, {'preferredName': 'Yinhao Liang', 'normalizedName': 'Y. Liang', 'firstName': 'Yinhao', 'lastName': 'Liang', 'searchablePreferredName': 'Yinhao Liang', 'id': 37088347707}, {'preferredName': 'Gengcheng Yao', 'normalizedName': 'G. Yao', 'firstName': 'Gengcheng', 'lastName': 'Yao', 'searchablePreferredName': 'Gengcheng Yao', 'id': 37087239330}, {'preferredName': 'Chunquan Li', 'normalizedName': 'C. Li', 'firstName': 'Chunquan', 'lastName': 'Li', 'searchablePreferredName': 'Chunquan Li', 'id': 37086131934}, {'preferredName': 'Ronigues J. Murat', 'normalizedName': 'R. J. Murat', 'firstName': 'Ronigues J.', 'lastName': 'Murat', 'searchablePreferredName': 'Ronigues J. Murat', 'id': 37089303498}, {'preferredName': 'Hua Yuan', 'normalizedName': 'H. Yuan', 'firstName': 'Hua', 'lastName': 'Yuan', 'searchablePreferredName': 'Hua Yuan', 'id': 37894318500}]", "abstract": "This paper proposes a novel method for active collision avoidance to protect the human who enters a robot\u2019s workspace in a human-robot collaborative environment. The proposed method uses a somatosensory sensor to monitor the robot\u2019s workspace and detect anyone attempting to enter it. When someone enters the workspace, a Kinect detects and calculates the position of his or her skeleton points in re...", "text": "This paper proposes a novel method for active collision avoidance to protect the human who enters a robot s workspace in a human robot collaborative environment The proposed method uses a somatosensory sensor to monitor the robot s workspace and detect anyone attempting to enter it When someone enters the workspace a Kinect detects and calculates the position of his or her skeleton points in re With the development of Industry 4 0 robots tend to be intelligent and collaborative in the future The human robot collaboration will bring wider use of robots and promote robots to become indispensable partners in human life However the traditional methods of industrial production could not meet the needs of the enterprise s production safety Safety has become one of the most important factors in the future development of human robot interaction Although human robot collaboration allows robots to do broader and more complex tasks it raises the safety concerns of the robots However increasing robot safety often means a compromise in their performance which requires designers to find a balance between safety and performance At present collaborative robots can perceive the environment and change their behavior accordingly For example when a robotic arm hits the human arm the robot can perceive the existence of the human arm through the force sensor Then it stops or goes away to protect human safety This feature limits the performance of collaborative robots such as speed load and so on Therefore an effective method is proposed to predict the possible collision between robots and humans and timely feedback is provided to ensure human safety When a robot performs work in an unstructured dynamic and populated environment finding a way to work safely and efficiently becomes more challenging Safe and efficient human robot interaction is indispensable to future robotics applications where humans and robots must collaborate to perform tasks jointly To meet the strictest safety requirements robots must have an excellent ability to change motion paths in complex environments In any human robot collaborative system or any environment where humans and robots coexist ensuring human safety at all times is a considerable challenge In order to help humans in daily life several studies have been performed to develop robot systems which include interaction and cooperation with humans These studies indicated that robots are very useful in a variety of tasks performed in collaboration with humans For this reason finding a way to protect human operators in human robot collaborative system is highly relevant This not only includes detecting possible collisions in real time but also avoiding collision with humans during runtime Active collision avoidance in real time consists of three parts 1 Environment perception 2 collision avoidance 3 robot control Collision avoidance is one of the most important fields in robotics Many real time capable planning concepts are based on the potential field methods that were introduced in two previous works In these methods virtual attractive and repulsive fields are used to represent targets and obstacles In this case the robot is attracted to targets and moves away from obstacles Real time planning algorithms are significant to robotic systems Despite many achievements in robotic safety most real time collision avoidance methods are either too expensive or have very limited security in practical applications Some methods for example are not practical in the real world because these methods require a dedicated set with built in sensors which can hinder the user s motion and increase cost in practical applications References treat the human body as a moving obstacle so the robot may mistakenly believe that the human body is the operation object and put people at risk In addition due to the lack of independent intelligence the efficiency of collision avoidance is relatively low Current collaborative robots are relatively expensive so replacing a large number of traditional robots with cooperative robots is not a realistic task Moreover cooperative robots tend to adopt post collision detection which inevitably puts people at risk To address these issues this paper proposeds an active collision avoidance method for human robot incorporation In this proposed method Kinect is used to detect humans in real time instead of particle filter PF the improved particle filter IPF algorithm for Kinect is applied to estimate the position of the skeleton joint accurately thus improving the accuracy of human tracking Moreover robot velocity control is implemented to ensure the safety of humans while improving production efficiency A rule based logic system is developed to enable the collision avoidance system to obtain the most appropriate response strategy according to different behaviors of the human When humans are immobile dynamic roadmap DRM is used to plan new paths for the robot directly When humans move slowly the boundary box is calculated and the robot bypass this box by using DRM to plan a new path When humans move too fast the robot immediately stops Therfore the proposed collision avoidance method can take different measures according to different human behaviors for more effectively and safely protecting humans The main contributions are listed as follow Cylinders were adopted to build a bounding box model for both the human bones and the robots Hence collision detection can be achieved by detecting the relative position between the cylinders instead of the point cloud interaction in the depth image thereby improving the timeliness and effectiveness of the collision avoidance method The proposed active collision avoidance method can analyze and predict human behavior and choose the best path for the robot so that the flexibility and intelligence of the robot movement can be improved In the proposed active collision avoidance method an intelligent system with rule based logic is proposed for analyzing human behavior so that the system can correctly respond to different human movement speeds and the DRM was adopted to plan a new path thereby improving the effectiveness and the independent intelligence of the proposed collision avoidance method The remainder of the paper is organized in sections Section III provides an overview of the paper Section IV presents details about the modeling of humans and robots which is the basis of the paper Section V describes the process of estimating skeleton position using improved PF Section VI presents fast path planning using DRM Section VII details robot velocity control and Section VIII describes active collision avoidance using a set of rules The experiments and results are presented in Section IX Discussion is given in Section X and the conclusion in Section XI Planning algorithms allow robots to avoid obstacles in real environments Jiang et al described a planning algorithm that can adaptively coordinate humans and robots in the hybrid assembly system Zhang et al proposed an autonomous path planning method based on improved rapidly exploring random tree algorithm Han et al detected obstacles in real time and re planed the path based on distance calculation and discrete detection Some studies have assessed obstacle avoidance in dynamic environments Most of them have treated operators as moving obstacles This meant that they only need to consider collision avoidance of robots References computed the distance between the human and the robot by analyzing images of the environment Most of the collision detection methods cited above require that information about the environment must be available Such collision avoidance algorithms are primarily based on the distance between the industrial robot and the detected objects in the real environment In human robot collaboration systems two approaches of avoiding collision have been used for safety vision based methods which analyze motion color and texture and inertial sensor based methods which uses special suits for motion capture Currently vision based methods are a better choice for avoiding collisions in robotic control systems because in this way the operator does not have to wear additional equipment Meanwhile the surrounding environmental information not just the motions of the operator can be obtained In addition with the development of various 3D vision sensing system some new depth cameras like the Microsoft Kinect sensor have facilitated the development of a powerful and easy to use sensor system In recent years studies of the vision based method have focused on increasing the efficiency of collision detection One method involved a multi camera system to detect obstacles Tan and Arai used a triple stereo vision system to track the movement of a sitting operator who was wearing a color marker References implemented collision detection and avoidance based on the depth images Ahmad and Plappera proposed an automatic path planning algorithm for robot manipulator which located the unknown obstacles based on the time of flight TOF sensor Human safety can be questionable in environments containing working robots In this paper an active collision avoidance method is proposed to protect the humans who enter robot s workspace Shows the flowchart of the proposed method First a Kinect which has two infrared cameras for depth detection and one visual spectrum camera for visual recognition is used to detect the skeletons inside humans who enter the workspace According to the position data of the skeleton joints of the human body from the Kinect the human breaking into the robot s workspace can be detected even if the human is static In this way the risk that the robot mistakes humans for the object of operation can be reduced thereby ensuring the safety of the human After building bounding cylinders for humans and robots collision detection is achieved by detecting the relative position between the cylinders instead of the point cloud intersections of the depth image which improves the accuracy and effectiveness of the proposed method Process chart of the proposed method Show All To improve the precision of the skeleton points detection an IPF was used instead of the traditional PF for estimation For the sake of the safety of humans while improving efficiency the robot velocity control is implemented based on the distance between the robots and humans Once the robot to human distance is less than the safe distance the speed of the robot begins to linearly reduce relative to the distance Furthermore in order to achieve real time obstacle avoidance the most commonly used strategy is to repeatedly plan the path but this will greatly increase the avoidance time and the avoidance number reducing the intelligence and flexibility of the system Therefore a rule based logic system was developed to analyze the behaviors of the human so that the system could take different measures based on the current motion states of the human When the human is static the system plans a new path using DRM If the human approaches slowly the system builds a bounding sphere for the human and then plans a new path using DRM When the human approaches quickly the robot immediately stops According to the current motion state of the human the robot adopts different reaction strategies thus improving the overall intelligence of the system In this way not only can the avoidance time of the robot caused by the repetitive planning path be reduced but also the required cost can be reduced The human movement tracking approach is driven by two key design goals computational efficiency and robustness Nowadays various depth cameras are widely used in human motion recognition as the position of the skeletal joints can be estimated by the depth image caught by these cameras As illustrated in a single input depth image is segmented into dense body parts with different probabilities and labels which are used to estimate the 3D skeletal joints In the proposed method Kinect is used as a depth camera for extracting human motion data considering that the Kinect has a larger infrared sensor size and a wider field of view and produces a depth image with a higher quality relative to other depth cameras The Kinect also provides an application program interface API for human tracking and positioning When someone enters the robot s workspace the API can detect the human according to the characteristics of the three dimensional data and calculate the location of the human s skeleton As mentioned above the joints of the skeleton can be located using Kinect shows the 15 skeleton joints in the RGB image The 15 skeleton joint points are numbered from top to bottom and left to right Cylinders are used to construct the human body based on the volume of the different parts of the body Pairs of adjacent points are used to build each cylinder The distance between the two adjacent points was used to determine the radius and length of the cylinders so accounting for the sizes of different persons The cylinders are different for every part of the body For example the cylinder for the torso is bigger than the cylinder for the arm The scaling relationship between the size of a cylinder and the length of the corresponding two adjacent points can be calculated using statistical data Joint points in the 3D space 1 HEAD 2 SHOULDER CENTER 3 RIGHT SHOULDER 4 RIGHT ELBOW 5 RIGHT WRIST 6 LEFT SHOULDER 7 LEFT ELBOW 8 LEFT WRIST 9 HIP CENTER 10 RIGHT HIP 11 RIGHT KNEE 12 RIGHT FOOT 13 LEFT HIP 14 LEFT KNEE 15 LEFT FOOT Show All The standard Denavit Hartenberg D H convention is the most widely used means of describing robot kinematics The position of the robot and its tools and their orientation are defined according to the controller conventions Through consecutive homogeneous transformations from the base coordinate to the robot tool coordinate the kinematic equation can be defined as follows where is the translation matrix from the coordinate to the coordinate is the number of joints or joints coordinates The position of each joint can be calculated using  Because the size of each link can be obtained from factory parameters the radius of the cylinder of each link can be determined The length of the bounding cylinder of link is the distance between the position of joint and Then the cylinder of each link can be built after the radius and the length is determined GOOGOL GRB3016 robot and the Skeleton Show All Because both the robot and the human are bounded by cylinders using cylinder cylinder collision detection can efficiently approximate the collision case between a human and a robot Given two colliding cylinders and the relative positions between them could be classified into three categories according to whether the cylinders intersect or not at their bases as shown in The case in which one base of interact with one base of is called base intersection a The case in which the two cylinders intersect only at the sides is called lateral quadrature b And the case in which one base of one cylinder intersects with the side of another cylinder is called lateral heterotrophic c Possible relative positions of colliding cylinders Show All For details about collision detection for cylinder shaped bodies please refer to Although the human skeleton position can be measured by the Kinect the inherent noises of the sensor can increase over time To reduce the measurement errors and noise an improved PF was used to improve the reliability of the proposed methodology The PF algorithm which is a state estimator can be used to approximate the posterior with a finite number of state samples that have corresponding normalized weights At time the approximation of posterior density can be defined as follows where is the position state of the particle at time represents the observations is the number of samples is the normalized weight of the particle at time and is the Dirac delta function As in one previous study an ensemble Kalman filter EnKF is used to approximate the posterior probability function PDF of state variables Given the initial ensembles the forecast ensembles can be calculated as follows where is the model error and represents the covariance of the model error The analyzed particles can be calculated as follows where is the observation error represents the covariance of the observed error and is the measurement matrix The Kalman gain can be obtained as follows According to the literature the weight updating formula is calculated as follows where is the importance density When is selected as  can be expressed as To avoid particle degradation particles must be resampled and then the same weight is assigned to all the particles A Markov Chain Monte Carlo MCMC method is used to improve the diversity of particles after resampling The transition process is implemented as follows where is the Markov transition kernel In the Metropolis Hasting algorithm the resampled particles move to the proposed particle only if which are defined as After the MCMC step the new particles are more diverse because they have a distribution closer to the posterior PDF In the proposed method the human skeleton position is measured by the Kinect and an improved PF is used to estimate the position Each particle position has three states The deterministic input is set to zero since there is no deterministic input to the system The accumulated position difference of the particle at the position iteration as follows Here and is the position state of the particle and is the position state of the particle estimated by EnKF at time Through the values the weight of each particle would be recalculated in every period Then should be adopted in the IPF instead of the direct measurements Therefore the approximate posterior is represented by Then the following equation is satisfied and the weight of the particle can be expressed as By picking the important density and resampling the weight of the particle can be obtained as follows In dynamic environments the angles between joints and the degree of rotation will change a lot The motions of the robot in three dimensional space are very complex and may be occluded at any time considering that obstacles are mostly dynamic In this way obstacle avoidance is necessary for the robot to work in changing environments In this paper dynamic roadmap DRM an adaptive real time path planning method that incorporates a pre processing stage and a planning stage was adopted for obstacle avoidance DRM uses pre computation to increase online efficiency by generating a pre processed roadmap For a given time DRM can quickly determine which parts of the roadmap are blocked before each planning step An enhanced DRM can be produced through a pre processing stage and a planning stage In the preprocessing stage the first step is to build undirected roadmaps in the dimensional configuration space of the robot in which is the number of robot joints The second step is to look for conflict points and edges in the roadmaps First the nodes of the roadmaps were created using a uniform sampling method Then for each node according to pseudo norm space its nearest neighbors were identified provided that the segment between the two nodes was collision free and connected as part of the roadmap The discrete configurations at roadmap nodes and along edges for self collision were checked by using a related algorithm The direct workspace metric was defined as follows Here is the set of all reference points on the surface of the robot and is the location of reference point in the workspace if the robot is in configuration  provides the maximum degree to which any reference point may be displaced between two checked configurations If the robot is bounded by a convex polyhedron and all of its vertices are selected as reference points the metric gives the maximum displacement of any point on the robot It is to be noted that self collision points and edges must be removed when creating the roadmap For this reason after creating the roadmaps in order to look for conflict points and edges the DRM method was used to create a map of the area between the workspace grid and the configuration space roadmap The planning stage involves the following steps invalidating blocked parts of the roadmap connecting start goal configurations to the roadmap and searching the graph In a human robot collaborative environment human safety can be difficult to guarantee In order to ensure the safety of humans while improving the efficiency of the robot in the process of human robot cooperation the movement speed of the robot is determined based on the distance between the robot and the human When the present distance is less than the safety distance the speed of the robot decreases linearly with When the present distance is less than the minimum distance the robot stops working As shown in according the distance between the robot and the human the zone is divided into three regions including the safe zone the acc dec zone and the stopping zone Three regions for the robot manipulator Show All In the proposed method the robot takes different speeds based on the distance between the robot and the human The allowable speed of the robot is defined in the three areas as follows The safe distance is given by where and represent the speed of the human and the robot respectively and denote the collision avoidance system reaction time the braking time with respect to and the braking distance of the robot respectively and represent the resolution distance of the sensor and the uncertainty distance based on the human and the sensor is the maximum speed of the robot under the maximum load The minimum distance is defined as From  When the distance between the robot and the person is greater than the speed of the robot remains static When the distance is greater than and less than the speed of the robot mainly depends on the speed of the robot and the person and the braking distance When the distance is less than the robot stops moving to ensure safety Since human behavior is unpredictable real time path planning alone cannot achieve intelligent obstacle avoidance Our collision avoidance system uses a knowledge based set of rules for decision making The knowledge based contains rules that are usually expressed in the form of IF A THEN B where A is the antecedent conditions and B is the consequences In the proposed approach when operators enter the robot s workspace that is when the distance of the operator from the robot is lower than the dangerous distance refers to the maximum working range of the robot and its tool the collision avoidance system is triggered According to the distance of the Kinect from the industrial robot the maximum detection radius of the sensor can be determined From the above Section of the robot velocity control the safety distance represents the distance for which the robot can avoid people in time Let and formula 18 can be rewritten as follow where represents the maximum speed of the robot under the maximum load and denote the collision avoidance system reaction time the braking time with respect to and the braking distance of the robot respectively and represent the resolution distance of the sensor and the uncertainty distance based on the human and the sensor Once the allowable maximum speed of the robot is determined the allowable maximum speed of the people can be obtained Furthermore the final speed is determined by simulating the robot obstacle avoidance in the simulation environment To achieve active collision avoidance a set of rules is formulated in three cases where a human may enter the robot s workspace Case 1 The human is approaching at a fast pace When the human gets close to the manipulator at the speed m s where is a dangerous speed the system cannot ensure the new planned path is safe during the next iteration The wisest choice is to avoid the human rather than plan a new path right away When the human gets close to the manipulator in the direction of the manipulator should avoid the human in the direction of so that they do not collide Case 2 The human is slowly approaching the robot When the human gets close to the manipulator at the speed m s the system predicts the motion trail of the human and plan a new path to avoid the human using DRM Because of the randomness of human movement the system calculates a bounding sphere that should contain all possible motion trails in a period including cases in which the human suddenly stops Then the robot should plan a new path to avoid the calculated bounding sphere rather than the human during that period When the human suddenly accelerated m s the system should respond as in case 1 Case 3 The human is immobile When the human stands near the robot the system judges whether the human impedes the robot s movements If so the system plans a new path using DRM to avoid him or her In this case the robot does not need to avoid a bounding sphere Instead the system can plan a shorter more efficient path When the human suddenly moves at the speed m s the system should respond as in case 1 Otherwise the system should respond as in case 2 when the speed m s The rules can be expressed as follows IF THEN IF THEN IF THEN A series of experiments were performed to assess the effectiveness of the proposed method The time requirement and efficiency of the proposed method were compared to those of previous methods and During the experiments the proposed active collision avoidance system was executed on an eight core CPU of which four were for the visualization and the robot movement control and the other four were for the complex position calculation and planning path In the proposed method a GOOGOL GRB3016 robot was used to finish the experiments The link parameters in D H model of the robot are listed in Table 1 In the robot control system a working path was designed and the robot was made to move along this path repetitively by using the reverse kinematics algorithm An emulation scene including the robot manipulator model and the human model was built into the robot control system In order to locate the 3D positions of the human and the robot a position measurement system with a Kinect and a calibration board was designed The calibration board was tightly attached to the robot near its base and the Kinect was firmly fixed to a tripod which was placed 2 1 meters away horizontally 1 6 meters away vertically and 1 6 meters height relative to the robot base see The resolution of the Kinect depth images was and the capture frequency was 30 Hz In order to determine the position of the robot and the human the angle of the Kinect was adjusted before the experiments so that the human skeleton and the calibration board can be captured in real time Suppose that and are the frames of the Kinect robot base and the calibration board respectively The relative location between the robot base frame and the calibration target frame could be measured offline using a ruler A method described in previous work is used to calculate the relative location between the calibration target frame and the Kinect frame Then the Kinect frame with respect to the robot base frame could be determined using a calibration target The transformation from the robot base to the calibration board should be measured and determined before any experiments begin During the experiments the positions of the human were obtained by the depth information from the Kinect By capturing the images of the calibration board with the RGB camera in the Kinect the position of the robot could be determined by detecting the corners of the calibration board Experimental environment Show All Structure of the system Show All The goal is to allow the robot to avoid the human who has entered its workspace In the experiments to ensure the safety of the human the robot velocity control is introduced and the movement speed of the robot is determined based on the distance between the robot and the human When the distance is less than the minimum distance the robot would stop working Moreover in order to further protect the human from the accidental injuries in the event of an experimental failure in the experiments we set the force threshold to 50 Newton which is less than the maximum permissible forces of various part of the body as illustrated in the Collaborative Robot Collision Standard ISO TS 15066 When the detected force is greater than the set value the robot stops working In this way when incidental contact between robots and humans occurs contact shall not result in pain or injury The parameters used for the experiment were as follows m s m During the experiments the robot moved all the way along the designed path and the position of the robot EE with respect to the Kinect frame was calculated by the robot control system in real time When a human enters its workspace the skeleton of the person was detected immediately by the Kinect With the captured human skeleton the position of the human related to the Kinect frame and the speed at which the human moves could be determined by the robot control system Once the system determined that the human was approaching the robot the active collision avoidance system performed collision avoidance according to the speed of the human s movements Five testers were invited to carry out the experiments to verify the efficiency of the proposed method The testers are aged from 20 to 26 four males and one female Note that the testers were asked to approach the robot at different speeds Firstly the testers moved toward the robot at a relatively high speed which was more than 0 2 m s Then the testers approached the robot at a speed between 0 and 0 2 m s Finally the tester moves away from the robot Each experiment was repeated three times for a total of 15 times In the experiment the testers attempted to interrupt the robot s work a shows the experimental scene In order to monitor the robot s environment a three dimensional virtual scene was constructed as shown in b During the initialization of the virtual scene there was only one virtual robot When the Kinect sensor detected a human the sensor calculated the position of his or her skeleton in real environment and then a 3D skeleton and a 2D skeleton were drawn in the virtual scene b and on the 2D image c respectively Experimental scene a RGB image b Virtual scene c Three dimensional virtual model projected onto the two dimensional image Show All shows the experimental procedure The human tried to approach the robot then broke into its planned path and finally left it The avoidance component of the robot was equal to the approach component of the human as shown in The pink dotted line is the movement trajectory of the human the blue solid line is the new path of the robot and the red line is the old path of the robot In the beginning as the human was approaching the robot quickly the system found that his or her speed exceed Then the robot tried to avoid the human directly When the human slowed down the system calculated a bounding sphere to plan a new path for the robot Experimental procedure a The human approached the robot b The human broke into the planned path of the robot c The robot avoided human d The human left Show All Three dimensional trajectory Show All shows the velocity curve of both the human and the robot in the direction of the connections between them  s to 1 3 s the human accelerated as he or she approached the robot Because the approach of the component was less than the avoidance component of the robot was 0 In the period between 1 3 s to 2 4 s the approach component was greater than the robot accelerated to avoid the human The avoidance component should be equal to the approach component After 2 4th s the human slowed down and then left the robot after 4 2 s Because the robot needed to avoid the human by bypassing the bounding sphere the avoidance component remained positive until 3 95 s Then when the human began to move away from the robot the robot returned to its previous path Velocity curve of the human and robot in the direction of the connections between the human and the robot Show All In the experiment the proposed method was compared with method and method When a human is approaching the robot at a faster speed the robot moved to avoid him directly while in these previous methods the robot realizes obstacle avoidance by repeatedly planning a new path When the speed of the human slows down the robot avoids him or her by avoiding the calculated boundary sphere while in methods and the robot still avoids humans by repeatedly planning new paths shows the comparison of the proposed method and other methods in the robot moving trajectory Robot moving trajectory Show All shows the comparative results in X Y Z Compared with methods and in the X direction it is obvious that the path of the proposed method is most close to the default trajectory which is nearly the same as the default trajectory And in the Y and Z direction though three methods paths all have clear differences with the default trajectory from 7th sec to the 10th sec we can find that the difference between the proposed method and default path is minimum In general the path of the proposed method is most close to the default trajectory in a three dimension space It is because that the proposed method can correctly respond to different human movement speeds by analyzing the human s behaviors and choose the best path for the robot so that the flexibility and intelligence of the robot movement can be improved The comparison in robot moving trajectory a In X direction b In Y direction c In Z direction Show All shows the comparisons of the proposed method method and method in terms of the avoidance time the number of avoidance events and the increased route Table 2 lists the statistics for the data in including the average minimum and maximum values of the five testers using the three methods In the proposed method the system had to move to avoid the human 3 4 times on average which collectively took 4 76s on average In the method because the system planned a new path for the robot repeatedly the robot had to move 8 4 times on average to avoid the human and spent more time doing it at an average of 11 52s In the method the average number of avoidance events and average avoidance time are 5 2 times and 6 72s respectively Because the robot moved in a new path to avoid the human the route was longer than the initial one In the current method the path is increased by an average of 783 2 mm and the paths that were built by method and method are 1173 2 mm and 937 4 mm respectively Comparisons of the three methods in terms of a Avoidance Time b Number of Avoidance Events c Increased route Show All Moreover a serial of statistical tests is conducted The statistical test data of the paired t test are shown in Table 3 All the two tail p values are less than alpha level 0 05 which indicated that the proposed method has better performance than method and method significantly in our experiments When the human slowed down the proposed method calculated a bounding sphere for the robot so that it could avoid the human quickly which saved a considerable amount of time However the previous methods constantly calculated a new path for the robot so that the robot could move around the human Although the increase in the length of the route was less pronounced using the previous methods avoidance time and avoidance events increased significantly It can be concluded that the proposed rule based logic system can formulate reasonable response strategies according to different behaviors of people In addition the establishment of a cylindrical bounding box model for human bones and robots helps to improve the timeliness and effectiveness of the collision avoidance system For the sake of safe interaction human beings and robots are usually required to work in separate spaces In current industrial applications that lack sensor surveillance robot work cells must be static If a human enters the robot s work cell some additional strategies must be used to ensure the safety of both the human and the robot Existing applications for collaboration between robots and humans are relatively common These applications allow robots to work in less space in situations in which there are no physical barriers or in which no humans are involved In complex environments humans may be mistaken for operation objects like workpieces Robots lack of independent intelligence can place humans in danger This paper uses Kinect to detect humans who entered the robot s workspace and the IPF is applied to improve the accuracy of estimating the skeleton position By building bounding cylinders for the human and the robot the collision detection between the human and the robot can be achieved by detecting the relative position of the cylinders which can improve the effectiveness and accuracy of the system In order to make a reasonable reaction strategy for the obstacle avoidance system based on human behaviors a rule based logic system is proposed and DRM is applied to plan a new path to avoid humans When humans are immobile DRM is used to plan a new path for the robot directly When the humans are moving slowly a bounding sphere is calculated and the robot bypasses it by planning a new path using DRM When the human is moving at a fast pace the robot stops working immediately By taking different measures the system was found to actively protect humans In future work voice control will be added to the system so that the robot can talk to any human who enters its workspace The robot can be made more clearly aware of humans intentions This may make avoidance more efficient This paper proposes an active collision avoidance method for robots using the Kinect sensor and the IPF This proposed method can effectively detect human breaking into the workspace of the robots In the proposed method the collision detection is achieved by detecting the relative position of the bounding cylinders of the human and the robot To take different reaction strategies according to human movement a rule based logic system was proposed to analyze the behavior of the human and DRM was used to plan a new path for the robot to avoid the human Finally experiments were performed to show the effectiveness of the proposed algorithm Experimental results demonstrated that this proposed method could be applied to practical applications such as protecting the behaviors of humans who enter a robot s workspace in a human robot collaborative environment"}
{"title": "Constrained Oversampling: An Oversampling Approach to Reduce Noise Generation in Imbalanced Datasets With Class Overlapping", "number": "9179814", "authors": "[{'preferredName': 'Changhui Liu', 'normalizedName': 'C. Liu', 'firstName': 'Changhui', 'lastName': 'Liu', 'searchablePreferredName': 'Changhui Liu', 'id': 37088592711}, {'preferredName': 'Sun Jin', 'normalizedName': 'S. Jin', 'firstName': 'Sun', 'lastName': 'Jin', 'searchablePreferredName': 'Sun Jin', 'id': 37086853078}, {'preferredName': 'Donghong Wang', 'normalizedName': 'D. Wang', 'firstName': 'Donghong', 'lastName': 'Wang', 'searchablePreferredName': 'Donghong Wang', 'id': 37089514491}, {'preferredName': 'Zichao Luo', 'normalizedName': 'Z. Luo', 'firstName': 'Zichao', 'lastName': 'Luo', 'searchablePreferredName': 'Zichao Luo', 'id': 37089516376}, {'preferredName': 'Jianbo Yu', 'normalizedName': 'J. Yu', 'firstName': 'Jianbo', 'lastName': 'Yu', 'searchablePreferredName': 'Jianbo Yu', 'id': 37878462300}, {'preferredName': 'Binghai Zhou', 'normalizedName': 'B. Zhou', 'firstName': 'Binghai', 'lastName': 'Zhou', 'searchablePreferredName': 'Binghai Zhou', 'id': 37289066800}, {'preferredName': 'Changlin Yang', 'normalizedName': 'C. Yang', 'firstName': 'Changlin', 'lastName': 'Yang', 'searchablePreferredName': 'Changlin Yang', 'id': 37089515897}]", "abstract": "Imbalanced datasets are pervasive in classification tasks and would cause degradation of the performance of classifiers in predicting minority samples. Oversampling is effective in resolving the class imbalance problem. However, existing oversampling methods generally introduce noise examples into original datasets, especially when the datasets contain class overlapping regions. In this study, a n...", "text": "Imbalanced datasets are pervasive in classification tasks and would cause degradation of the performance of classifiers in predicting minority samples Oversampling is effective in resolving the class imbalance problem However existing oversampling methods generally introduce noise examples into original datasets especially when the datasets contain class overlapping regions In this study a n The class imbalance problem occurs when some of the classes in a dataset have significantly more samples than the others The discrepancy in sample number between classes brings about the imbalanced data structure in such kinds of datasets Researchers have reported that this imbalanced structure hampers learning resulting in an undesirable performance of data mining algorithms Imbalanced datasets exist in many real world classification applications such as detecting oil spills in satellite radar images conducting medical diagnosis detecting credit card frauds analyzing neuroimaging data predicting binding site and so on By convention classes with a larger quantity of samples are called the negative classes or majority classes while the others are referred to as the positive classes or minority classes In these domains minority samples e g oil spills are often the ones of interest and misclassifying minority samples is costly However traditional classification algorithms constructed under the assumption or expectation of balanced data distribution are usually inadequate in tackling imbalanced classification problems Disappointing performances of classifiers are often derived on recognizing and predicting minority instances because the imbalanced structure of training set biases the decision boundary of the classifiers toward the majority class Class imbalance issue has received considerable attention and is identified as one of the main challenges in data mining Methods designated to remedy this pervasive problem roughly fall into two categories data based methods and algorithm based methods Algorithm based methods attempt to modify learning algorithms to adjust the classifier to imbalanced datasets They rely on selected classifiers so once we demand the use of a different classification algorithm the extra computation cost is inevitable Moreover the research presented by Maloof suggests that operations of re sampling and adjusting classification algorithms by varying the cost matrix produce similar sets of classifiers Thus we narrow the scope of our study to data level solutions to imbalanced classification problem without loss of generality Data based methods usually refer to re sampling approaches that preprocess datasets to achieve a balanced data structure Before data level approaches are applied we should notice that datasets differ from each other in the number of classes binary class datasets contain only one majority class and one minority class while multi class datasets compose of several majority classes and minority classes In multiple class conditions we can either decompose the problem into multiple two class classification problems or extend the schemes developed on binary class scenarios to multi class problems via pair wise coupling techniques Therefore we focus our efforts on imbalanced classification with binary classes the same as most researches in the literature Data level strategies on binary class imbalanced classification problems can be further divided into undersampling and oversampling Undersampling removes samples belonging to the majority class according to designed criteria to reduce the degree of imbalance in the dataset The least intricate undersampling proposal is Random Under Sampling RUS which randomly discards samples in the majority category to adjust the imbalanced data distribution One of the more advanced undersampling techniques is the One Sided Selection proposed by Kubat and Matwin This method detects and then deletes noisy and redundant samples in the majority category and leaves minority category untouched By choosing a representative subset of the negative examples it balances the dataset Thus the classifier learns a clean boundary and shows better predicting performances Undersampling based on clustering put forward by Yen and Lee is an approach that clusters all the training samples in the first step Then this method decides the number of majority instances that should remain in each cluster according to the ratio of the number of majority samples to the number of minority samples in the particular cluster In the last step it removes the calculated amount of majority samples in each cluster to mitigate the imbalance between classes ACOSampling presented by Yu et al employs an ant colony optimization algorithm to remove less informative instances and search for optimal subsets of a randomly divided part of the original dataset By doing that repeatedly the statistical results from all local optimal training subsets are achieved and presented in a frequency list Then high frequency majority samples are extracted and they are combined with the original minority set to form a new training set Unlike undersampling oversampling concentrates on increasing the number of minority samples Japkowicz and Stephen conducted a series of experiments on how re sampling methods perform on data sets of different complexity and concluded that undersampling methods appear to be less effective than oversampling when two classes were assigned a symmetric cost Meanwhile oversampling was shown to help quite dramatically at all complexity and training set size levels Similar results were reported by Bastista et al We are more concerned with oversampling methods because they are reported to be more effective and are also more related to our work Random Over Sampling is a simple oversampling method that replicates minority class samples at random to get prescribed imbalance level It was pointed out in previous work that ROS is prone to incur overfitting problem in learning Based on this intuitive method more sophisticated oversampling techniques have been developed in researches Some of the improved methods are as below Synthetic Minority Oversampling Technique SMOTE proposed by Chawla et al forms new minority samples by linearly interpolating between minority samples that lie close to each other in feature space Despite its efficaciousness this method has a major shortcoming it blindly generates new samples for minority class examples without considering the distribution of original data so sometimes noise samples are added into the dataset This problem can be more serious when the original data set has one or more overlapping regions or holds noise samples in the original minority class Han et al developed an oversampling technology called Borderline SMOTE based on the observation that misclassified samples usually located on the borderline between minority and majority class In this proposal only minority samples that lie near the borderline are operated on and thus learning on the borderline is reinforced There were two versions for their proposal Borderline SMOTE1 which only generates samples among borderline instances belonging to minority class and Borderline SMOTE2 which also synthesizes examples between minority samples on the boundary and their nearest negative neighbors Another improved oversampling method is Adaptive Synthetic Sampling ADASYN which was presented by He et al This method considers the original distribution This method uses the number of samples that belong to the majority class in k nearest neighbors of a minority sample as a criterion to automatically judge the number of samples synthesized around this specific minority sample More samples would be generated around instances situate close to the majority class region than those far away from the borderline between classes Cluster Based Synthetic Oversampling CBOS devised by Barua et al attempts to avoid noise generation by first conducting unsupervised clustering among minority samples and then carrying out SMOTE process inside the clusters This data generation mechanism avoids the creation of synthetic minority samples in the majority region under some circumstances However when overlapping areas between classes occur in the dataset majority samples would also be included inside the clusters Thus oversampling in these clusters generates minority samples that fall into regions belonging to the majority class and leads to misclassification of majority samples Safe level SMOTE assigns each positive instance a safe level ratio which is the ratio of positive samples in k nearest neighbors of that instance Synthetic samples are generated along the same line segment as SMOTE but these samples are placed closer to minority instances to which higher safe level rations are attached This approach is effective in eliminating noise generation however it does not help recognize samples in an overlapping region since it generates no samples in this region As a result the usefulness of this proposal is weakened by the fact that overlapping between classes is pervading in real world datasets All in all oversampling is widely investigated and used as a remediation strategy to cope with imbalanced datasets By far researchers have developed a batch of advanced oversampling methods such as SMOTE Borderline SMOTE and so on They have been applied to many real world imbalanced datasets and are shown to be effective in improving the performance of classifiers But unfortunately though it was pointed out that class overlapping is a main cause of misclassification in imbalanced datasets previous researchers have not yet devised competent oversampling methods to deal with class overlapping regions as far as we know In class overlapping regions majority samples mingle with minority samples so the boundaries between categories become ambiguous Since classification algorithms are designed to learn the borderline of each class they are less powerful in overlapping areas where borderlines seem to be confusing Consequently the examples situated in these regions are prone to be misclassified Poor learning in overlapping regions calls for the demand of oversampling in these regions to improve the performance of classifiers However most existing oversampling approaches rest on the assumption that samples close to each other in feature space belong to the same class which is not the case in overlapping regions So when existing oversampling methods are applied to these overlapping regions noise minority samples that fall into the majority region are introduced into the datasets These noise samples are detrimental to the performance of classification algorithms As a result traditional oversampling approaches show a deficiency in handling datasets with overlapping between classes In this study we introduce an oversampling method that distinguishes itself from other oversampling techniques by incorporating constraints in the oversampling process to inhibit noise generation in overlapping regions This method namely Constrained Oversampling CO composes of three steps First samples placed in the overlapping area are identified with a simple k nearest neighbors KNN based algorithm Second in these overlapping regions we apply the ant colony optimization ACO algorithm to search for feasible paths from randomly chosen majority samples to a specific minority sample and the majority points on the paths which are closest to the destination are picked out as boundary samples Finally oversampling under distance constraints is performed between boundary samples and the chosen minority point to attain a more balanced dataset The rest of this paper is organized as follows The proposed constrained oversampling method is described in Section II In Section III experiments are conducted on various real world datasets from the UCI repository to test this method and the results are presented and discussed At last Section IV concludes our work In order to balance the dataset without producing noise samples we execute oversampling in overlapping regions and the oversampling process constraints are used to prevent noise generation As is depicted in our strategy is divided into three stages Three steps of Constrained Oversampling Show All In the first step of our method we build a KNN based method to extract overlapping regions included in the original dataset As a geometric classifier the KNN classifier determines the class membership of a data point from its distance to reference data points and it is widely used in clustering Thus KNN is a density based method to classify the input data by the percentage of samples belonging to a different class in their nearest neighbors In overlapping regions minority and majority samples mix with each other By the definition of KNN minority samples locate in overlapping regions are more likely to be misclassified This characteristic of KNN can be taken advantage of to identify samples locate in overlapping regions When the minority samples in overlapping regions are classified to the majority class by the KNN classifier we can know they are misclassifications because the data labels are known in our study First we apply a 5 NN classifier to the original data set as recommended by Mani and Zhang and record the misclassified minority samples For every misclassified minority sample we choose its nearest neighbors to form an overlapping set If there are misclassified minority samples then we get data points In these data points there are some duplicate samples The overlapping set is extracted when these duplicate samples are removed The similarity metric of the KNN algorithm adopted in this paper is Euclidean distance The Pseudo code description of this step is summarized as Algorithm 1 Input Original training set controlling parameter of the scope of overlapping region Process for number of minority samples in find 5 nearest neighbors for minority sample if number of positive samples 5 2 Add and its nearest neighbors in overlapping set endif endfor Remove duplicate samples in Output Overlapping set It should be noted that the total number of overlapping regions do not need to define first It is determined by the distribution of the misclassified minority samples It forms when the overlapping set is extracted To better illustrate our method we apply it to a simulated dataset with two features a displays the binary class imbalanced dataset with two overlapping regions which consist of 800 majority samples represented by green circles and 80 minority instances represented by red pluses After the above mentioned method is applied to the dataset overlapping regions which contain 191 samples are extracted and shown in b According to the distribution of the misclassified minority samples two overlapping regions are generated shown in b The following steps of our oversampling method will be carried out on these overlapping regions a The original data distribution b Overlapping regions extracted with the KNN based method Show All Traditional boundary definition methods such as Tomek links and KNN are density based approaches that judge the boundary samples by the percentage of samples belonging to a different class in their nearest neighbors In overlapping regions where minority and majority samples mix with each together traditional boundary definition methods are prone to take all majority samples in the overlapping regions as boundary samples and are thus imprecise We developed an ACO based method to define the boundaries between classes in extracted overlapping regions With this method the boundaries between majority and minority regions are expressed in the form of a set of majority samples Our proposal embodies two major merits on one hand it is more robust to noise in the original minority set since it does not solely depend on information conveyed by minority samples on the other hand it is advantageous in expanding decision region of minority category to identify boundary majority samples and then introduce them into the oversampling process In defining the boundaries we require an algorithm that can automatically seek feasible itineraries from one class to the other Ant colony optimization developed by Colorni et al has been reported to perform well in solving various discrete combinatorial optimization problems such as traveling salesman problem protein folding fingerprint matching fuzzy identification map matching route improving This population stochastic search method was inspired by the communication of ants during the foraging process Its ability to find an optimal path from nest to food in discrete space through iterative search is what we demand in defining the boundaries surrounding minority regions In this work the ant system version proposed by Dorigo et al is adopted and in particular the sight of ants is limited to 3 i e an ant can only search in the area of 3 nearest points at once to figure out a way toward the food source In each iteration of our ACO based method different majority samples are randomly chosen from the pre defined overlapping region as well as one minority example Then na ants are put on each majority instance and are ordered to search for the shortest path to that particular minority sample We get paths from which we can get to that minority sample from the majority area and then the last station in majority region is picked out and added to boundary set After each minority sample has been taken as the destination once the boundaries between majority and minority categories are constructed and described with a collection of majority samples demonstrates this process step by step and pseudo code of the establishment of boundaries based on ACO is listed as Algorithm 2 a Pick out three starting majority points represented by magenta solid circles and a destination in minority class displayed by the red solid circle b Generate a feasible path generated with ACO represented by the black line c Choose the last station in majority region as a boundary point for the particular minority sample represented by the solid black circle Show All Input Overlapping set number of ants on each node na max iteration number NC max number of starting points the ACO based method in majority region Process for number of minority samples in Randomly choose different majority samples as nests for Start from majority instance and na ants are simultaneously ordered to search for the shortest path to minority sample using ACO After NC max iterations the optimal path is saved as The last majority sample in this path is stored as endfor endfor Output A set of majority samples defining the borderline of minority region It should be pointed out that in our method the rigid convergence of ACO is not a must since we aim at finding borderline points through which we can get into minority areas from the majority region Once a feasible path is identified we can find a majority sample locates on the boundary between two classes so it does not matter that whether this route is the optimal one or only one of the sub optimal solutions In step 3 we construct the final training set by oversampling between the boundary samples and the minority instances in overlapping regions To reduce the generation of synthetic minority samples in majority regions we impose distance constraints to the oversampling process The aim of applying constraints to oversampling is to constrain new samples to the neighborhood of existing minority samples but not to that of majority samples Before oversampling we shall deliberate over three questions which samples are worthy of oversampling How many sample points shall be created for each chosen minority sample Where to place the generated minority samples Question 1 is responded to in Subsection A and B of Section II In terms of the second question ADASYN is integrated into our method to decide the number of new synthetic samples generated for each minority sample in overlapping regions And question 3 can be answered by the constraints demonstrated as follows When a sample between a minority sample in the overlapping regions Min and a majority sample belonging to the boundary set Maj need to be generated we denote the Euclidean distance between these two instances as Dist Min Maj the distance between Min and its 5th nearest neighbor as Dist Min 5th NN of Min and that between Maj and its 5th nearest neighbor as Dist Maj 5th NN of Maj The distance between the new synthetic sample and Min Dist is decided according to rules depicted in Rules to generate a new sample in different scenes Green circles stand for majority samples red pluses represent minority samples and blue squares are newly generated minority samples Show All After Dist is calculated the new sample is synthesized according to where Min new refers to the synthetic instance Dir is a direction vector The illustrations of the location of the newly synthesized sample on different occasions are as in In Subsection A we applied a 5 NN classifier to the original data set as recommended by Mani and Zhang to extract the overlapping area For this data set 5 NN has the best performance in classifying the original data It means that 5 NN reflects the distribution characteristic of the original data very well when classifying the data In the creation of synthetic samples for the minority samples we increase the density of the minority samples in the overlapping area but the distribution characteristic of the original data should be inherited Therefore when we define the Dist the 5th NN of Maj or Min is chosen In this way when we classify the newly generated data Min new it will be classified with a low error rate The whole procedure of this stage is described in detail as Algorithm 3 Input Original data set borderline sample set minority set in overlapping region the majority set in overlapping region amount of SMOTE N number of nearest neighbors Process Calculate the number of minority samples to be generated G N number of minority samples in original data set for number of samples in Find nearest neighbors based on the Euclidean distance and get the ratio number of majority samples in nearest neighbors endfor for number of samples in Calculate the number of synthetic samples that need to be generated for each minority sample according to sum of all endfor for number of samples in for Except for samples that have been selected in the selected sample set randomly choose a minority sample in overlapping region retrieve its corresponding majority samples on the borderline in Randomly pick one out and denote it by Record the minority sample selected in this iteration to the selected sample set so that the samples in this iteration will not be re selected in the next iterations Generate a new sample Min new between and according to the rules depicted in Add Min new to endfor endfor Merge and original set to form a new training set Output New balanced training set In the way mentioned above to enhance learning on minority areas newly generated minority samples are placed in overlapping regions where learning and predicting minority samples are difficult Meanwhile learning on majority regions is not hindered since these synthesized minority samples would not become noise points situated across the boundaries between classes Our constrain based oversampling method is powerful in handling class overlapping regions where traditional clustering based methods do not perform well In this section we conducted experiments on five UCI datasets with different numbers of features and imbalance levels To investigate the performance of the proposed method six benchmark oversampling strategies described in Section II namely original data without oversampling Origin SMOTE Borderline SMOTE1 BOS1 Borderline SMOTE2 BOS2 ADASYN CBOS were applied to the datasets as well as Constrained Oversampling CO and the results were compared A well known binary decision tree learner called Classification And Regression Tree CART was chosen as the test classifier in these experiments Among the five benchmark datasets from UCI Repository that were used in our tests Pima and Haberman are composed of binary class samples and the others are multi class In multi class problems we selected one of the classes as the minority class and the remainders were merged into the majority class Table 1 shows the characteristics of the five data sets namely label of the minority class number of attributes number of minority instances and number of majority samples sorted by the number of minority instances in ascending order Particularly imbalance level IL in the table refers to the ratio of the number of majority examples to that of minority examples The performance of a classifier can simply be shown by the raw data produced during testing the counts of correct and incorrect classifications Generally this information would be displayed in a confusion matrix as follows In Table 2 Tp and Tn are the numbers of true positives and true negatives respectively Fp and Fn are the numbers of false positives and false negatives respectively A widely adopted measurement of classification performance is overall accuracy calculated as follows However in imbalanced data sets accuracy behaves poorly as a metric to evaluate the performance of classifiers over minority examples When a data set is highly imbalanced we attain a high accuracy even if all of the minority examples are misclassified To overcome the problem mentioned above other statistics such as G mean and F measure are also introduced into the assessment of classifiers They are calculated as follows where corresponds to relative importance of precision versus recall and is usually set to 1 Both F measure and G mean are values between 0 and 1 Larger F measure and G mean indicate better performance And Precision Recall and are further defined as They are more reasonable indications of overall precision than accuracy due to some of their unique properties One is that the magnitude of either G mean or F measure relies heavily on how the classifier performs on the minority class It is determined that in this paper Overall Accuracy G mean F measure and would all be calculated to provide a comprehensive understanding of the performance of the classifier on given datasets The initial parameters of the proposed method in this study are listed in Table 3 For each dataset presented in Subsection A of Section III the minority class was over sampled at 100 200 300 400 and 500 of its original size We conducted 3 independent 10 fold cross validation experiments at each percentage of oversampling and the final results were the average value of 3 tests In each test all 7 different oversampling strategies Origin SMOTE BOS1 BOS2 ADASYN CBOS and CO were applied to the same datasets Performances of different methods on datasets oversampled at 200 are shown in Table 4 where four different metrics Overall Accuracy G mean and F measure are listed and compared Other results are displayed in to 9 Experimental results on Glass a Overall Accuracy b G mean c Acc d F measure Show All Experimental results on Abalone a Overall Accuracy b G mean c Acc d F measure Show All Experimental results on Yeast a Overall Accuracy b G mean c Acc d F measure Show All Experimental results on Haberman a Overall Accuracy b G mean c Acc d F measure Show All Experimental results on Pima a Overall Accuracy b G mean c Acc d F measure Show All We present the discussion of the results displayed above in two aspects the impact of oversampling rate on the behavior of oversampling methods and the influence of class overlapping to the classifier and oversampling methods Generally speaking a larger oversampling rate leads to a more advantageous dataset for learning the minority samples This point is justified by c to c However the progress in the performance of predicting minority is usually at the cost of more misclassified majority samples As is depicted in a and a more synthesized samples sometimes mean more unreliable or noise minority instances that fall into the majority region and lead to the misclassification of majority samples So a larger oversampling rate in traditional oversampling methods may give rise to lower Overall Accuracy G mean and F measure As to our method in which we try to eliminate the generation of noise in the oversampling process the figures show ascending performance when the oversampling rate becomes larger However this approach expands the decision region of the minority class in a mild manner and this characteristic leads to a relatively low performance when the oversampling rate is not large enough When more samples are generated our method is shown to be effective As mentioned in Section I we rest our method on the opinion that overlapping between classes is to be blamed for the loss of performance of learning systems in imbalanced datasets To examine this stand we exhibit the characteristics of an overlapping region extracted with the method proposed in Subsection A of Section II in Table 5 First we explored the influence of class overlapping on the performance of our classifier We note that generally of CART on datasets in which most minority samples located inside the overlapping region is undesirable This phenomenon can be easily accepted by observing which shows the relationship between the overlapping ratio of minority sample and in different datasets It can be seen that on Haberman which has as many as 64 20 minority samples placed inside the overlapping area our decision tree suffers from great loss in precision only an of 25 97 is achieved On the contrary on Pima a dataset which possesses a similar imbalance level with Haberman but differs in the ratio of minority samples in overlapping region this ratio is only 38 81 in Pima our classifier turns out to be far more effective in recognizing minority samples by resulting in an of 56 01 Similar situations happen to the changes of G mean and F measure on various datasets The relationship between the overlapping ratio of the minority sample and Acc in different datasets Show All Based on the discussion above we conclude that the performance of classifiers at least CART on an imbalanced dataset is heavily influenced by the overlapping ratio of minority samples The kind of data distribution which most minority samples in the dataset are positioned in the overlapping region could well lead to performance degradation This result verifies and extends the conclusion reached by Yu et al and Jo and Japkowicz Further we investigated the influence of class overlapping on different oversampling methods The relationship between the overlapping ratio of minority sample and F measure in different datasets when oversampling rate equals 200 is shown for different oversampling strategies in In Haberman Abalone and Yeast where the large overlapping ratio of minority samples in these datasets may give birth to a lot of synthetic noise samples SMOTE ADASYN and CBOS which do not take the noise generation around the borderline into consideration bear a greater loss in G mean and F measure compared to the other methods Borderline SMOTE shows better performance because they are devised to strengthen learning on the boundaries But Borderline SMOTE is still undesirable since its assumption is somehow oversimplified in handling overlapping regions Among these techniques our CO method renders to be the best solution in datasets with serious class overlapping The relationship between the overlapping ratio of minority sample and F measure in different datasets when oversampling rate equals to 200 Show All Based on this observation we believe that noise generation in oversampling imposes a negative effect on learning algorithms and the noise prevention mechanism would improve the effect of oversampling methods In this paper we have proposed a new oversampling technique Constrained Oversampling to address the class imbalance problem especially in datasets with overlapping between classes This technique is executed in three successive steps extract overlapping regions based on the KNN algorithm define boundary samples for each minority instance using ACO synthesize the required amount of minority samples under constraints Two major results are expected after this technique is applied learning on the border is strengthened so that minority samples in imbalanced datasets are more easily recognized and few noise samples are introduced to the original dataset when oversampling the minority class in overlapping regions so that the performance would not be harmed by the generation of noise According to our experimental results CO generally produces satisfying accuracy G mean and F measure on various datasets differ with each other in imbalance level number of features and size Further analysis of the results also reveals the impact of class overlapping on the performance of the classifier and oversampling strategies It should be noted that this study has examined only the influence of class overlapping in the imbalanced classification problem To get a comprehensive understanding of the impact of other factors in imbalanced datasets there are still a lot of works to be done Meanwhile there remains substantial room for future work considering the excessive computational and storage cost of the proposed method Random number between 0 and 1 Ant colony optimization Adaptive Synthetic Sampling Relative importance of precision versus recall Borderline of minority region defined by a set of majority samples from to in O Borderline SMOTE1 Borderline SMOTE2 Cluster Based Synthetic Oversampling Classification and Regression Tree Constrained Oversampling Direction vector Distance between the newsynthetic sample and Min Distance between a minority sample in the overlapping regions Min and a majority sample belonging to the boundary set Maj Distance between Min and its 5th nearest neighbor Distance between Maj and its 5th nearest neighbor Numbers of false negatives Numbers of false positives the number of synthetic samples needs to be generated for the minority sample Number of minority samples to be generated Imbalance level K nearest neighbors based algorithm Majority sample Corresponding majority sample on the for Minority sample The minority sample The synthetic instance the new generated Min Number of ants on each node The nest for ACO Max iteration number Amount of SMOTE Overlapping set The optimal path from to in O Synthetic Ration for Random Over Sampling Random Under Sampling Original training set New balanced training set majority set in the overlapping region minority set in the overlapping region Synthetic Minority Oversampling Technique New synthetic samples set The minority sample selected in iterations of generating synthetic samples the numbers of true positives the numbers of true negatives"}
{"title": "A Second Order Algorithm for MCP Regularized Optimization", "number": "9183931", "authors": "[{'preferredName': 'Wanyou Cheng', 'normalizedName': 'W. Cheng', 'firstName': 'Wanyou', 'lastName': 'Cheng', 'searchablePreferredName': 'Wanyou Cheng', 'id': 37089372217}, {'preferredName': 'Hanlin Zhou', 'normalizedName': 'H. Zhou', 'firstName': 'Hanlin', 'lastName': 'Zhou', 'searchablePreferredName': 'Hanlin Zhou', 'id': 37089372139}, {'preferredName': 'Ziten Guo', 'normalizedName': 'Z. Guo', 'firstName': 'Ziten', 'lastName': 'Guo', 'searchablePreferredName': 'Ziten Guo', 'id': 37089372484}]", "abstract": "In this article, we provide two optimal property of MCP regularization optimization. One shows that the support set of a local minimizer corresponds to linearly independent columns of  $A$ , the other provides two sufficient conditions for a stationary point to be a local minimizer point. An active set subspace second-order algorithm for MCP regularized optimization is proposed. The active sets ar...", "text": "In this article we provide two optimal property of MCP regularization optimization One shows that the support set of a local minimizer corresponds to linearly independent columns of the other provides two sufficient conditions for a stationary point to be a local minimizer point An active set subspace second order algorithm for MCP regularized optimization is proposed The active sets ar In this article we focus on the MCP regularized optimization problem where is continuously differentiable and where and The MCP penalty is a fast continuous nearly unbiased and sparse penalty function An important application of 1 found in the signal processing literature is the problem which approximates the known where and and the norm counts the nonzero entries of The problems 1 3 are used in many applications such as in compressive sensing high dimensional variable selection data fitting and image reconstruction Problem 3 is known to be NP hard since it involves the norm and hence is a nonconvex and discontinuous optimization problem Several different classes of approaches have been proposed to solve problem 3 Convex relaxation is the first class of methods in which the norm is replaced by the convex norm One of the most popular methods in convex relaxation is the class of iterative shrinkage thresholding algorithms ISTA and various variants where each iteration requires a matrix vector multiplication involving and followed by a shrinkage soft threshold step Other algorithms for the minimization include alternating direction method of multipliers SALSA coordinate wise descent method projected gradient method reduced space algorithm active set methods We refer to papers for recent advances in this area Another class of methods to deal with optimization problem are greedy methods such as matching pursuit orthogonal mathching pursuit OMP CoSaMp and single best replacement These greedy methods update the support of by exploiting the residual and then solve a least squares problem on the support at each iteration These type of algorithms are widely used in practice due to their relatively fast computational speed Under a sufficient incoherence assumption Tropp proved that OMP recovers the sparsest representation of the input signal We refer to a survey of greedy algorithms for recent advances in this area The convexity of the penalty allows designing fast and global convergent algorithms However it has a drawback which requires more restrictive conditions on the matrix to establish the equivalent and unique global solution to model 3 In addition it tends to produce biased estimated for larger coefficients and lacks oracle property To overcome the drawback produced by the penalty many continuous nonconvex approximations have been proposed to approach the norm Among the variety of like continuous penalties one can find the nonnegative garrote log sum penalty and capped Fan and Li defined necessary conditions to obtain a good penalty function unbiasedness continuity in data and sparsity and proposed the smoothly clipped absolute deviation SCAD penalty Fan and Li discussed a variable splitting and penalty decomposition minimization technique for 2 along with other approximations of the norm Lv and Fan used concave function on to approximate the norm and proved a nonasymptotic nearly oracle property of the resultant estimator Zhang introduced the notion of sparse convexity to compare penalties and proposed the minimax concave penalty MCP Fourcat and Lai investigated the use of norm and an interesting work on the equivalence between minimal and solutions of linear systems of equations equality and inequality was given in A general iterative shrinkage thresholding algorithm in was proposed and illustrated on the SCAD MCP and capped penalties They also proved that the algorithm converged to a critical point Fornasier and Ward proposed an iterative thresholding algorithm for minimizing 2 where the norm is replaced by a reasonable sparsity promoting relaxation and convergence to a local minimizer is established In zhang et al proved that the transformed penalty can recover the exact solution of constrained norm with the null space property NSP and the restricted isometry property RIP They also presented two difference of convex algorithms for the transformed optimization which guarantee the convergence to a stationary point Recently Jiao et al developed a unified primal dual active set algorithm for a class of nonconvex sparsity penalties which includes SCAD capped and MCP Under the restricted isometry property they proved the proposed algorithms converges to the underlying regression target We refer to papers for recent advances in this area In this article we propose a second order algorithm to solve problem 1 The main contributions of our paper can be summarized as follows We provide two optimal property of problem 1 One shows that the support set of a local minimizer corresponds to linearly independent columns of the other provides two sufficient conditions for a stationary point to be a local minimizer point We introduce an active set identification technique An attractive advantage of the identification technique is that it can accurately identify the zero components in a neighbourhood of a stationary point of problem 1 By means of the identification technique we propose a second order algorithm for solving 1 1 To accelerate the convergence of the algorithm the nonmonotone line search is used We prove that every accumulation point of the sequence generated by the new algorithm is a stationary point of problem 1 We do comprehensive numerical experiments to show the efficiency of the proposed algorithm Numerical results demonstrate that the proposed algorithm is competitive with several known methods The remainder of the paper is organized as follows Two optimal properties related to 1 are given in Section II The active set estimate is presented in Section III In Section IV we propose the algorithm and establish the global convergence of the algorithm We report some numerical results in Section V and make some conclusions in section VI In this section we provide two optimal property of problem 1 We begin with some notations A point is a stationary point of problem 1 if it satisfies the following system where is the th component of the gradient vector of and is the function defined by We use the set to denote the support of and use the set to be the set of indices corresponding to the zero components of Namely The set can be subdivided into the following two sets and The support can be subdivided into the following three sets and The following property shows that if is a local minimizer of problem 2 then the columns of are linearly independent Namely the sparsity of the local minimizer is at most Suppose that is a local minimizer of problem 2 then is of full column rank i e columns of are linearly independent Since is a local minimizer of problem 2 there exists a neighbourhood of such that Assume that columns of are not linearly independent Then there exists a vector such that Consequently we have Further we can scale such that Let and By 5 and the definition of we have for any index which means that Since is strictly concave for we have Thus we have which contradicts the assumption that is a local minimizer of problem 2 The proof is completed Chen et al gave an affine scaled second order sufficient condition for local minimizers of nonconvex penalties which include the MCP penalty However it is difficult to verify the condition Jiao et al gave a second order sufficient condition for a stationary point to be a local minimizer point of 2 Below we provide two sufficient conditions for a stationary point to be a local minimizer point of 1 Condition 6 is an extension of the result Suppose that is twice continuously differentiable and is a stationary point of problem 1 If or where denotes the minimizer eigenvalue of the matrix and denotes the minimizer eigenvalue of the matrix then is a local minimizer point of problem 1 Suppose condition 6 holds If the conclusion is clearly Suppose that We shall prove that for any sufficiently small We shall consider two different cases Note that Case 1 For any sufficiently small vector we have Case 2 For any sufficiently small vector we have where the fifth equality uses 4 and the fourth equality uses 7 If by 7 and 8 we have for any sufficiently small The proof is completed In this section we propose the active set estimate and give some properties of the active set estimate We use the following sets and to approximate and respectively Furthermore we partition into two parts That is and Below the theorem shows that when the point is sufficiently close to a stationary point the set is a subset of the set Furthermore when holds the set is actually equal to the set Assume that is a stationary point of problem 1 and is continuous on Then we have the following statement There exists a neighbourhood of such that In particular if then we have There exists a neighbourhood of such that There exists a neighbourhood of such that If holds at then the second statement in 1 follows from the first statement 1 Thus it suffices to prove the statement 1 For each we have By the continuity of we have as Thus as For each by the continuity of we have We claim that it must hold In fact by 4 we get and which contradicts 9 Then we get the statement 1 For each by the continuity of we have as Thus On the other hand for each by 4 we have Thus Then we get the statement 2 For each by 4 we have By the continuity of we have as On the other hand for each by the continuity of we have Then we get as Then we get the statement 3 The proof is completed To get a better descent direction of the new algorithm we partition into and respectively and Furthermore we partition and into two parts respectively The following theorem shows that is a subset of the set when is sufficiently close to Assume that is a stationary point of problem 1 and is continuous on Then there exists a neighbourhood of such that the following statements hold It is easy to see that 10 11 give 12 In the following we will only prove 10 since by a similar analysis we can show 11 For each by the continuity of we have Then we must have and In fact and which contradicts 13 The proof is completed In a similar way as we proved 12 we can prove the following conclusion Assume that is a stationary point of problem 1 Then we have In this section by use of the active set identification technique in Section 3 we shall propose a second order algorithm for solving problem 1 The following assumptions were made on the objective function The level set is bounded In some neighbourhood of is continuously differentiable and its gradient is Lipschitz continuous i e there exists a constant such that Now we describe how to compute the search direction in the proposed algorithm Let be the th iteration For simplicity we let and Define the search direction where where We obtain by solving the following linear system We use the CG algorithm to solve this linear system An attractive property of the CG algorithm is that it tries to determine a good approximation of the full Newton s direction even if is not positive definite It is easy to see that Theorem 2 2 c of ensures that there exist positive constants and such that the direction produced by the CG algorithm satfaisfies the following condition and Then it follows that For each we have and Then we have From Theorem 3 we know that approximates the set when Thus it is a better choice to fix those variables with indices in to 0 Thus we set We will get for all if the unit steplength is accepted see Algorithm 1 Below the theorem establishes that if and only if is a stationary point of problem 1 Initialize an initial guess and constants and Set If satisfies the stopping condition then stop Otherwise go to Step 2 Compute by 16 and 17 Determine satisfying where Set Set and go to Step 1 Assume that is determined by 16 19 Then if and only if is a stationary point of problem 1 Suppose that For each and we have For each by 20 we get By 21 and the assumption we note that For each by the definition of and the assumption we get On the contrary suppose that is a stationary point of problem 1 By Theorem 5 we get For each by 4 we get By 20 we get for all For each we must have In fact if by 4 we get which contradicts the definition of For each by 4 we get The proof is completed The following theorem together with Theorem 6 demonstrates that is a descent direction of at if is not a stationary point of problem 1 Consequently it guarantees that Algorithm 1 is well defined Suppose that is not a stationary point of problem 1 and determined by 16 19 Then we have By the convexity of we get for any For each by 21 we get For each without loss of generality we assume By the definition of we have Thus Then we have For each and by the last inequality we have By the definition of we can get where the first inequality uses 23 the second inequality uses 20 and the last equality uses 24 and 25 By Theorem 6 we get the statement The proof is completed Now we state the steps of the new algorithm for solving 1 as follows The nonmonotone line search condition 22 was introduced by Grippo Lampariello and Lucidi see It has been successfully incorporated into a variety of optimization algorithms Now we need the following two lemmas to establish the convergence of Algorithm 1 Using a similar proof as the one in we get  Assume that satisfies Assumption 1 and is generated by Algorithm 1 Then The following theorem shows that if and then is a stationary point of problem 1 Assume that and where is determined by 16 19 Then is a stationary point of problem 1 Taking into account that the number of distinct sets is finite there exists a subsequence without loss of generality we label again such that index sets are constant and hence we can write For each by the definition of the continuity of and we get that For each by the definition of and we get that For each we have or By we have and Thus we have Thus for each by 20 we get Thus is a stationary point of problem 1 The proof is completed By  we will prove that every accumulation point of is a stationary point of problem 1 Suppose that satisfies Assumption 1 Let be the sequence generated by Algorithm 1 If for all then every accumulation point of is a stationary point of problem 1 It is clear that Since the level set is bounded there is an accumulation point of Suppose that is an arbitrary accumulation point of Then there exists an infinite index set such that Taking into account that the number of distinct sets and is finite there exists a subsequence of without loss of generality we label again such that the index sets and are constant and hence we can write for any We consider two different cases Case 1 By 26 we obtain By  we get the conclusion Case 2 By the line search condition 22 we get for all By the mean value theorem we get for sufficiently large that where the first inequality uses 15 the convexity of and the last equality use 24 and 25 Substituting 27 into the last inequality we have By the boundedness of the last inequality and 26 we get By  we get the conclusion The proof is completed In this section computational experiments for testing the performance of the proposed algorithm on the compressed sensing problems 2 are presented We compare the proposed algorithm with the following three existing solvers GIST PDASC and MSCR The MATLAB code package Unified PDASC is available at http www0 cs ucl ac uk staff b jin software updasc zip We note that package Unified PDASC contains the codes GIST and MSCR MCP form of the three codes are implemented with default parameters All codes are written in MATLAB R2013a and all tests described in this section were performed on a PC with Intel I5 3230 2 6GHZ CPU processor and 16GB RAM memory with a Windows operating system In Subsection VI A parameter settings and data generated are presented In Subsection VI B the effect of different concave parameters on the proposed algorithm is presented We compare our method GIST PDASC and MSCR by solving middle scale problem 2 in Subsection VI C and by solving large scale problem 2 in Subsection VI D The results in Subsecitons VI C and VI D show that our method outperforms GIST PDASC and MSCR both in solution quality and in running time for all most tested problems We now briefly describe the implementation details of Algorithm 1 For convenience we abbreviate Algorithm 1 as AMCP We implemented AMCP with the following parameters and For all the tests the computation is initialized with zero vector To obtain a good initial guess we embed AMCP in a continuation procedure Specifically instead of solving problem 1 directly from scratch we solve a sequence of problems where is a positive integer and and use the solution or an approximate solution as the initial estimate of the solution to the next problem We set and The interval is divided into equally distributed subintervals in logarithmic scale The sequence are chosen to be the endpoints of the subintervals We set in all tested algorithms In our implementation at the end of iteration the next parameter is set to a value smaller than if the point satisfies the condition where We implemented the CG algorithm in to solve 17 with parameters In many applications it is not necessary and wasteful to obtain an exact solution at each iteration of 17 when is large Thus we use only an approximate solution of system 17 and also terminate the CG algorithm if one of the following is satisfied the maximum number of CG iterations exceeds 10 In the following subsections we demonstrate the viability of our approach and focus on the problems from compressed sensing Namely we consider the problem 2 where the goal is to reconstruct a lenghth sparse signal from observations where Given the dimension of the signal and the number of observations we generated a random matrix as follows The type of matrix was chosen from the following types DCT matrix orthogonalized Gaussian matrix whose rows are orthogonalized using a QR Sparse normally distributed random matrix In the experiment we examines the sensitivity of the algorithm with respect to the concavity parameter The type of measure matrix is DCT matrix The observation is generated by where is drawn according to the Gaussian distribution with zero mean and variance 10 2 To generate the true signal we first generated the support by randomly selecting indices between 1 and and then assigned the sign of a normally distributed random variable to for each in The performance is evaluated in terms of CPU time in seconds time and the error error which are computed as the average of 10 independent realizations of the problem Table 1 lists the average CPU time and the average error for each problem From Table 1 we see that the CPU time is fairly robust with respect to the concavity of parameter Further the average error varies little with the parameter indicating the robustness of AMCP In all the following tests we set the parameter In the experiment the type of the measure matrix is orthogonalized Gaussian matrix whose rows are orthogonalized using a QR The true signal is a random sparse vector with sparse signal with a support The dynamical range of the true signal is defined by with and The nonzero spike positions are chosen randomly and the nonzeros values are chosen randomly from The observation data is generated by where is drawn according to the Gaussian distribution with zero mean and variance 10 2 In the experiment we consider a range of degrees of sparseness the number of nonzero spikes in ranges from 1 to 100 in the support We set and In the following figures we use CPU time to denote the CPU time in seconds and error to denote the error error CPU time and error are computed on the average of 10 independent realizations of the problem To have more insights into the numerical results we utilize the performance profiles by Dolan and Mor to evaluate CPU time and error in Figures 1 4 From Figures 1 4 we observe that AMCP is faster than UPDASC and UPDASC is faster than GIST and MSCR Moreover for most problems we also observe that AMCP obtains the similar error as the one of UPDASC for most problems and the error obtained by AMCP and UPDASC is better than the error obtained by GIST and MSCR Performance profiles based on CPU time in log2 scale for Show All Performance profiles based on error in log2 scale for Show All Performance profiles based on CPU time in log2 scale for Show All Performance profiles based on error in log2 scale for Show All In the experiment we use the MATLAB command sprandn m n density to generate the random matrices We also adjusted each column of such that the norm of each column of is one The true signal and the observation were generated by the same way as the second experiment We set the parameter density 0 01 for and density 0 001 for 219 Table 2 lists the CPU time time and the error error From Table 2 we can see that AMCP PDASC and GIST obtain the similar error when and both AMCP and PDASC obtain the similar error for all tested problems In summary we can observe clearly that AMCP obtains the best CPU time and the best error among all tested algorithms for most problems In this article an active set second order algorithm for MCP regularized optimization is proposed The active sets are estimated by an identification technique that can accurately identify the zero components in a neighbourhood of a stationary point The search direction consists of two parts some of the components are simply defined the other components are determined by a second order algorithm A nonmonotone line search strategy that guarantees global convergence is used The numerical comparisons with several state of art methods demonstrate the efficiency of the proposed method"}
{"title": "Self-Collaborative Unsupervised Hashing for Large-Scale Image Retrieval", "number": "9233258", "authors": "[{'preferredName': 'Hongmin Zhao', 'normalizedName': 'H. Zhao', 'firstName': 'Hongmin', 'lastName': 'Zhao', 'searchablePreferredName': 'Hongmin Zhao', 'id': 37089296506}, {'preferredName': 'Zhigang Luo', 'normalizedName': 'Z. Luo', 'firstName': 'Zhigang', 'lastName': 'Luo', 'searchablePreferredName': 'Zhigang Luo', 'id': 37538021300}]", "abstract": "Learning based hashing approaches have achieved considerable success in large-scale image retrieval due to the query effectiveness and efficiency. However, most studies highly rely on supervised knowledge like data labels, thus might fail in unsupervised setting. To address this issue, we propose a self-collaborative unsupervised hashing method (SCUH), which jointly learns hashing function and vir...", "text": "Learning based hashing approaches have achieved considerable success in large scale image retrieval due to the query effectiveness and efficiency However most studies highly rely on supervised knowledge like data labels thus might fail in unsupervised setting To address this issue we propose a self collaborative unsupervised hashing method SCUH which jointly learns hashing function and vir Massive concerns on hashing have emerged in computer vision machine learning information retrieval and related areas due to amazing characters of low storage and fast retrieval Currently learning based hashing techniques mainly learn binary codes from data and meanwhile keep their neighborhood relations in original samples This kind of hashing methods are called data dependent In contrast data independent hashing algorithms directly construct hashing functions with random projections or other strategies Of them LSH as the most representative method efficiently generates hash functions by random projections It is probabilistic far from deterministic Notably LSH can be used anywhere dimension reduction are accounted for A disadvantage of the LSH family is that LSH usually needs long bit to achieve both high precision and recall This leads to a huge storage overhead and thus limits the sale at which an LSH algorithm may be applied Many LSH based variants have been investigated to advance the other shortcomings of LSH Despite their simplicity and flexibility they need longer binary codes with reasonable performance as compared to data dependent ones Many methods tend to keep the neighborhood relations among the original samples mapped in a low dimensional Hamming space Thus it is not strange that data dependent counterparts become the main candidate for large scale image retrieval With or without use of supervised data labels data dependent hashing methods can be separated into two groups unsupervised and supervised Compared with the latter the unsupervised can work well in more broad cases by aid of some extra method such as clustering To name a few spectral hashing seeks compact binary codes of data points so that the Hamming distance between binary codes correlates with semantic similarity The discrete constraints imposed on the binary codes that the target hash functions generate lead to mixed integer optimization problems To simplify the optimization involved in a binary code learning procedure some methods discard the discrete constraints and then get an approximate solution which will increase the accumulated quantization error ITQ tries to minimize the binary loss by an iterative quantization without relaxations One disadvantage of ITQ is that it learns orthogonal rotations over pretreatment operation e g PCA or CCA The two step learning procedure usually makes ITQ suboptimal For the sake of non optimal discrete relaxation discrete graph hashing DGH and large graph hashing with spectral rotation LGHSR either directly solve discrete constraints or the rotation transformation to reduce quantitation errors Principal component analysis hashing PCAH also acquires compact binary codes and preserve the similarities among neighbors Later on feature clustering hashing FCH further considers the unbalanced variance distribution and uncertain similarity relations when learning the projection matrix as in PCAH and PCA ITQ To model manifold structure scalable graph hashing SGH and anchor graph hashing AGH are devised to capture the geometrical structure within dataset in a Hamming space and show the advantages of unsupervised graph hashing AGH leveraged anchor graphs for solving the eigenfunctions of the resulting graph Laplacians making hash code training and out of sample extension to novel data both tractable and efficient for large scale datasets Shen et al proposed a general Inductive Manifold Hashing IMH scheme that also generates nonlinear hash functions Orthogonal to such studies this paper proposes a self collaborative unsupervised hashing method SCUH which learns the hashing function with virtual labels in a self collaborative manner This manner has three distinct aspects 1 SCUH offers itself with supervised virtual labels in a self supervised fashion 2 the learned virtual labels behave like modality features engage in hash function learning processing thereby enabling the model stability and 3 the specific and common latent semantics are modeled by three corresponding projection matrices in a collaborative framework In detail owing to this joint self collaborative manner we learn two specific projection matrix for virtual labels and sample features and then learn the common semantics between them for semantic consistency This way can capture diverse specific information from different features We optimize SCUH via an alternate optimization algorithm in which several sub problems are solved with the corresponding analytical solutions However when deriving virtual labels since solving the closed form solution is very expensive we provide an efficient alternative to achieving the same goal through the fast iterative threshold shrinkage algorithm Experiments of image retrieval on CIFAR 10 YouTube Faces and MNIST datasets show that SCUH outstrips several representative counterparts in quantity In large scale image retrieval efficiency is a core issue for retrieval performance even if the hash trick is adopted Thus it is non trivial to efficiently learn hash function As mentioned in Introduction most effective hashing methods often belong to supervised ones For instance Shen et al proposed supervised discrete hashing SDH for the purpose of fully leveraging supervised labels in a simple form Suppose that there are training samples The corresponding objective function is where is dimensional column vector obtained by the RBF kernel mapping and are by clustering on the training data and is the kernel width The matrix projects the data onto the low dimensional space is a set of bits binary codes for and are projection matrices is the label of the training sample According to  it seems simple and intuitive In fact its formulation gives out the collaborative learning way which treats labels and sample features in a fair manner But it is still inefficiency to learn hash function Moreover SDH is not stable for learning binary code To address this issue Gui et al developed a fast supervised discrete hashing method FSDH which directly regresses the labels with the learned binary codes For clarity we list the objective as below Obviously solving  equals to two least square problems with the closed form solutions At the meantime they can be derived efficiently More importantly this has theory interpretation for its effectiveness Inspired by this point our proposed method can inherit the merits from FSDH in both efficiency and theory Different from FSDH our method can be applied to unsupervised settings Zhang et al claimed that diverse multi modal features was beneficial for retrieval performance To this end a learning framework is introduced by simultaneously learning specific modality transformation matrices and a common transformation matrices This insight can be formulated as where indicates the hashing codes shared by two modalities and indicate the modality specific transformation matrices of image and text data respectively is the shared transformation matrix and maps the latent features to a shared collaborative subspace whilst reducing the quantization loss for hashing codes denotes that other function which is not related to our idea and more details are offered in Here we easily derive that image features and text features play a cooperative role in each other which provides the basis for our method Besides since sample features and their labels are heterogeneous and thus might contain diverse semantics which can be used to improve retrieval performance Inspired by this we further refine this collaborative learning way in a self supervised manner This section introduces a self collaborative unsupervised hashing method then details an efficient alternate optimization algorithm where solving soft labels is non trivial Most studies investigate how to regress the labels with the learned binary codes so as to advance retrieval performance Recently a fast supervised discrete hashing method FSDH is explored to learn hashing function in a distinct way which treats the labels as another features and then learns the corresponding projection matrix to leverage supervised knowledge More impressively the sound theory about this learning way is provided to prove its effectiveness Considering this insight we take a further step by introducing the interplay between labels and features in a collaborative manner In detail we introduce two individual projection matrices to keep specific information for labels and features respectively meanwhile the shared projection matrix is learned to make the low dimensional representation be into a common latent subspace Among them the shared projection matrix plays a role of associating the labels and features This point is not mentioned in FSDH Besides such collaborative learning is introduced in cross modal hashing method and proven beneficial for modeling the diversity of cross modal features Thus we formulate this idea into  as where and denote the projection matrices for the virtual labels and sample features We can view  as a synthesis of two merits of  and the so called collaborative learning way The devised joint model can enjoy the sound theory and information fusion between labels and samples simultaneously The  like other supervised methods rely on manual annotations To this end we consider to learn soft labels in a self collaborative manner which unites self supervised learning and collaborative learning together into a unified framework A candidate approach to arriving at this goal is the usage of unsupervised clustering methods Of them spectral clustering as a typically unsupervised method tries to smooth the soft labels on graphs This scheme acts as a simple and intuitive yet effective methodology and has been broadly used into many other learning methods We absorb the merit of this methodology and introduce it into our model named self collaborative unsupervised hashing SCUH then it can be formulated as where denotes the Laplacian matrix in where denotes the similarities between samples and specific anchor points and according to In fact most previously mentioned methods consider to directly discard orthogonal constraint over virtual labels in order to efficiently solve the optimization problem This could not be optimal To address this issue we design an efficient optimization algorithm to directly solve this problem in the following content It is clear that the  is non convex yet sub convex about each variable Thus we alternatively optimize multiple sub problem to solve each variable with the other fixed The procedure at each round iteration consists of four steps as follows W step By fixing the other variables we can get the solution of where C step With the other variables fixed the objective function about can be addressed Suppose The objective function 7 can become There is a closed solution to which is the eigenvector of responding to the s largest eigenvalues and then and can be obtained B step By introducing  into  we can rewrite the objective function as In order to balance binary codes we initialize where and are the indices of the matrix Obviously the value of is constant so the objective function  can be rewritten as follows We can yield the solution Y step To solve we fix the other variables Firstly suppose we can get The problem 5 can be simplified as The object function 13 can be recast as Denoting we can obtain In empirical studies solving is time consuming thus it is forbidden in large scale retrieval To efficiently calculate we apply a fast iterative shrinkage thresholding algorithm FISTA to defeat this issue in Algorithm 1 Input Output while not convergence do if break end while return Input training examples code length maximum iteration number parameters Output the hashing codes Construct m anchor points of and get the via the RBF kernel Initialize as Initialize by  while not convergence do Update by  Update by  Update by  Update by algorithm 1 end while The proposed SCUH can converge in finite iterations as in Figure 1 The corresponding objective function has the monotone non increasing tendency and remains stable after around 100 iterations on MNIST 200 iterations on CIFAR 10 and 400 iterations on YouTube Faces respectively The objective value of SCUH versus the number of iterations for 64 bit image retrieval on a MNIST b CIFAR 10 and c YouTube Faces datasets respectively Show All In this section we account for the stability of learning hashing codes to indicate the robustness of SCUH according to Different from which is a supervised hashing method our method is unsupervised According to the output hash codes of a stable algorithm can not change much if a training example is deleted or replaced with an independent and identically distributed iid one Let be the training samples for SCUH and be the sample with the th example in replaced with an iid one Suppose that the sample size is the learned class size is code length is and anchor number is Following we modify  as where From that the objective function ensure that the regularization parameters are invariant to the sample size class size and code length Suppose that is the label then From  the solution of composed of and is an eigenvector and then and are bounded respectively which derives that and are the rank expresses the similarity between sample X and its anchors and owns up limit A hashing coding algorithm is stable if the following holds where and are the hash codes learned by adopting respectively and converges to zero with respect to the sample size Owing to the discrete characteristic of and it is not easy to prove the convergence of but the sensitivity and stability of is equal to the robustness and stability of Next we evaluate the stability of In the stability of seems to be independent of other variables including Due to binary values of where is the number of samples and is the code length For they are the eigenvectors thus their bounds equal to due to orthogonality Obviously it is non trivial to seek the bound of which is the soft label to be learned For sake of relaxation the entries in might be negative Despite this we still can yield its bound The objective function w r t is Since  is convex about the local optimal can be achieved Obviously the value of the objective function at can be treated as its upper bound on the convergence situation Thus By simple algebra the  can become Due to the unknown lower bound of we approximate the upper bound of the left of  as follows Let where Then In terms of  we have Let be the non zero minimum eigenvalue of Then Thus where is assumed Following the proof of we obtain and where is the upper bound of Thus and According to  and  we find that the learned changes the stability of This way differs from the generic way More specifically the convergence becomes more stable when the number of instances and the minimum eigenvalue of the laplacian graph increases This indicates the effect of the learned graph over the learned binary codes In Bousquet and Elisseeff have proven that stable algorithm well generalize well on the future coming dataset Moreover experimental results provide another evidence to support this claim as well This also implies the efficacy of the proposed method In this section we verify the effectiveness of our proposed method by carrying out experiments of image retrieval on three real world image datasets including CIFAR 10 YouTube Faces and MNIST We compare SCUH with six well behaved hashing algorithms including ITQ LGHSR SH SGH AGH DGH PCAH and FCH MNIST contains 70 000 784 dimensional handwritten digit images from 0 to 9 Each image is cropped and normalized to The dataset is split into a training set with 69 000 examples and a test set with all remaining examples As a subset of the well known 80M tiny image collection CIFAR 10 contains 60 000 images from 10 classes with 6 000 instances for each class Each image is represented by a 512 dimensional GIST feature vector The entire dataset is split into a test set with 1 000 examples and a training set with all remaining examples YouTube Faces is a database of 3 425 face videos captured from 1 595 different people a new subset is constructed by selecting the person who has at least 500 face images which results in 370 319 samples And 1 770D LBP vector is also pre extracted to represent images For this subset the testing set consists of 3 800 images from 38 people who have more than 2 000 images and we uniformly sample 100 images from each people The remaining samples constitute the training set The evaluation metrics in common have the mean of average precision MAP the precision recall P R and F measure respectively Among them MAP is based on the top retrieved samples which is the average of the rank aware precision of all the samples relevant to the query The precision indicates the ratio of the positive instances predicted to be positive over all the instances i e the true positive and negative instances The recall is the proportion of the positive instances predicted to be positive over all the true positive instances The big area under the P R curve means the good retrieval performance According to the definition of both the prediction and the recall we give out the definition of F measure as below According to  the higher the F measure the higher either the precision or recall We set in all the experiments During constructing anchors which is used to efficiently construct the graph matrix we set 000 and for CIFAR 10 000 and for MNIST and for YouTube Faces All of the source code of the compared methods are test in the same running environment We perform each individual experiment for five times and report their average values as the resultant evaluation results The parameters in Table 1 can be obtained by multiple experiments To improve the reliability of the results the replicated experimental for five times is afforded Obviously the bigger the value of MAP the more precise the results Table 2 reports the different methods based on different bits on three datasets SCUH outperforms the other compared methods in all the cases As shown in Table 2 the MAP of SCUH is stable with increasing the length of binary codes as compared to the other eight methods This indicates our proposed method is effective and provides an alternative to the other ones for image retrieval As the above definition of P R if the P R curve of one method completely covers another method this means that the former is better than the latter Besides MAP we further show the experimental results of all the compared methods on three datasets in terms of the precision recall curves shows that SCUH dominates a larger area under the precision recall curve than the other methods on MNIST Similarly SCUH behaves well in most cases on CIFAR 10 as in Fig 3 and on YouTube Faces in respectively Precision recall curves of the compared methods versus different numbers of bits on MNIST dataset a 24 bits b 32 bits and c 48 bits d 64bits Show All Precision recall curves of the compared methods versus different numbers of bits on CIFAR 10 dataset a 24 bits b 32 bits and c 48 bits d 64bits Show All Precision recall curves of the compared methods versus different numbers of bits on YouTube Faces dataset a 24 bits b 32 bits and c 48 bits d 64bits Show All In the retrieval the F measure value comes from the results of precision and recall we want to have high precision and recall rate at the same time As in 7 the F measure curves provide ample evidence to exposit the effectiveness of SCUH shows that SCUH outperforms the baseline methods in all cases and achieves higher F measure than the compared counterparts with the rise of the number of retrieved images also indicates that F measure of SCUH is higher than the other methods This also implies that the proposed method is sound for image retrieval shows that SCUH is excellent The F measure of image retrieval of the compared hashing methods versus number of retrieved images on MNIST datasets with different bits a 24bits b 32bits and c 48 bits d 64bits respectively Show All The F measure of image retrieval of the compared methods versus number of retrieved images on CIFAR 10 datasets with different bits a 24 bits b 32 bits c 48 bits and d 64 bits respectively Show All The F measure of image retrieval of the compared methods versus number of retrieved images on YouTube Faces datasets with different bits a 24 bits b 32 bits c 48 bits and d 64 bits respectively Show All MAP essentially refers to the enclosed area under PR curve and is adopted by many algorithms In this section we will use it to construct the average error rates which is equals to 1 MAP as the input of the test We perform the paired sample test experiments on three previously used datasets Suppose that the difference between the error rates of our method and the compared methods is The null hypothesis is that the total mean is equal to a value The paired sample test is defined as follows where is the mean value of denotes the standard deviation of the difference between paired samples and is the number of the paired samples Besides is obtained after is calculated by looking up the table of values of Student s distribution If the value is below the statistical threshold usually adopted the null hypothesis will be rejected which means that the compared methods are considered to be statistically identical error rate with 95 confidence Table 3 shows the statistic differences between the compared methods used in experiments on average error rates On CIFAR 10 dataset the of between SCUH and LGHSR exceeds 0 05 This is because the images in CIFAR 10 are often blur and easily derive the noisy embedded graph which affects the retrieval performance Nevertheless overly SCUH is statistically better than the other methods such as PCAH ITQ SH SGH FCH AGH and DGH This paper proposes an unsupervised hashing method in a self collaborative manner called self collaborative unsupervised hashing SCUH In this way SCUH learns virtual labels in local manifolds and simultaneously enjoys specific subspaces for both samples and virtual labels and a common subspace to associate with each other to better induce hashing function The involved collaborative and self supervised learning manners are seamlessly coupled with each other in a unified formulation for retrieval task Experiments of image retrieval on three benchmarks indicate the efficacy of SCUH as compared to several well established counterparts"}
{"title": "Interactive Analysis of Epidemic Situations Based on a Spatiotemporal Information Knowledge Graph of COVID-19", "number": "9239940", "authors": "[{'preferredName': 'Bingchuan Jiang', 'normalizedName': 'B. Jiang', 'firstName': 'Bingchuan', 'lastName': 'Jiang', 'searchablePreferredName': 'Bingchuan Jiang', 'id': 37086834733}, {'preferredName': 'Xiong You', 'normalizedName': 'X. You', 'firstName': 'Xiong', 'lastName': 'You', 'searchablePreferredName': 'Xiong You', 'id': 37086352969}, {'preferredName': 'Ke Li', 'normalizedName': 'K. Li', 'firstName': 'Ke', 'lastName': 'Li', 'searchablePreferredName': 'Ke Li', 'id': 37089375728}, {'preferredName': 'Tingting Li', 'normalizedName': 'T. Li', 'firstName': 'Tingting', 'lastName': 'Li', 'searchablePreferredName': 'Tingting Li', 'id': 37089375524}, {'preferredName': 'Xiaojun Zhou', 'normalizedName': 'X. Zhou', 'firstName': 'Xiaojun', 'lastName': 'Zhou', 'searchablePreferredName': 'Xiaojun Zhou', 'id': 37089376009}, {'preferredName': 'Liheng Tan', 'normalizedName': 'L. Tan', 'firstName': 'Liheng', 'lastName': 'Tan', 'searchablePreferredName': 'Liheng Tan', 'id': 37086833604}]", "abstract": "In view of the lack of data association in spatiotemporal information analysis and the lack of spatiotemporal situation analysis in knowledge graphs, this article combines the semantic web of the geographic knowledge graph with the visual analysis model of spatial information and puts forward the comprehensive utilization of the related technologies of the geographic knowledge graph and big data v...", "text": "In view of the lack of data association in spatiotemporal information analysis and the lack of spatiotemporal situation analysis in knowledge graphs this article combines the semantic web of the geographic knowledge graph with the visual analysis model of spatial information and puts forward the comprehensive utilization of the related technologies of the geographic knowledge graph and big data v During the COVID 19 Coronavirus Disease 2019 outbreaks related methods and spatiotemporal big data have been continuously used to develop differentiated prevention and control measures For example measures such as COVID 19 patients maps the population flow and distribution of potential patients the spatiotemporal tracking of case trajectories the allocation of medical resources in epidemic areas and the differentiated control of the epidemic have been used in the areas where they may have social and economic impacts Applications include the spatial transmission of the epidemic the dynamic analysis of disease conditions and the allocation of emergency resources In general spatiotemporal big data is of great importance in the emergency stage of the epidemic prevention and control but rather weak in real time disease monitoring and the spatiotemporal prediction of the epidemic s spread Due to the limits of the data model it is difficult to associate spatiotemporal big data with multifactorial data related to the impact of the epidemic Single single domain and single mode data analyses which are used to conduct analyses and make predictions through the adjustment correlation and optimization of an algorithm s components that focus on a single domain Thus there is a lack of comprehensive analysis of multiple domains and multiple modalities Therefore it is necessary to consider comprehensively analysing the association of large scale heterogeneous data and using heterogeneous data fusion to expand the multimodal knowledge support of epidemic infection prediction This expansion requires integrating multiple data factors such as spatial and temporal distributions population migration and patients relationships for collaborative analysis This integration must be done to analyse the underlying causes of the outbreaks and accurately discover the spatiotemporal mechanism of the epidemic spread which will provide scientific and technological knowledge that can be used to prevent similar events from happening again Knowledge graphs a branch of artificial intelligence and the most important way to represent knowledge in the big data era have been widely used in intelligent search intelligent Question Answering Q A and knowledge recommendation Faced with the epidemic situation relevant scholars and professionals in the field of knowledge graph research have constructed the knowledge graph of medical resources based on medical data encyclopaedias news announcements etc The graph provides technical support such as medical resource tracing epidemic early warnings etc Constructing a patient knowledge graph can realize the functions of super spreader mining key node discovery prevention and control measures for high risk groups etc In addition the core of a knowledge graph is a large scale semantic web The advantage of a semantic web is that it can well establish the semantic relationships between entities and store them in a simple formalized format for convenient and fast retrieval and analysis Knowledge graphs are stored in triples or quads or quintuples for the purpose of weight reduction and simplification which facilitates the identification by computers and reduces the computing requirements The advantage of the spatiotemporal information analysis model is that it can make full use of spatial location information visualize the spatial distribution and discover the temporal and spatial patterns by using the geographic analysis model It is difficult to visualize the spatial distribution and spread of an epidemic situation when only use a semantic web Complicated relationship information such as the patient s social network relationships cannot be described when only use the geographic spatiotemporal model Therefore the authors consider the combination of the semantic web and geographic information model to give full play to their advantages to grasp the spatiotemporal distribution situation and relationship links In view of the lack of data association in spatiotemporal information analysis and the lack of spatiotemporal situation analysis in knowledge graphs this article combines the semantic web of the geographic knowledge graph with the visual analysis model of spatial information and puts forward the comprehensive utilization of related technologies of the geographic knowledge graph and big data visual analysis Then it realizes the situational analysis of COVID 19 and the exploration of patient relationships through interactive collaborative analysis The main contributions of the paper are as follows Based on the characteristics of the geographic knowlege gradph a patient entity model and an entity relationship type and knowledge representation method are proposed and a knowledge graph of the spatiotemporal information of COVID 19 is constructed To analyse the COVID 19 patients situations and explore their relationships an analytical framework is designed The framework combining the semantic web of the geographic knowledge graph and the visual analysis model of geographic information allows one to analyse the semantic web by using the node attribute similarity calculation key stage mining community prediction and other methods An efficient epidemic prevention and anti epidemic method that has referential significance is put forward Based on experiments and the collaborative analysis of the semantic web and spatial information it has been proved that this method it allows for real time situational understanding the discovery of patients relationships the analysis of the spatiotemporal distribution of patients super spreader mining key node analysis and the prevention and control of high risk groups In the medical field infectious disease prediction analysis has ranged from model predictions to big data driven research Increasingly more attention has been paid to the application of artificial intelligence and big data in the prevention and prediction of infectious diseases The early dynamic model provided a basic method using qualitative analysis and quantitative analysis to predict infectious diseases However due to the small sample data and limited model factors considered the output results were a qualitative prevention test and management mode reports With the development and utilization of multi type sensors various kinds of epidemic related data with more types and more complete samples have been obtained Many scholars turned to machine learning and deep learning to achieve infectious disease prediction However the training model is highly dependent on the size and accuracy of the training data set The setting of the training model parameters is often subjective The epidemic prediction itself has a large number of independent variables making it a complicated problem which could be solved using the combination of a complex network system and a simulation method that could simulate the epidemic spreading process Complex network and agent simulation that considers individual autonomous behaviour and the interaction between individuals can reflect the impact of a dynamic social network and individual autonomous behaviour on epidemic transmission The core of this method is simulation modelling but it does not take into account the attributes of all individuals such as patients the relationships between individuals and the spatial and temporal factors such as behavioural patterns geographical distribution and environmental impacts that affect individuals Spatialtemporal elements are used in the medical research of infectious diseases in three ways The first one is using them to analyse the spatial temporal distribution and epidemic situation of diseases based on a spatial temporal geographical simulation Such methods can provide good spatial and temporal references for the spatial spread of the epidemic the dynamic analysis of the epidemic and the allocation of emergency resources but it is difficult to analyse the underlying causes of the outbreaks and the development of the epidemic The second one is where the elements act as spatiotemporal influencing factors of the epidemic spreading in the process of building a simulation model of medical infectious diseases which takes into account the geographical space time factors in the prediction Such methods consider social and economic indicators meteorological factors population density road traffic spatial patterns personnel movement and other factors which provide a more comprehensive perspective and entry point for the current monitoring of the epidemic situation However the number of influencing factors that a single model can consider is limited In addition the variable factors such as changes in time and space could be hard to model The third way is using them in interactive data analysis driven by multi dimensional data Using spatial temporal big data technology such methods integrate more environmental elements into collaborative analysis The data driven big data analysis method provides a new idea for the prediction analysis and mining of infectious diseases which makes up for the shortcomings of the prediction model that is weak at complex model building The data driven spatiotemporal analysis method enhances the collaborative analysis ability of multi source heterogeneous data emphasizes the relevance of data makes better use of the correlation of data allows for the mutual verification of data cross domains and disciplines and improves the accuracy and scientificity of epidemic prediction By fully using time series data the intermediate process of the epidemic spread can be accurately controlled In addition prediction grading on different levels and stages of the epidemic makes it more conducive to establishing epidemic prevention and control measures The introduction of different types of related data results in problems such as how to establish association models between data and how to realize the collaborative analysis of large scale multi source heterogeneous data furthermore such large scale data samples make the model calculation a problem The knowledge graph is essentially a networked knowledge base that is linked by entities with attributes through relationships In other words a knowledge graph is a knowledge base with a directed graph structure where the nodes of the graph represent entities or concepts and the edges represent various semantic relationships between entities concepts A geographic knowledge graph is the expansion of a knowledge graph using geography and it is a structured geo semantic knowledge base By formally describing the concept entity and attribute and their relationships in the field of geography the concept and entity are connected with each other forming a network knowledge structure The core of a geographic knowledge graph is to construct a large scale geographic knowledge semantic web which is essentially a large scale directed network graph The core technology is geographic entity association model building which allows one to build relationships between large amounts of multi source heterogeneous epidemic situation data and to achieve the collaborative analysis of multiple sources and multiple elements With the rapid development of artificial intelligence big data and other emerging technologies geographic knowledge graphs have become an effective way to realize the intelligent organization of geographic information and it has been rapidly recognized by relevant experts in the field of geography Studies have been done on the aspects of geographic entity extraction topology and orientation relationship extraction and geographic knowledge graph storage Related technology research results on the construction of geographic knowledge graphs mainly include the construction framework of the geographic knowledge graph put forward by Lu et al the analysis and interpretation of the key technology of the construction of the geographic knowledge graph for the intelligent application of a virtual geographic environment that was conducted by Jiang et al the CrowdGeoKG constructed by Zhou and Chen who used wikidata and human geography knowledge to strengthen the geographic entities of OpenStreetMap and the formal representation model of geographic knowledge proposed by Wang et al which can deal with the spatial temporal and dynamic characteristics of geographic knowledge A geographic knowledge graph can provide a good reference for forming the knowledge graph of the spatial temporal information of COVID 19 The modelling and representation of spatial temporal features could be realized using a knowledge graph The knowledge graph of the spatiotemporal information of COVID 19 should include spatial and temporal characteristics geographic entities and data types such as space time object social network and regional environment data A large scale spatiotemporal knowledge graph needs to combine spatiotemporal data representation models such as maps spatiotemporal distributions etc for collaborative analysis Sun and Sarwat proposed a universal geographic knowledge graph indexing framework Riso tree to implement semantic web and geospatial indexing and constructed a location aware search and query based on the geographic knowledge of interactive maps Having combined the characteristics of trajectory data and the definition of a knowledge graph Wu et al extracted the entities relationships and attributes of trajectory data and constructed the trajectory map which supports basic queries range queries nearest neighbour queries keyword queries and trajectory mode queries Xiao et al proposed a rumour propagation dynamics model based on an evolutionary game and anti rumour information and the model can effectively describe the propagation of rumours and the dynamic change rule of the influence of anti rumour information and further proposed a group behavior model for rumor and anti rumor In paper Yunpeng Xiao et al used user multidimensional attributes and evolutionary games combined with the traditional susceptible infected recovered SIR epidemic model which was used to quantify the impacts of external and internal driving factors on group state transitions during hotspot propagation In the application of geographic knowledge graphs geographic knowledge association queries based on knowledge graphs are widely used Typical examples are Geo Wiki a geo semantics sharing network system and KIDGS a geographical knowledge informed digital gazetteer service Lin and Chen and You and Lin stated that to conduct VGE knowledge engineering it is necessary to realize the intelligent transformation data information knowledge smart and realize intelligent virtual geographic environment services Robert presented a tentative conceptual framework for managing practical geographic knowledge and the geographic knowledge base GKB includes geographic objects geographic structures geographic relations geographic rules geographic ontology a gazetteer physico mathematical models and external knowledge Jiang et al stated that combining a spatial temporal data model and semantic web model is one of the main ways to implement an intelligent geographic information service and they also realized intelligent Q A and interaction with a virtual geographic environment by using a geographic knowledge graph Liu et al realized the intelligent querying of Chinese Guqin masters related information based on a knowledge base and a geographic information model Liu et al built an event representation model centred on events that combined spatiotemporal and semantic features which could organize multi source heterogeneous data using knowledge graph technologies to represent terrorist event relationships and attribute information The geographic knowledge graph is an improvement of the knowledge graph which has been successfully applied in fact based knowledge Q A geographic knowledge association searches and intelligent interaction In the face of the epidemic the constructed epidemic Knowledge Q A system and patient relationship query system have played positive roles in the popularization of epidemic knowledge and in self protection measures However due to the incomplete correlation data and inadequate correlation analysis methods it is difficult to carry out in depth analysis which makes it a problem to closely follow and further predict the epidemic situation In summary the related research on knowledge graphs can direct the construction of epidemic knowledge graphs in these two aspects The construction of an ontology model of geographical knowledge Geographical ontology research started earlier and has formed a relatively complete set of ontology systems and ontology model construction methods which can provide ontology system guidance for the construction of the COVID 19 patients spatiotemporal information knowledge graph however it needs to be improved based on the characteristics of the medical field The geographic knowledge extraction and representation model This model has certain referential significance for the extraction of different geographical knowledge especially knowledge with temporal and spatial variables such as spatiotemporal information and geographic events However it requires further research regarding the formalized representation of the geographic spatial temporal model about the epidemic s spread in order to build a model or representation that presents both factual knowledge and epidemic changes Referring to the basic process of building a geographic knowledge graph the basic framework for building the spatial temporal information knowledge graph of COVID 19 patients is shown in Figure 1 The knowledge sources of the spatiotemporal information knowledge graph of patients include the following social network data personal relationship data news data migration data epidemic surveillance data trajectory data and basic geographic information data The data sources formats and extraction methods are shown in Table 1 The basic framework of the spatial temporal information knowledge graph of COVID 19 patients Show All The ontology layer of COVID 19 patients knowledge graph is a subset of geographic ontology ON Concept is defined as follows patients residence hospital vehicle location event ontology Here refers to the confirmed COVID 19 patients refers to the patients residence refers to the hospital where the patients get medical treatment refers to the patients means of transport refers to the location of the patients and E event ontology which mainly includes medical treatment events confirmed events travel events contact events shopping events gathering events home events and fever events Define the relationship in the ontology ON Relationship as follows take the patient ontology and other types of ontology as examples where is the relationship between patients is the relationship between the patients and the places they live i e residence is the relationship between patients and the hospitals where they get medical treatment i e hospital in is the relationship between patients and the transportation i e take is the relationship between the patients and their locations i e location and is the relationship between patients and events i e has event The entity conceptual model is shown in Table 2 The knowledge graph of COVID 19 patients has clear characteristic spatiotemporal information mainly including the following as shown in Figure 2 Physical position characteristics All entities e have basic location characteristics including lng lat where lng represents longitude lat represents latitude and describes the scope of the entity and mainly refers to the administrative radius of the area where the patient is located Triple related to the location In the triple at least one of the two entities and contains a location feature At present the entities with location features in the graph include residences departures destinations hospitals for medical treatment locations and so on The COVID 19 patients entities and relationships Show All An event includes 6 elements who when where action state and what An event expressed as e could be defined as a sixtuple as follows The letters and represent the subject including entities such as people and medical organizations time place actions such as trigger words for actions state set and situations event descriptions respectively For example in patient 23 returned to Taikang County by taking private car from Wuhan on January 7 the event subject is patients 23 the time is January 7 the position is Wuhan Taikang county the actions are take return the state set is return and the event description is take private car to return The relationships between events include constituent relationships sequential relationships causal relationships and correlation relationships The relationship between events can be linked through time place and participants For example Patient XXX female 25 years old now living in the Evergrande Oasis of Minghu Office of the Economic Development Zone contacted the returnees from Wuhan in Zhengzhou on January 18 She took the G658 high speed rail carriage 11 to Xinxiang on January 19 and went back to Zhengzhou on January 21 by high speed rail G1813 carriage 4 Then she drove to Ruzhou PingDingshan on the same day and drove back to Zhengzhou on January 30 at last Since then she had been stayed at home until fever symptoms occurred on February 1 when she drove to Zhengzhou Seventh People s hospital On February 3 she drove to the people s Hospital of Henan Province for treatment and was diagnosed on February 6 As shown in Figure 3 there is a causal relationship between the contact event contact with Wuhan returnees on January 18 and the fever event fever symptoms on February 1 The migration event take G658 high speed rail carriage 11 to Xinxiang on January 19 and the migration event take high speed rail G1813 carriage 4 to Zhengzhou on January 21 are sequential The fever event fever symptoms on February 1 and the event drive to Zhengzhou Seventh People s Hospital on February 1 have a causal relationship The schema of events and relationships Show All The spatiotemporal information knowledge graph of COVID 19 patients expresses the concept entity attribute and event of the COVID 19 subfield with resource the description framework RDF as a triple namely s subject P predicate O object Then we establish the relationships among its elements entities and events It is represented by the directed graph of point edge as shown in Figure 4 The representation of the COVID 19 spatiotemporal information knowledge graph Show All To realize the monitoring and analysis of epidemic situations by combining the patient semantic relation network model and spatiotemporal information visual analysis model the following analysis tasks need to be done Epidemic situation analysis analyse the regional distribution of patients to understand the current situation Patients relationship analysis analyse patients who are infected by gathering for super spreader mining Analysis of the spatial and temporal patterns of the patients analyse patients activity areas to set up early warning and protection areas and Early warning and prediction of high risk groups find the possible cross infection patients based on the relationships between patients and other entities As shown in Figure 5 the basic analytical framework for the comprehensive geographic knowledge graph is based on the knowledge graph semantic web model and the temporal and spatial information visual analysis model Almost all nodes in the spatiotemporal information knowledge graph have spatiotemporal position information and all types of nodes can be mapped to a unified spatiotemporal framework This framework can take full advantage of the characteristics of the semantic web map analysis and geographical spatiotemporal analysis for comprehensive analysis and application The graph analysis based on the knowledge graph semantic model includes functions such as relationship analysis network graph distribution link prediction and graph visualization The analysis based on spatiotemporal data models includes functions such as geographical spatiotemporal distribution spatiotemporal trajectory analysis character and event analysis and spatiotemporal situation analysis Collaborative analysis of the semantic web and spatiotemporal model Show All The patients spatiotemporal information knowledge graph is highly dynamic The entities in the network and the relationships between them would evolve over time and space New nodes might show up while the existing ones might disappear and currently disconnected nodes might be connected Link prediction can be used to analyse the potential relationships between the confirmed patients and to report on an epidemic situation that requires an early warning Node similarity calculation methods are divided into attribute based methods and link based methods The COVID 19 patients knowledge graph semantic web is a directed graph with relatively clear node attributes where the attribute based method should be applied It calculates the similarity by comparing the attribute values of the nodes First the attribute vectors are constructed for each node and each attribute is regarded as a dimension of the multi dimensional space For example and are two nodes in the network and both of them have m attributes which could be expressed as follows The nodes are mapped to the low dimensional space and then the node similarity is calculated using the Euclidean distance formula and cosine similarity formula of vector space The betweenness centrality which is the probability that a node is located on the shortest path connecting any two nodes is closely related to the length of the path The COVID 19 patients knowledge graph semantic web has rich knowledge of node attributes and relationships edges which can realize super spreader mining using the betweenness centrality algorithm The betweenness centrality of a node refers to the ratio of the number of shortest paths passing through the node to the number of shortest paths between any two nodes in the network Its value indicates the network s control ability over information transmission The larger the value is the greater the number of shortest paths between any two nodes in the network passing through the node and the more important the node Suppose that the number of shortest paths between node pair and is the number of shortest paths that pass through node is and represents the betweenness centrality of i e the probability that is on the shortest path between and The calculation formula for the betweenness centrality is as follows is the number of shortest paths between and that passes is the number of the shortest paths between and COVID 19 patients spatiotemporal information knowledge graph is essentially a large scale directed graph The cluster analysis of network nodes is similar to the community detection of a social network A community is a set of nodes that are similar to each other and different from other nodes in the network The community detection algorithm helps to mine some nodes at the centre or edge of the community For example it can be used for the analysis of infected patients caused by a family gathering The Louvain algorithm is a community detection algorithm based on Modularity it is efficient in providing good results and detecting hierarchical community structures Its optimization goal is to maximize the Modularity of the community network The Modularity function is used to measure the quality of the results of the community detection algorithm It can describe the compactness of detected communities Its function is defined as follows where is the number of edges in the network and is the adjacency matrix If equals then otherwise it would be 0 The formula can be further transformed into the following represents the sum of the weights of the edges in community and represents the sum of the weights of the edges connected to nodes in community The Louvain algorithm consists of the following steps Traverse all the nodes in the network assign a single node to the community of each neighbouring node calculate the changes in the Modularity measure represented by and add it to the community of the neighbouring node with the maximum change in Modularity measure Repeat this step until all nodes are no longer change and the generated small community is the input of the second step Compress the graph All nodes in the same community are compressed into a new node The weights of the edges between the nodes in the community are converted into the weights of the ring of the new node and the edge weights between the communities are converted into the edge weights between the new nodes Iterate these two steps until the algorithm is stable When node is assigned to the community of neighbouring node can be calculated by formula 6 The pseudocode of the Louvain algorithm is shown in Table 3 The time complexity of the Louvain community discovery algorithm is where is the number of nodes in the network As the data scale increases the time complexity of the algorithm increases by levels of The experimental data is from Henan Province Government that have been collected from the confirmed COVID 19 patients in Henan Province in China until March 13 2020 After data cleaning the patient knowledge graph of the spatiotemporal information is constructed with 2312 entities and 5055 entity relationships The entity labels include patient train number place of residence place of departure destination hospital event etc The experimental platform adopts the B S architecture which is developed and implemented based on the ECharts and Java languages By taking advantage of both rich GIS information and the interactive visual analysis system a visual analysis system of the spatiotemporal information knowledge graph of COVID 19 is constructed The system interface is shown in Figure 6 Visual analysis of the COVID 19 patient s knowledge graph Show All Figure 6 is the network analysis of the patients relationships in Henan Province which uses the following components Maps and migration maps It mainly displays the location information urban trajectory and migration trajectory of patients Interactive visual analysis of the knowledge graph of COVID 19 patients Through the collaborative analysis with the maps the patient type analysis the regional prevention and control situation analysis the gathering infected patient analysis high risk group prevention and control the city track analysis and the missing reports and concealed cases analysis the following functions are realized patient relationship tracking high risk group prevention and control information release event pattern analysis etc Event axis These lists are used to show the temporal and spatial characteristics of patients visiting events diagnosis events travel events contact events shopping events gathering events quarantine events and fever events After screening the patient and departure place entities in Figure 7 it can be clearly seen that in the early stage of the epidemic most of the infected people are related to Wuhan Hankou and other Hubei areas belonging to the direct input patient type Analysis of patient types Show All After screening the patient and location entities as shown in Figure 8 it can be seen clearly that the patients are mainly located in Zhengzhou Xinyang Pingdingshan Nanyang Jiaozuo Shangqiu Xinxiang Kaifeng Hebi Luohe Zhoukou etc Some patients moved directly between different cities resulting in cross regional infections There is no personnel movement in Jiyuan Luohe and Sanmenxia due to strict control measures Analysis of regional prevention and control situation Show All After the character relationship is screened and the free node is removed the high risk of infection from families has been proved by the Louvain group analysis as shown in Figure 9 Patients analysis of family gathering infection Show All Therefore it is of necessity to implement quarantine measures for families There are several patients who are infected during family gatherings For example as shown in Figure 10 based on the betweenness centrality analysis case 450 case 572 case 758 case 1270 and case 1106 all get infected by their family members they have contacted These cases are typical super spreaders as shown in Figure 11 a and thus it is necessary to remind the personnel who have close contact with them Analysis of super spreaders based on betweenness centrality Show All As shown in Figure 11 b Case 49 case 786 case 424 case 922 and case 787 took the same flight to travel abroad on the same day It proves the necessity of the release the key flight information in time for early warnings and the passengers on this flight should be quarantined for particular observations Analysis of typical cases Show All Node prediction analysis may discover further information For example as shown in Figure 11 c the data show that case 41 and case 758 took the same train on the same day The similarity analysis of node information is conducted and a similarity value of 1 5 is obtained This outcome indicates that case 41 who has not been listed as one of the contactees in the released details might have contacted case 17 The value of the associated forecast between case 17 and case 41 is 1 649 as shown in Figure 12 Therefore it is suspected that case 41 or case 17 is unreported or concealed Node attribute similarity calculation Show All The analysis of the patient trajectories in urban areas could help to take particular measures to report and control the places where cases have visited and the public transportation they have taken In Figure 13 the left graph in the following shows the patient trajectories in the city the middle graph is the sequential events network of confirmed diagnosis confirmed diagnosis confirmed diagnosis confirmed diagnosis and the right is the event axis Analysis of patients trajectories in urban areas Show All Experiments have proved that the knowledge graph of the spatiotemporal information of COVID 19 based on various heterogeneous data could take advantage of both semantic web analysis and the visual analysis of the spatiotemporal information for analysis and application to measures that are used to realize macro situation control of the epidemic outbreak and precise prevention and control for patients Such measures help to improve the accuracy of locating the epidemic transmission path and preventing the epidemic from further spreading However the database of this article is still relatively small The proliferation of the data scale and semantic web scale would lead to low analysis efficiency and weak robustness The knowledge graph of the spatiotemporal information for epidemic prevention and control in the future can be further studied from the following two aspects The expansion of the data association scale The data could be improved by adding associated multi source cross domain data types such as social network data spatiotemporal location information epidemic surveillance information and geographic environment data to build knowledge models and representations suitable for large scale temporal features This improvement would help to form a large scale spatiotemporal information semantic web of the COVID 19 epidemic situation and build up the relationships among epidemic information cross domains and disciplines For example the relationship between characters in addition to family relatives there are friends colleagues teacher student relations etc After expanding the character relationship type a complex character relationship network can be established based on large scale data such as company human resource data government household registration data etc which would help to identify people suspected of contacting confirmed cases Use a graph neural network to learn the spatiotemporal knowledge graph COVID 19 patients spatiotemporal information knowledge graph is essentially a large scale directed graph A graph neural network model can be used to design a highly dynamic spatial temporal neural network that meets the sequential characteristics of directed graphs This design could be used for large scale node prediction link prediction and community clustering prediction algorithms in large scale complex networks to realize the real time prediction and simulation of the epidemic situation of the COVID 19 spatiotemporal information complex network For example we could preset the weights of spatial locations patients influence hidden relations and patients preferences to study and construct a graph neural network model to implement community detection epidemic control area classification and analysis of infection patterns In response to the outbreaks geographers took the initiative to continuously integrate the measures related to space time big data such as COVID 19 epidemic maps the distribution of communities with diagnosed residents and the dynamic trajectory of patients Their efforts in such areas like the spatial transmission of the epidemic the dynamic analysis of the disease s condition and the allocation of emergency resources might be help of differentiated prevention and control and thus cause social and economic impacts They have constructed spatial interaction models of population migration and spatial interaction models of epidemics In general spatiotemporal big data played a particular role in the emergency stage of the epidemic prevention and control In view of the lack of data association in spatiotemporal information analysis and the lack of spatiotemporal situation analysis in knowledge graphs this article combines the semantic web of the geographic knowledge graph with the visual analysis model of spatial information and puts forward the comprehensive utilization of related technologies of the geographic knowledge graph and big data visual analysis This realizes the situation analysis of COVID 19 and the exploration of patient relationships through interactive collaborative analysis Compared to previously conducted studies there are seven innovations in this article It constructs the COVID 19 patients knowledge graph based on the information of the COVID 19 patients in Henan Province It conducts patient type analysis regional prevention and control situation analysis gathering infected patients analysis high risk group prevention and control analysis city trajectory analysis and missing report and concealed case analysis to trace the relationships between patients publish information for high risk group prevention and control and analyse event patterns It designs curve graphs to present the epidemic situation in China and in Henan Province based on the released COVID 19 information that has been analysed using multi view collaborative analysis technology It designs a bubble chart showing the development trend of COVID 19 in all provinces and cities in China It designs a migration chart that is mainly based on the train passenger numbers published on people com cn that have reported confirmed cases It designs a network node chart showing the relationship network in the patient knowledge graph It designs the distribution map showing the distribution of the daily confirmed cases in Henan Province Starting from the analysis of the semantic web and the spatiotemporal distribution a multi level analysis and application from macro situation control to precision patient prevention and control has been implemented which can be used to improve the scientificity and accuracy of the surveillance prediction and responses related to new coronavirus pneumonia epidemics support to further promote the intelligent prevention and control of major epidemic outbreaks and improve the accuracy of locating epidemic transmission path and preventing it from further spreading The authors would like to thank the work of different scholars in knowledge graphs that are cited in the references provided in this article They would also like to thank the developers of ECharts for their excellent open source visualization tools"}
{"title": "Robust Concurrent Detection of Salt Domes and Faults in Seismic Surveys Using an Improved UNet Architecture", "number": "9290013", "authors": "[{'preferredName': 'Mustafa Alfarhan', 'normalizedName': 'M. Alfarhan', 'firstName': 'Mustafa', 'lastName': 'Alfarhan', 'searchablePreferredName': 'Mustafa Alfarhan', 'id': 37088348002}, {'preferredName': 'Mohamed Deriche', 'normalizedName': 'M. Deriche', 'firstName': 'Mohamed', 'lastName': 'Deriche', 'searchablePreferredName': 'Mohamed Deriche', 'id': 37270790800}, {'preferredName': 'Ahmed Maalej', 'normalizedName': 'A. Maalej', 'firstName': 'Ahmed', 'lastName': 'Maalej', 'searchablePreferredName': 'Ahmed Maalej', 'id': 38193736800}]", "abstract": "Interpretation of seismic structural traps for accurate hydrocarbon reservoirs characterization is a challenging task. Seismic interpreters learn to accurately delineate subsurface structures after going through a lengthy process of training and expertise-acquiring that is challenging and time-consuming. In this paper, we propose a novel semantic segmentation model for salt domes and faults identi...", "text": "Interpretation of seismic structural traps for accurate hydrocarbon reservoirs characterization is a challenging task Seismic interpreters learn to accurately delineate subsurface structures after going through a lengthy process of training and expertise acquiring that is challenging and time consuming In this paper we propose a novel semantic segmentation model for salt domes and faults identi Interpretation of seismic records is a crucial task for understanding and analyzing geological information about subsurface structures Seismic interpretation is a workflow that is traditionally undertaken within a collaborative work involving domain experts i e geologists geophysicists geoscientists etc and is normally done interactively on robust interpretation workstations These workstations are sets of high powered computers and software tools meant to assist interpreters with storing rendering and analyzing seismic images The main goal of seismic interpretation is to accurately identify geological structures from seismic surveys Such structures include salt domes faults unconformities horizons facies and gas chimneys to name a few Seismic hazard analysis natural resources exploration hydrocarbon reservoir characterization and depositional environments understanding are some of the broad range applications of seismic interpretation Even though the process of seismic interpretation is computer aided it still requires many hours of manual interpretation including visualizing editing picking and labeling different seismic features along with using distinct marks or colors on a slice by slice basis The difficulty of seismic interpretation is compounded by successive and iterative processes of manual corrections modifications to guarantee acceptable seismic velocity models that are compliant with geological and geophysical knowledge Being time consuming labor intensive and subject to prediction biases plenty of efforts have been put into developing automated seismic interpretation tools A variety of approaches have been developed for seismic interpretation automation over the past two decades These approaches can generally be classified from three perspectives namely the seismic data modality seismic event entity and feature extraction methodology From the first perspective approaches either deal with 2D seismic sections or slices w r t a specific acquisition direction i e in line cross line or time line or with 3D seismic volumes resulting from a combination or stack of 2D seismic sections From the second perspective a large majority of approaches address a single seismic event detection at a time with some using diverse texture and even quality metrics for seismic multi event identification From the third perspective approaches can be categorized into handcrafted feature based and DL feature based In this work we propose a new approach for seismic interpretation using a deconvolutional neural network DCNN We focus on the challenging concurrent detection of salt bodies and faults from 2D seismic sections using real world data from the Netherlands offshore F3 block in the North Sea Currently seismic interpreters are more than ever faced with increasingly larger seismic data volumes and continually dealing with tight deadlines Thus global and integrated solutions are needed to automate the seismic interpretation process Data driven solutions capable of exploiting the full potential of the challenging seismic big data and speed up workflows while guaranteeing high interpretation accuracy Recent years have witnessed the rapid development of deep neural networks DNN resulting in significant performance improvement and great success in numerous computer vision and pattern recognition tasks including image classification segmentation and enhancement object detection and so on The overwhelming efficiency of these techniques triggered the interest of researchers in developing robust seismic interpretation by leveraging the power of DNNs to solve problems associated with seismic surveys understanding modeling and interpretation This work is another attempt in this direction and from a new perspective The novelty of the proposed approach resides in solving the problem of multiple seismic events detection using a hybrid DL architecture Here we focus on analyzing complex seismic structures involving salt domes and faults both known for being reliable hydrocarbon indicators These are very challenging seismic structures due to the weak and chaotic reflection patterns of salt deposit the varying geometry and distribution of faults and the complicated wavefield behavior involved in these structures The primary objective of this work is to concurrently identify faults and salt domes using an improved deep convolutional encoder decoder architecture capable of performing a pixel based prediction on seismic sections to determine whether a pixel is a fault salt or none of these The main challenges encountered within this study involve the following issues 1 how to deal with the complexity of remote sensing data types such as seismic profile records which fundamentally differ from natural images and where the signal to noise ratio SNR is quite low 2 What will be the intuition behind building a task oriented DL architecture for semantic segmentation of multiple classes of seismic events 3 How to encounter the problem of insufficient quantity of labeled samples despite the availability of massive seismic data and large scale seismic volumes The main contributions of this paper can be summarized as follows The accurate detection of multiple seismic structures in a concurrent scenario using an improved DCNN model for semantic segmentation The leverage of transfer learning using pre trained models on natural images onto the context of seismic image analysis and interpretation The remainder of the paper is organized as follows Related works for seismic interpretation of salt and fault structures are presented in Section II In Section III we introduce the workflow of the proposed deep encoder decoder network for relevant seismic features segmentation where different UNet variants are employed to address concurrent detection of salt domes and faults in real world seismic data In Section IV comprehensive experiments are presented and detection results are obtained to assess the performance of the proposed approach Both qualitative and quantitative evaluations are reported with a comparison to a recently developed approach dealing with multi event detection Lastly the conclusion is reported in Section V In this section we present an overview of previous works on seismic interpretation We propose a two fold categorization first we review handcrafted based methods then we give an overview of DL based methods In the literature of salt domes and faults interpretation the majority of approaches were applied to 2D seismic sections using well established feature extraction methods Basic edge detection techniques are primitive but still being in use for simplistic seismic interpretation These techniques can be classified into two families coherency based and differencing based algorithms Coherency metric checks similarity dissimilarity between adjacent seismic traces and can be calculated using different explicit formulations such as cross correlation semblance variance eigen structure analysis and gradient structural tensor GST In contrast differencing based algorithms detect discontinuities through differencing amplitude attributes of adjacent seismic traces using operator based edge detection techniques such as Sobel Robert Prewitt and Canny Wu and Hale were among the first to work on the problem of multiple geologic structures detection and interpretation Fault unconformity and horizon surfaces were extracted automatically from a single 3D seismic volume Seismic attributes estimated from differencing seismic amplitudes and seismic normal vector fields are then used to compute fault and unconformity likelihoods respectively Unfaulting and flattening processes are conducted for straightforward extraction of horizons Most of the processing was achieved by solving partial differential equations Along with edge detection techniques we can distinguish other engineered features such as geometric texture and graph based features Geometric features are obtained by quantifying geometric variations of seismic reflectors using reflector curvature or flexure Textural features are rather difficult and challenging to extract compared to other types of handcrafted features They are based on statistical analysis where the spatial distribution of intensity levels in a pixels vicinity is estimated The relationships between neighboring pixels are evaluated w r t the corresponding gray level and spatial arrangement so as to decide if they form one and the same region of interest or not Different texture measures are proposed directionality smoothness and edge content in gray level co occurrence matrix GLCM contrast and homogeneity gradient of texture GoT code book based learning and seismic saliency Despite being widely used in remotely sensed data texture based techniques suffer from the problem of finding line like edges when dealing with spatial resolution statistics As for graph based features they were first applied in to generate mesh representation of seismic images Inspired by this later work in the authors applied the normalized cuts image segmentation NCIS technique and validated the successful use of graph based representation for seismic interpretation Even though designed with expert knowledge engineered feature extraction methods are unable to fully describe seismic objects of interest and exploit to the fullest extent greater value from complex and noise contaminated real world data Besides most of these approaches address single event interpretation tasks while in a real world scenario seismic data are most likely to contain multiple events and variant seismic features Unfortunately the aforementioned methods remain either trapped in the experimental phase and impractical for industrial deployment or serving as a suite of computer aided tools to assist seismic interpreters Moreover the proliferation of large three dimensional 3D seismic surveying technologies with a large scale coverage relative to basin size has allowed for capturing a massive amount of high resolution seismic data This has revealed the weaknesses of handcrafted features based methods Usually not robust and computationally intensive these methods struggle in achieving high accuracy when dealing with such large scale data The DL paradigm brings data driven technologies to the next level by providing powerful tools that are capable of automatically extracting extremely detailed features from an enormous amount of data The effectiveness of DL based methods has been shown in different applications of seismic data analysis In an interesting investigation by Di et al a proof of principle study is conducted with focusing on the contributing factors to the superiority of CNN based methods over traditional techniques in detecting important seismic structures In the study two key strengths are highlighted the ability to generate a rich suite of feature maps and the patch based encoding of seismic reflection patterns to map seismic signals into targeted seismic structures In what follows we give an overview of main DL approaches related to seismic interpretation while sundering them into two families CNN based and DCNN based Under the first type of approaches the interpretation task is a classification oriented problem whereas under the second type it is a segmentation oriented one The equivalence relation between image segmentation and pixel level classification legitimates the adopted problem solving direction Waldeland et al work appears to be one of the early attempts at applying CNNs to learn features from seismic data for salt bodies delineation A simple CNN architecture is proposed built using 5 convolutional layers and one fully connected layer and a two node softmax for salt or non salt pixel classification By using only one manually labeled inline slice a set of small cubes centered around the corresponding pixels are selected to train the proposed model Only qualitative assessment of the salt detection is reported using an illustration of pixel wise classification results for a few selected sections of the Netherlands off shore F3 block seismic volume Xiong et al applied CNN for fault mapping within a 3D seismic volume Fault probability cube is generated using the CNN model composed of only 2 convolutional layers two fully connected layers and a two node softmax classifier i e fault prediction Only three seismic slices forming orthogonal cross sections are used to feed the 3 channels input layer Fault or non fault prediction is set to be associated with the cubes central point Real data from 8 annotated seismic cubes are used to generate the training data set with one holdout for validation To test the model both synthetic and real seismic data are used The fault probability cube imaging generated by the proposed model highlights seismic faults and shows off discontinuities more clearly compared to the traditional coherence cube method Wu et al used a CNN based pixel wise classification method not only to predict fault probability but also to estimate fault orientations i e dips simultaneously To train and validate the proposed CNN model the authors also developed a well established workflow to automatically generate synthetic 2D seismic data and their corresponding labeling The proposed model outperforms conventional methods when tested on real seismic data Inspired by the latter work Zheng et al used two CNN models for predicting fault presence and its orientation i e dip and azimuth attributes simultaneously They demonstrated that CNN models trained on synthetic data can be used efficiently for fault predictions on field data Shi et al considered salt body detection as semantic image segmentation problem Inspired by both Segnet and UNet they developed a DL encoder decoder architecture for an end to end salt body detection Zeng et al applied the state of art UNet model along with the residual learning framework ResNet for salt body identification Alaudah et al proposed a deconvolutional network for various seismic interpretation tasks including salt domes and faults Di et al proposed a real time seismic interpretation approach using a DNN model The method is capable of accurately identifying several seismic features simultaneously Karchevskiy et al got into the place in Kaggle competition for salt identification using UNet variant and fine tuned the encoder based on the pre trained ResNeXt50 model More recently Li et al used the UNet for seismic fault detection and highlighted the efficiency of such a model in achieving good performance without any issues regarding insufficient training data To perform 3D fault segmentation Wu et al proposed FaultSeg3D a simplified version of UNet where a set of 15 convolutional layers are used instead of the original 23 of UNet and also a reduced number of feature channels per layer Although trained on synthetic data the FaultSeg3D model showed high efficiency in recognizing faults in several seismic data volumes acquired at different surveys A thorough comparison against several conventional methods is reported in terms of both qualitative illustrations and quantitative measurement to demonstrate the superiority of the FaultSeg3D in achieving state of the art results Very limited number of works have addressed the multiple seismic structures detection problems To handle this particular challenging task the proposed approaches either tackle the issue from an image processing perspective using complex engineered features or employ a simple deconvolutional network architecture with poor performance appraisal when it comes to detection results Since UNet was introduced by Ronneberger et al for medical image segmentation it has become the go to architecture for segmentation tasks due to its simplicity and success in tackling diverse segmentation problems In this work we employ the UNet model for concurrent detection of salt domes and faults in real seismic data The UNet has become the benchmark approach for semantic segmentation which led us to select it for tackling our problem over other semantic segmentation deep learning based approaches In addition the simple design of the UNet allows for more customization flexibility which we need to develop our own workflow And most importantly the UNet is suitable for small training datasets Also since the UNet is an encoder decoder network type we exploit transfer learning using two different encoders VGG19 and ResNet34 that have been trained on a substantial natural images database The use of pre trained encoders improved the detection accuracy of our DL network and led to excellent accuracy despite the fact that we only have a small number of labeled seismic images Moreover we show the benefits of using transfer learning by applying pre trained networks compared with the ones trained from scratch Building a successful DL workflow requires the availability of an adequate amount of labeled data so that the network can learn the relevant features to the problem at hand The availability of diverse DL frameworks and various libraries facilitate the use of off the shelf CNN architectures as well as selecting best practices in the field However the bottleneck most of the time for a high performing model is the lack of labeled data For seismic applications there are several publicly available datasets such as the Netherland F3 dataset and the SEAM Phase I dataset Nonetheless the seismic interpretation field is actually facing shortage of labeled data since it requires experts effort time and knowledge to acquire Researchers mainly dealt with this problem in 4 different ways 1 Acquiring a few manually labeled sections 2 Labeling data using conventional image processing techniques 3 Synthesizing data which labeling can be derived automatically or 4 using a small set of labeled data to train a weakly supervised learning approach for predicting labels of a larger pool of dataset In this experiment we choose to use a small set of manually labeled sections from the F3 block for the problem of concurrent detection of salt domes and faults in seismic data Figure 1 shows three seismic images from the training data top row and their corresponding event labeling bottom row From left to right the seismic samples illustrate salt dome fault and multiple faults seismic events respectively Illustration of seismic images first row and their corresponding manual labeling of the underlying events second row Show All We propose to explore two UNet variants in order to find out which architecture has higher detection accuracy The two networks have similar decoders where they contain convolution layers up sampling layers and concatenation layers The concatenation layers are used to concatenate the output of the up sampling layers with the feature maps passed from the encoder along the feature map axis Each convolution layer is followed by a batch normalization BN layer and a rectified linear units ReLU activation layer The difference between the two networks resides in the encoder The first DL network employs a VGG19 network as encoder where successive convolution layers and max pooling layers are used The number of filters is doubled after the max pooling layer and this process is duplicated five times On the other hand the second network s encoder is built using ResNet34 where identity mapping is used to improve the backward flow of the gradient of the error during the training of very deep networks and avoid the vanishing gradient problem In the ResNet network a residual block Figure 2 is used where the input to the block is added to the output after two convolution layers and passed to the next stage However when the input and the output have a different number of feature maps the input is passed through a convolution layer to match the number of feature maps to that of the output Therefore the ResNet34 is built by stacking Residual Blocks Residual Blocks 1 and 2 used to build Resnet Cin is the number of input feature maps and Cout is the number of output feature maps Show All Figure 2 shows Residual Blocks 1 and 2 where the first block is used when there is a mismatch between the number of feature maps of the input and output and the second block is used when there is no such mismatch Figures 3 and 4 show the proposed architectures namely UNet VGG19 and UNet ResNet34 respectively which we built for simultaneous salt domes and faults segmentation The last layer in both networks is a convolution layer with three filters followed by a softmax layer and where the three channels in the output represent the background fault and salt classes respectively The fusion of the UNet with either VGG19 or ResNet34 is achieved by substituting the UNet encoder with VGG19 and ResNet34 networks respectively However both CNNs have their fully connected layers removed usually used for classification tasks and are not relevant to semantic segmentation ones Proposed UNet VGG19 architecture numbers in parentheses are height and width of input and number of filters in convolution layer Show All Proposed UNet ResNet34 architecture numbers in parentheses are height and width of input and number of filters in convolution layer Show All The UNet VGG19 and UNet ResNet34 networks were trained on seismic data from the Netherland F3 block using two methods The first training method involves random initialization of the weights of the neural networks which is commonly referred to as training from scratch The second training method uses the weights of pre trained encoders as initialization parameters Then a fine tuning process is applied to adjust the weights for our specific task The pre trained encoders we used are the VGG19 and ResNet34 which were trained on natural images from the ImageNet dataset With off the shelf pre trained deep neural network architectures we can use the attributes that are learned from a huge amount of data such as the ImageNet dataset and avoid the need for a large amount of labeled data in our seismic interpretation task Ideally we want to train deep neural networks with thousands of labeled images However we were limited by a small number of seismic labeled images where both salt domes and faults coexist The seismic dataset we used contains 61 labeled salt dome images and 43 labeled fault images Out of the 61 salt images 49 are used for training and 12 for validation Similarly out of the 43 fault images 35 are used for training and 8 for validation So 80 of the salt dome and fault images are used for training and the remaining 20 are held out for validation The images are rectangular and have different sizes but neural networks accept a fixed input size Following the practice in the field we chose to set the size of the images to be of the power of 2 and to have a square shape Otherwise we would have to change the implementation of the networks which would make it difficult to use transfer learning Thus the input to the network is chosen to be which is obtained by randomly cropping the input image Also the input image pixel values are normalized and the labels are one hot encoded While carrying out experiments we noticed that faults are mainly line like structures that are thin occupying only one or two pixels in width Thus the fault class makes the data highly imbalanced and the network would be biased toward predicting all samples pixels as non fault We remedy this class imbalance by manually thickening the faults in the ground truth and by using the balanced cross entropy loss function The balanced cross entropy loss is given by where is the ground truth label is the prediction probability is the number of samples in the image is the ratio of the non fault samples and is the ratio of fault samples First we pre process each input image from the North Sea F3 Block by only normalizing the image and then randomly cropping it to a image size After that we train 4 DCNN architectures on these samples Overall we have 4 training scenarios 2 U Net networks trained from scratch where the first has a VGG19 encoder and the second has a ResNet34 encoder The other 2 networks similar to the first two except that the encoders this time are pre trained on ImageNet Each network is trained for 100 epochs with a learning rate initialized at 10 4 and decayed by a factor of 0 5 when the validation accuracy does not improve for successive 5 epochs Unsurprisingly the pre trained networks noticeably show better accuracy results compared to the ones trained from scratch Table 1 summarizes the accuracy rates comparing performances using the pre trained network against the non pre trained ones For further assessment of the models performance we use the most common evaluation metrics utilized to quantify the accuracy of classification segmentation models namely Precision Recall F1 score and IoU Table 2 summarizes the evaluation metrics values obtained using the deployed deep networks Their performance is also compared with the results obtained from the basic UNet model serving here as a baseline model Regarding these evaluation metrics measures the four networks reach high accuracy in detecting the background and salt samples Nonetheless it is worth noting that the applied deep models still struggle to a certain extent with detecting the fault event The pre trained networks however show a big improvement in fault detection This performance improvement is obviously gained from the benefits of incorporating pre trained CNN models in our proposed deep model which allowed for the enhancement of our proposed framework with better generalization ability The incorporation of pre trained CNN models in the encoder side of the proposed architecture brings a deep learning model capable of retrieving low level features i e primitive features such as curves line segments and edges learned from a substantial database of natural images The baseline UNet model shows also good performance and achieves the highest precision rate in detecting faults but it misses out on most of the fault structures as revealed by the low rate of the corresponding recall measure Furthermore qualitative evaluation of the proposed framework is provided through the visualizations of seismic event class prediction for all proposed networks Figures 5 and 6 show typical salt dome and fault images respectively with class prediction results along with manual ground truth and the superposition of both labeled as overlaid respectively We can clearly observe that the 4 networks achieve accurate detection of salt dome events across all sample images As for faults the two networks trained from scratch either missed the fault events or detected them partially Similarly UNet shows accurate detection of salt domes but fails in detecting faults as depicted in Figure 7 On the other hand the two pre trained networks have benefited from being trained on the ImageNet even though it contains natural images and both networks were able to detect most or all fault samples accurately as shown with the 2 sample fault images in Figure 6 The improved performance of the pre trained networks suggests that attributes learned from natural images can be transferred to the seismic domain and can be used to obtain high detection accuracy with small size labeled datasets Event prediction of the 4 networks for an example of a sample salt dome image Show All Event prediction of the 4 networks for an example of two sample fault images first 2 rows left Show All U Net prediction for salt on the top and fault at the bottom Show All Note that our semantic segmentation model is fed with sub images generated from the original seismic section using random cropping This resulted in small geometric variations position wise in the images used for comparison in Figure 6 However these small variations due to random cropping do not invalidate the comparison between the networks In Figure 6 we display some failure cases of seismic event detection The figure shows zoomed regions where the pre trained ResNet34 failed to accurately detect fault s or salt boundaries We can see that for the salt sample the network has thickened more the salt boundary On the other hand for faults samples the network struggled to delineate the upper and lower bounds of the fault endpoints and failed to label these extreme parts as faults We speculate that a possible cause for such failure may be due to the strategy of random region cropping we adopted for preparing the input data to our deep framework The lack of large amounts of labeled data with more variance in the geometry and subsurface conditions can be another reason for such missed detection With more labeled sample images and as we showed in Tables 1 and 1 the proposed architecture is able to learn effectively from the data and extract the most useful and relevant features to delineate accurately fault and salt boundaries The sizes of the UNet VGG19 and the UNet ResNet34 networks are very large with 29 million parameters for the VGG19 version and 24 5 million parameters for the ResNet34 version Conversely the training dataset is relatively small only 84 images Therefore we decided to carry out further testing to ensure that the models are not overfitting and only learning attributes that are useful for seismic data Moreover since the networks trained from scratch did not perform well on faults in what follows we focus on the test results obtained using the pre trained networks We selected the LANDMASS dataset to perform our tests which contains different types of seismic events including salt domes and faults To evaluate the networks prediction performance we overlay the prediction on the original seismic image for visual inspection In Figure 9 we show the results for salt domes detection for three images with the prediction for the UNet VGG19 network on the left and the UNet ResNet34 on the right as well as for three fault images in Figure 10 In the case of salt examples both pre trained networks were able to detect salt domes and delineate salt boundaries accurately but in some cases the UNet VGG19 cannot distinguish between salt boundaries and faults Also the fault detection is accurate but the UNet VGG19 network is more sensitive to discontinuities in the sense that it detects more discontinuities as faults that are not faults Both seismic and natural images contain primitive features i e line segments edges corners etc that can be learned as low level features using DL architectures VGG and ResNet were trained to detect low level features from substantial natural images databases Similar low level features exist in seismic images hence transfer learning using these networks exploits what has been learned from natural images to help with the detection of seismic events Zoomed in portions for 3 sample images showing situations of network s failure Show All 3 salt test images from the LANDMASS dataset with the predictions using the pre trained networks blue green and red represent background fault and salt respectively Show All 3 fault test images from the LANDMASS dataset with the predictions using the pre trained networks blue green and red represent background fault and salt respectively Show All It is worth noting that very few related works provide quantitative evaluation of their seismic interpretation models Most approaches do not apply common evaluation metrics other than accuracy rates for performance assessment Experimental outcomes are usually limited to qualitative evaluation through some illustrations of the interpretation results on test data Moreover some approaches generated synthetic data to train their models and reported high performance through very few illustrations of interpretation results on field data This is mainly due to lack of labeled data Indeed as metric formulas involve ratios of prediction and ground truth samples a sufficient set of labeled data is therefore needed which is not easily accessible when it comes to seismic data Table 3 summarizes comparison with several other seismic interpretation approaches As outlined in the table the comparison study takes into consideration the approach category handcrafted or DL based the seismic event object of interpretation the dataset type for training synthetic or real world and eventually the evaluation metrics along with corresponding maximum values As mentioned before most proposed approaches tackle one particular seismic event using different state of art DL models Only two approaches consider more than one seismic event the handcrafted based work in and Different metrics were reported in standard metrics such as IoU and AUC but also new ones were introduced such as pixel accuracy PA mean intersection over union MIU and frequency weighted intersection over union FWIU In the case of only qualitative evaluation is carried out through visualization of segmentation results on field test data For salt domes the authors in trained a SegNet network using 8 crossline sections on salt dome segmentation and obtained 98 77 accuracy on the training dataset with no qualitative evaluation on the test dataset Also the authors in proposed to use a U Net ResNet DL model for salt domes segmentation They used 2 inline sections for training and 1 section for testing with no quantitative evaluation of the network s performance The approach in for salt dome segmentation trained a basic U Net on 10 crossline slices and again no qualitative evaluation As the authors who used SegNet pointed out SegNet is prone to checkerboard artifacts which makes it not suitable for small features learning such as faults thin line segments Visual comparison of our salt dome detection segmentation against the three aforementioned approaches shows that we achieve better segmentation results of salt domes where salt boundaries are accurately traced over the whole image For fault detection the authors in generated synthetic data to train CNNs where each image contains straight lines faults They reported their result for one DL model that gave the best results on synthetic data and another that gave the best visual results on real world data The results for the second CNN on synthetic data are Accuracy 0 94 Sensitivity 0 69 Specificity 0 99 F1 score 0 80 and AUC 0 96 Even though our pre trained networks have a lower F1 score 0 7712 with the VGG19 network and 0 6817 with the ResNet34 network our results were obtained on challenging field data Moreover using CNN introduces redundancy since the network classifies only one pixel in each run whereas U Net classifies all pixels at once in one run The authors in used a small set of real seismic data to train a U Net model on fault detection The best obtained result achieves IoU 0 500 after a post processing stage In contrast our pre trained networks reached higher IoU with 0 6588 with VGG19 and 0 5419 with the ResNet34 network Lastly we compare our proposed method with the multiresolution approach developed in Four multiresolution techniques based on texture attributes were used to label seismic structures from the Netherland F3 block Specifically the Gaussian pyramid the Discrete Wavelet Transform Gabor filters and the Curvelet Transform In the Curvelet Transform provided the best results with a detection accuracy 0 7955 IoU 0 2656 for faults and IoU 0 5261 for salt domes using only four inline images from the F3 block for validation Our results are significantly better with IoU 0 6588 compared to 0 2656 for faults and IoU 0 9776 compared to 0 7953 for salt domes using the U Net VGG19 pre trained network We should note however that the authors considered four seismic event classes chaotic fault salt and other whereas we considered only three fault salt and background Figure 11 shows predictions of our proposed DL models on the same 4 inlines that were used in from the Netherland F3 block only the bottom section is displayed Since our network accepts input of size we divided each image into patches of size and passed them to the network one by one The network was able to detect the salt dome with high accuracy and also detect most of the faults structures Our fine tuned neural networks were able to learn features that are specific to faults and others specific to salt boundaries with hardly any confusion Concurrent detection of salt dome and fault events for 4 inline images Show All Finally we should note that our proposed framework is well suited for interfacing with user friendly GUIs to assist interpreters in visualizing different types of events either simultaneously or separately The interpretation results can also be translated into saliency maps or likelihood maps i e probability maps with meaningful colormap encoding various seismic events for enhanced visualization The work discussed here fits well with the efforts put in the industry for optimizing oil and gas exploration processes Over recent years we have witnessed major partnerships between companies from the oil and gas industry and advanced IT companies joining efforts in developing intelligent systems for enhancing productivity such as the example of TOTAL France and Google Cloud or ExxonMobil with MIT Such partnerships turned to powerful AI ML Artificial Intelligence Machine Learning tools to make the work of seismic volumes interpreters more efficient Among the different IT companies focusing on developing dedicated tools and systems for 2D and 3D interpretation tasks for industry we mention Eliis International PaleoScan and GVERSE to mention a few These companies developed advanced software packages for seismic interpretation with some of the algorithms using diverse Deep Learning networks We should however be cautious when using diverse machine learning models as these can be sensitive to the data distribution For example if the distribution of data i e histogram for the training data e g F3 block is different from the distribution of data for another dataset e g TGS data as shown in Figure 12 their performance can be significantly degraded We show in Figure 13 the prediction of UNet ResNet34 on 3 samples from the TGS dataset with their corresponding ground truth The first 2 rows show accurate detection compared to the ground truth but the fault channel is activated in a very small portion at the salt boundary The last row shows an example of a partial failure where the network hardly detects the salt boundary at the bottom left of the image and instead a fault channel was activated in green Data distribution histograms for a sample from the F3 block top row and a smple from the TGS data bottom row Show All UNet ResNet34 prediction on 3 smaple images from the TGS dataset Show All Concurrent detection of various events from seismic surveys while extremely important is very challenging In this paper we introduced a novel semantic segmentation workflow for the simultaneous detection of salt domes and faults using an improved UNet deep network To further enhance the UNet performance we exploit transfer learning from two different encoders namely the VGG19 and ResNet34 The networks are first trained on natural images ImageNet before using the fused UNet on seismic surveys We showed that transfer learning paradigm alleviates the everlasting scarcity problem of labeled seismic data and is very useful in the case of faults identification given the limited availability of training data The knowledge learned from natural images edges corners intensities etc was very useful for identifying the subsurface structures solely from the seismic amplitude attributes Using transfer learning high delineation accuracy is obtained with reduced execution time and with using a small amount of labeled training data Moreover using transfer learning we developed a robust model that is not affected by the similarity between different types of discontinuities in noisy seismic data which is improved by utilizing the skip connections strengths of the ResNet model Comprehensive experiments were conducted through validation and testing on real world seismic data from the publicly available Netherlands offshore F3 block LANDMASS and TGS datasets Both qualitative and quantitative evaluations confirmed the superior performance achieved by our developed DL workflow under the challenging scenario of multiple events detection in subsurface surveys"}
{"title": "Transparent Antenna for Green Communication Feature: A Systematic Review on Taxonomy Analysis, Open Challenges, Motivations, Future Directions and Recommendations", "number": "9292914", "authors": "[{'preferredName': 'Omar Raed Alobaidi', 'normalizedName': 'O. R. Alobaidi', 'firstName': 'Omar Raed', 'lastName': 'Alobaidi', 'searchablePreferredName': 'Omar Raed Alobaidi', 'id': 37087140120}, {'preferredName': 'Puvaneswaran Chelvanathan', 'normalizedName': 'P. Chelvanathan', 'firstName': 'Puvaneswaran', 'lastName': 'Chelvanathan', 'searchablePreferredName': 'Puvaneswaran Chelvanathan', 'id': 37847114100}, {'preferredName': 'Sieh Kiong Tiong', 'normalizedName': 'S. K. Tiong', 'firstName': 'Sieh Kiong', 'lastName': 'Tiong', 'searchablePreferredName': 'Sieh Kiong Tiong', 'id': 37543107100}, {'preferredName': 'Badariah Bais', 'normalizedName': 'B. Bais', 'firstName': 'Badariah', 'lastName': 'Bais', 'searchablePreferredName': 'Badariah Bais', 'id': 37542964800}, {'preferredName': 'Md. Akhtar Uzzaman', 'normalizedName': 'M. A. Uzzaman', 'firstName': 'Md. Akhtar', 'lastName': 'Uzzaman', 'searchablePreferredName': 'Md. Akhtar Uzzaman', 'id': 37088542866}, {'preferredName': 'Nowshad Amin', 'normalizedName': 'N. Amin', 'firstName': 'Nowshad', 'lastName': 'Amin', 'searchablePreferredName': 'Nowshad Amin', 'id': 37409406500}]", "abstract": "Recent years have illustrated the significantly pervasive interest in transparent antennas. The number and the popularity of transparent antenna applications have escalated dramatically. Although antenna applications are diverse and available across multiple platforms, some of these antennas are unsuitable for practical usage, particularly in cases associated with renewable energy. As such, this s...", "text": "Recent years have illustrated the significantly pervasive interest in transparent antennas The number and the popularity of transparent antenna applications have escalated dramatically Although antenna applications are diverse and available across multiple platforms some of these antennas are unsuitable for practical usage particularly in cases associated with renewable energy As such this s Over the past 30 years optically transparent conductors have revolutionized electronics in many televisions laptops smartphones smartwatches and solar panels Such conductors are materials that allow light to be transmitted and at the same time and provide electrical conductivity Transparent films TCFs the most widely used optically transparent conductors are used in handheld touch screens and flat panel TVs among other uses Since these deposited thin films in the visible spectrum are usually transparent they can be deposited mounted on aircraft windows to provide aircraft electronics with electromagnetic interference EMI shielding These materials are typically used mostly in applications where optical clarity is desired since the material visible speed clarity is required to be seen readily by a person and the conductivity needs are limited since most applications are low frequency However Development of multimodal data mergers The growth in CubeSats and The beginning of a drone have Pressure on sensor developers among other improvements to improve payload efficiency while reducing scale weight and power SWaP High efficiency optically translucent drivers known as antennas can be used for single aperture lidar radar fusion for autonomous vehicle navigation as antenna communication and sensing on missions CubeSat as well as on camera lens integrated antennas for visible and thermal imaging Optically translucent conductors inventions and history have paved the way for products such as antennas connectors and filters that are transparent microwaves and millimeter wave mm wave devices Micro and mm wave frequency conductors that are optically transparent allow for modern fusion methods and electromagnetic systems that have not been feasible before This article includes a description of the fundamentals of optically transparent conductors and an efficiency comparative literature survey linked to a single merit figure indicating trade offs between optical transmittance and radio frequency RF resistance This paper provides an in depth analysis of the optically transparent antennas introduced in recent years along with the inspiring applications in which these antennas have an innovative potential The main purpose of this article is to look at the different forms of transparent antennas Discussions that revolve around optical and electrical properties are common in light of conducting thin films and transparent dielectrics This article compiles information about transparent conductive and non conductive antenna types By concentrating on their design issues and compromises the conduct of thin films and meshed conductor made transparent antennas are addressed They explored both optical and antenna efficiency The main goal is to perform research that is understandable for antenna and microwave engineers This review paper focuses on the transparent antenna Optically transparent antenna OTA Transparent conductive oxide TCO Mesh conductive metal MCM Overall this paper will explain the subject through different aspects and stages Firstly discussion over literature was grouped as Challenges Motivations Limitations Recommendations and Significant Study each group will have different categories Secondly Principles of Design and Development will bring the transparent antenna from the first step of the design to the technique used to get an efficient antenna Thirdly critical analysis on this stage we will give a brief on the fabrication process for each type of conductive material used for the transparent antenna Then we will discuss prototype validation and how researchers validated their prototype upon three stages which comprise of steps including testing the antenna in a microwave lab comparing with different designs with the same dimension or comparing with published paper on the transparent antenna finally we talk about antenna integrated with solar cell Figure 1 illustrates the main highlights of this review paper Main highlights of this review Show All This study adopted the systematic literature review approach It is an organized way to determine the literature of a definitive topic Such systematic review applies a scientific and systematic process to determine select and critically assess the related research samples to collect shreds of knowledge from past research work It offers different merits over conventional approaches as it incorporates the literature in a more transparent precise and reproducible manner This systematic review approach is well known for its compelling importance and its capability to integrate different types of research methods not only for researchers across scientific disciplines but also for students in postgraduate levels pursuing an integral objective in their research endeavor The process of a systematic review consists of several stages including identification of a research area search procedure criteria for research selection data extraction process and data synthesis In its core a systematic literature review aims at summarizing the topic at hand and identifying the gaps in the literature to position new research activities In the systematic review the studies mentioned are referred as primary studies The systematic review is also known as secondary study In search for targeted articles four digital databases were selected 1 Scopus database that covers the largest abstracts and literature reviewed by peers Scientific Magazines 2 Science Direct database that provides access to scientific technical and journalistic articles 3 IEEE Xplore Library of Technical Literature in Engineering and Technology and 4 Web of Science WOS that indexes research work across disciplines in both science and social science domains The selected databases are rich in numerous high impact scientific journals which demonstrate their academic resilience and scientific integrity thus considered adequate for this review The search was initiated in July 2019 by using the advanced search boxes found in the four selected scientific databases Science Direct Scopus IEEE Xplore and WOS Boolean operator i e AND OR and two groups of key words i e queries were used in the article search process see Figure 2 During the processes of searching and filtering the content of the articles was selected based on research and review articles This option appeared suitable to capture the latest and the most related contents to the designated topic of this review Flowchart of Search Query and Inclusion Criteria for Systematic Review Protocol Show All The research method began with a basic search that involved 829 articles These articles underwent the following processes 1 Duplication screening of articles 2 Scanning of titles and abstracts to discover initial importance of articles and 3 Full text reading and data extraction of each article to match the inclusion criteria of the protocol Some notes and comments were taken during the process which later turned into good insights that helped to shape the final form of this review During the data extraction process a range of elements were selected and keyed into the Microsoft Excel sheet Figure 2 illustrates the inclusion and exclusion criteria in selecting the articles The initial target was to classify studies pertaining to transparent antenna and solar cell into four general taxonomies With categories deriving from literature pre survey without restriction Google Scholar was applied first to understand the directions and the landscape within the literature Following the initial removal of duplicates the articles were removed in the iteration of filtering and screening upon failing to satisfy the eligibility criteria Some of the exclusion criteria are 1 non English articles and 2 articles that did not specify on antenna techniques but merely on general transparent antenna technology In order to simplify the steps ahead data collection was initiated The identified articles were grouped into some initial categories from various sources Four authors executed some full text readings This resulted in a large compilation of comments and highlights on the surveyed articles A much refined taxonomy was generated by classifying the articles The comments were then saved on the body of the texts relying on each author s chosen style softcopy or hardcopy Next the main findings were summarized tabulated and described A collection of important and related information was saved in WORD and EXCEL files inclusive of the full list of articles their source of database summary and description tables grouping tables based on transparent antenna review sources objectives target audiences and platform as well as other related aspects The datasets were then presented in supplemental materials as the complete reference in projecting the following results Initially 829 papers were gathered 261 papers from Scopus 150 papers from WOS 38 papers from ScienceDirect and 380 papers from IEEE Explore The articles were then filtered by publication date between 2015 and September 2020 Upon selection they were classified into three groups After filtering 330 out of the total 829 articles were duplicates Upon checking titles and abstracts 294 papers were further excluded thus leaving only 205 papers Next another 133 papers were discarded in the final full text review additional 9 articles where published when we are doing the review added to the total number of articles In total 81 papers were selected for this review in light of transparent antenna and solar cell integration as wireless communication techniques through various topics and approaches Based on the taxonomy portrayed in Figure 3 this study had reviewed the main streams of research prioritizing on transparent antenna techniques applied in wireless communication Taxonomy of literature on transparent antennas Show All This section presents the taxonomy applied in this study to cover all the development aspects of the transparent antenna In doing so the taxonomy was generated to identify the processes involved in developing a transparent antenna Figure 3 illustrate the taxonomies of the transparent antenna systems that portrayed the growth of studies and the applications used Analysis was performed for each class and subclass The first level of the taxonomy denotes conductive type of antenna that included articles that investigated the types of conductive used with various substrates for the transparent antenna system The second level of the taxonomy presents the transparent antenna type which included articles that described each antenna type of study The third level of the taxonomy describes transparent antenna or antenna used for many applications which embedded articles that looked into the applications in developing transparent antenna Figure 3 Taxonomy of literature on the transparent antennas Transparent conductive thin films are among the most common transparent conductors conductive polymers and Conductive inks such as silver coated polyester AgHT 4 AgHT 8 and transparent conductive oxides TCOs including indium tin oxide ITO fluorine doped tin oxide FTO Known for its high optical transmission relatively low resistivity non toxicity low material cost and good stability the Al doped ZnO AZO film is one of the most promising transparent and conductive oxide TCO thin films and Ga doped ZnO GZO Titanium Indium Oxide TIO and aluminum doped zinc oxide AZO silver nanowire AgNWs or silver nanowire AgNWs and graphene are popular transparent conductive thin films The thin films that can be applied in displays LEDs EMI screens and photovoltaic equipment are most popular is indium tin oxide ITO However there have been proposals for the future of indium shortages Furthermore ITO is fragile and so unacceptable in portable electronics as other TCOs A flexible alternative indium zinc tin oxide IZTO has been explored as an approach to enhancing its versatility An additional approach to improving the conductivity of the IZTO film was explored in by manufacturing a multilayer IZTO film where silver Ag was sandwiched between two IZTO films The IZTO Ag IZTO IAI multilayer film is a good conductor for the manufacture of flexible transparent antennas but its high sheet resistance 4 99 sq is responsible for poor antenna quality In the case of transparent conductional films the conflicting relation between their sheet resistance and their optical clarity is an essential weakness in antenna output The optical clarity of the film depends on its thickness for better optical clarity it is important to keep the film thickness very low However to keep the losses small film thickness should be greater than the skin depth The skin depth of thin conduction films is considerably higher than typical conductors due to its lower electric conductivity and the inversive link between skin depth and electrical conductivity So there is a trade o between the resistance of the layer and the optical transparency of thin films The third level of taxonomy deals with type of study and applications of the study Many conductive film materials have been used as the conductive layer in transparent antenna with varied substrates to design and fabricate the transparent antenna prototype such as conductive film antenna so many types of study for many applications like transparent Array antenna using quartz as a substrate with ITO for solar applications And Mesh antenna using a and transparent acryl substrate with multilayer film MLF IZTO Ag IZTO for Wi Fi application And as Compact and Transparent Antennas using a plexiglass substrate with AgHT 8 for 5G communication systems or soda lime glass substrate with fluorine doped tin oxide FTO for wireless local area network WLAN and as Optically Transparent Antennas using glass as a substrate with indium tin oxide ITO for solar energy harvesting or for car network communication application or using glass as substrate with AZO AgNWs for Bluetooth communication or using glass as substrate with AgHT 8 for WLAN and wireless applications or using glass as substrate with AgHT 4 for Wi Fi or using a plexiglass substrate with AgHT 8 for wireless multiple input and multiple output MIMO system and smart devices or using a polyimide as substrate with TIO Terahertz communication and satellites radar systems and using a MWCNT loaded ITO and TIO for Terahertz communication and using a borosilicate glass substrate with FTO or Pyrex glass substrate with FTO for outdoor applications such as transparent antenna over solar cells and using a Polyethylene Terephthalate PET substrate with AgHT 8 thin film for RFID tags and UWB home entertainment network commercial and medical applications and using a sapphire substrate with gallium doped zinc oxide GZO thin film for smart city concept and using a quartz substrate with graphene layers for practical applications or using Perspex acrylic dielectric substrate with AgHT 8 thin film for wireless applications or using a glass substrates with ITO films for millimeter wave bands or using a plexiglass substrate with AgHT 8 short range multiband versatile wireless applications And can be as Transparent and Flexible Antennas using glass or polymer as substrate with AZO for window integrated wireless sensor node or using a polyimide as substrate with IZTO Ag IZTO IAI for wearable glasses and using polyamide substrate for computer laptop or using a Polyethylene Terephthalate PET substrate with transparent silver nanowire AgNWs for automobile window liquid crystal display organic light emitting diode display and wireless communications for wearable systems and using a Polyethylene Terephthalate PET substrate with AgHT 4 for WLAN and Worldwide Interoperability for Microwave Access WiMAX deployments and using a cellulose acetate substrate with ITO film for WLAN and Worldwide Interoperability for Microwave Access WiMAX deployments and using a Polyethylene Terephthalate PET substrate with AgHT 8 thin film for 5G applications In total 33 articles 41 96 had looked into conductive film materials and five type of study transparent Array antenna Mesh antenna Compact and Transparent Antennas Optically Transparent Antennas Transparent and Flexible Antennas to identify or to evaluate transparent antenna approaches Many conductive metal materials have been used as conductive layer in transparent antenna with different substrates to design and fabricate the transparent antenna prototype transparent mesh conductors are conventional metals usually copper or silver consisting of pores or gaps within their surface through which light may pass through them Unlike thin film conductors the optical clarity of the metals themselves should not be present in meshed transparent conductors Potential candidates for transparent flexible antenna fabrication are metallic mesh materials such as tortuous copper micromesh and silver grid layers AgGL However the trade o between the efficiency of the antenna and the mesh characteristics implemented for light transmission imposes a restriction on the efficient production of transparent antennas The third level of taxonomy deals with type of study and applications of the study According to the literature there are many type of study using conductive metal for transparent antenna like transparent array antennas using polyamide as substrate with silver bus bar and copper Cu for self powered wireless systems or Lexan as substrate highly conductive mesh wire for communication systems or solar cell as substrate with Aluminum for television cellular mobile and Wi Fi and with copper Cu foils for Ka band or using a fused quartz substrate with silver layer for microwave application or fused silica substrate with gold for indoor application or platinum cured thermoset silicone with metal alloy Eutectic Gallium Indium EGaIn for applications involving microwave to millimeter wave bands other than that transparent antenna can be as Circuit using a plexiglass substrate with silver ink for 2 45 GHz applications and can be as Compact and Transparent Antennas using a poly vinyl chloride PVC substrate with sensitive metallic layer material for vehicular communication spectrum and can be as Optically Transparent Antennas using a glass as substrate with mesh silver film printed for X band applications or a PET substrate with copper Cu micro meshed for indoor applications or with bulk Cu for internet of things IoT application Or with MMF copper is printed on the substrate for UHD TV Applications or using a quartz substrate with silver epoxy for CubeSats application and can be a Mesh Antennas using rear glass substrate with metal Cu mesh metal mesh film MMF for DMB service receiving application or quarter glasses substrate with MMF Cu for DMB service receiving application or using a Borosilicate glass with copper Cu for multifunctional solar antenna devices or using PET substrate with silver for WLAN system or using Lexan as substrate with Cu for many future wireless applications that are projected to use unlicensed spectrum or using a square lattice structure with wired metal mesh WMM Cu for wireless transparent usages or using a Rogers 6002 substrate with wire mesh for cube satellites CubeSats and Other Small Satellites or using a transparent acryl substrate with MMF Cu for Wi Fi Twenty three 28 39 articles had looked into conductive metal materials and five type of study transparent Array antenna Mesh antenna Compact and Transparent Antennas Optically Transparent Antennas Circuit to identify or to evaluate transparent antenna approaches Apart from conductive film materials metal conductive is also applied as the ground plane with different substrates to design and fabricate transparent antenna prototype The literature depicts the use of many conductive films and conductive metal in the same unit in the prototype Some instance includes Optically Transparent Antennas using Indium tin oxide ITO conductive film with Gold for ultra wideband applications UWB and Multilayer Film Antennas using polyimide substrate with Indium Zinc Tin Oxide IZTO Ag IZTO multilayer film and a copper Cu layer for wireless application as well as Semi Transparent Using glass as a substrate with AgHT 4 as the radiating element and Cu as the ground component for radar application Three articles 3 70 had assessed conductive films and conductive metal materials in light of transparent antenna and two type of study Multilayer Film Antennas Optically Transparent Antennas to identify or to evaluate transparent antenna approaches Liquid antennas are becoming more common because of their Functionality such as configurability fluidity and tuning Since the water represents 100 optically transparent material with very low material costs transparent antennas are designed in contrast to other clear materials such as transparent oxide conduction or transparent film An optically transparent water patch antenna consisting of the patch and ground plane was proposed as Optically Transparent Antennas using plexiglass substrate with distilled water for many promising applications in future transparent electronics designs and flexible electronics or for many practical applications and proposed as Compact and Transparent Antennas using plexiglass substrate with distilled water for ultra wideband UWB Three articles 3 70 had assessed liquid antennas water in light of transparent antenna and two type of study Optically Transparent Antennas Compact and Transparent Antennas to identify or to evaluate transparent antenna approaches Another possible use of transparent antennas is wearable technology in which wearable devices such as the antenna Antennas built with a range of materials have been reported to display several shortcomings in efficiency except for antenna made from conductive polymers that exhibited a promising potential as mentioned in the literature proposed as Optically Transparent Antennas using glass as substrate with polymer for x band satellite application and proposed as Compact and Transparent Antennas using sticky tape substrate with patterned conductive polymer PEDOT for ultra wideband UWB application and as Transparent and Flexible Antennas using Veil Shield and PDMS for Industrial Scientific and Medical ISM band or using a polydimethylsiloxane PDMS substrate with conductive fabric tissue for many promising applications in future transparent electronics designs and flexible electronics four articles 4 93 had assessed polymer antennas in light of transparent antenna and three type of study Optically Transparent Antennas Compact and Transparent Antennas Transparent and Flexible Antennas to identify or to evaluate transparent antenna approaches The literature depicts the use of other materials for the development of transparent antenna These include Array Antennas using a polymethylmethacrylate PMMA substrates with aperture coupled microstrip patch ACMP antennas for Ka band or using a extruded acrylic material for satellite communications or using a DRA transmit array for 12 GHz usages and transparent antenna can be a Mesh antennas using a Borosilicate glass with solar cell ground plane for multifunctional solar antenna devices or using a Fire Retardant 4 FR4 for cube satellites CubeSats and Other Small Satellites application or using a Roger s RO4003C laminate for antenna arrays application Other antennas transparent antenna or antenna integrated with solar cell can be using a Rogers RT duroid 5870 for a cubesat applications or using a Rogers RT duroid 5880 for solar cell or harvesting system and using a protective textile foam as a substrate with conductive textile materials for solar cell or harvesting system and glass or flexible polymer substrates In total 11 articles 13 58 had reported on the use of others material in light of transparent antenna and three type of study Transparent Array Antennas Transparent Mesh Antennas Other antennas to identify or to evaluate transparent antenna approaches The challenges and issues that the researcher s faces are some of the most common academic intellectual dilemmas Whether these dilemmas specifically related in the researcher s interest field or not they require additional attempts to solve them in order to facilitate research in the specific field Limited availability of appropriate materials One of the main challenges in the optically transparent antenna and sensor development The traditional transparent conductor s lack of many properties the challenge in materials will lead to other challenges in antenna size flexibility performance and antenna characteristics other than that will lead to other challenges in applications research design fabrication and integrated with other system Beside all the challenges mentioned before for each research theirs are many limitations motivations and recommendations make this field very interest for the future For the area of systematic transparent antenna within the domain of antennas Challenges are classified into several groups and grouped according to common features Figure 4 shows an overview of the challenges reported in the literature Overview of challenges Show All Some of the most intriguing issues related to transparent antenna are gain efficiency and radiation parameters Not only they are linked with the properties of the antenna itself but they are associated with antenna applications Hence generating an antenna with high quality parameters is indeed a challenge Most of the challenges in this area which revolve around gain efficiency and radiation parameters of transparent antenna are classified into three categories materials design and position of the antenna In light of materials category the fabricated graphene antenna gain and radiation patterns have yet to be identified while the graphene antenna is yet to be proven for its function as a half wavelength dipole antenna Transparent antenna made of AgHT 8 suffered from low gain Antennas built with a range of materials displayed several shortcomings in efficiency except for antenna made from conductive polymers that exhibited promising potential Lastly the thinner the sheet of transparent conducting material TCM the better is the optical transmittance Reducing the thickness of TCM led to skin depth losses Due to increased skin depth losses the overall antenna efficiency was adversely affected Next under the design category most antenna engineers tend to face problems regarding wireless communication related to the improvement of radiation efficiency A single side metal antenna especially wire antennas generates low efficiency due to high loss of metallic meshes despite the increment in transparency Other challenges in the proposed transparent antennas are electrical performance and radiation characteristics that either matched or exceeded the performance of conventional antennas Meshed antennas can be designed in different combinations such as patch antenna or both patch and ground plane made of mesh conductor Transparency is enhanced in the last case Leakage in ground plane can cause back radiation Lastly shark fin antennas suffer from radiation interference in limited space and low reception sensitivity Under the position of the antenna category the placing of the antenna was located on the display s right upper side Still circumstances revealed that the antenna radiation gravitated in the direction towards the ground This phenomenon is called ground effect Wireless communication has become a huge issue in the field of antennas especially with its flexibility to indicate if it can contribute towards being mechanically pliable while enabling high electrical conductivity It is one of the challenges highlighted in the literature Transparent antennas have become a major hit in the field of antennas despite lacking in flexibility characteristics Drawbacks of mechanical flexibility were reported in antennas except for antennas made from conductive polymers materials that displayed promising potential Conventional radio frequency RF antennas were fabricated by patterning metals on rigid substrates that cannot easily be strained in response to mechanical deformations such as bending twisting and stretching In order to address this issue mechanically pliable antennas with high electrical conductivity are required Antenna size is a good predictor in determining the quality of antenna Size indication was mentioned as part of the challenges in the literature Wireless communication connected to the improvement of size reduction is an issue faced by antenna engineers especially in some applications such as RFID Reduction in antenna size leads to weak radiation efficiency Features of transparent antenna materials are an integral aspect that significantly contributes to the success and failure of evaluation especially in terms of quality assessments of wireless communication and applications Materials have formidable problems they are still prone to some challenges that stand as a barrier in the face of researchers The challenges in terms of materials are categorized into six parts 1 Replace traditional materials with TCM or new materials for the antenna 2 Difficulty for fabrication especially to produce thin transparent metallic film 3 Material performance limitation that limits research work on transparent antenna 4 Materials that affect antenna design 5 Costly and rare materials that have not been assessed For instance although ITO is commonly used it is costly and brittle due to its rare earth indium component and 6 Materials that have not been examined in light of design and materials loss Referring to the application for microwave frequencies the addition of dense dielectric patch antenna DDPA can be applied to create patch antennas made of novel materials in order to focus on the different challenges of complex wireless communications in future In spite of the increasing interest in pursuing cost assessment of wireless communication especially the transparent antenna system the lack of research work in this field is disabling researchers They see it as a challenge to solve such an interesting problem where the absence of systematic cost analysis is the biggest challenge The challenges in cost are due to antenna design This cost related challenge must be addressed to enable green energy techniques as alternatives This is because most integrated wireless systems rely on batteries that suffer from various disadvantages such as substantial maintenance costs In order to limit beam steering speed in mechanical motor platform transparent TA TTA can switch the beam in H plane at Ka band while concurrently increasing the cost of RFID Fabrication of transparent antenna design ascertains sufficient content quality and conformity with the current guidelines of the antenna Fabrication of antenna is a mechanism that ensures satisfactory antenna quality and effective prototype Transparent antenna fabrication has many methods and each mechanism has advantages and disadvantages based on varied inputs such as materials size and shape Referring to the latter definition it is clear how such a topic is vital to the quality of transparent antenna and wireless communication system Fabrication poses a huge challenge Conventional antennas are commonly fabricated using subtractive techniques that demand wet etching or lithography for structuring as well as vacuum systems for metallization via sputtering or evaporation These fabrication methods bear some severe disadvantages that limit the amount of research work such as the intricacy of fabrication smaller mesh features that lead to more challenges in fabrication a slight decrease in performance when compared to the simulation results in the dual band design due to fabrication challenges fabrication error that leads to outcome variance disagreement between simulated and measured results due to manufacturing procedure transparent sheet is patterned using laser cutter that generates heat thus causing the sheet to lose its conductivity while copper sheet is patterned using hand thus lacking precision Despite the increasing interest in transparent antenna and transparent wireless system source of literature is in scarcity particularly the dual band transparent antennas that are sparse in the literature For example transparent antennas based on stack film materials have received little attention Meanwhile in microstrip structures the electromagnetic fields reproduced tend to differ whereas no study has assessed the impact of two meshed layers on obtaining optically transparent microstrip OTM antennas in light of optical and electromagnetic performances Transparent antenna design has witnessed improvement in terms of making the communication wireless reliable efficient hassle free economical and environment friendly However challenges are bound to surface due to design and materials Prior studies have pointed out the design itself materials performance and size as some challenges faced in building a transparent antenna These challenges turned the design at some point anemic Researchers need to address the challenges in devising new devices or services that can empower those who may need them the most The literature depicts that the challenge related to design is mostly found in 1 materials especially when substituting conventional non transparent material with TCM Besides transparent antenna that employed fabric tissue on a PDMS substrate attained 70 90 transparency This result however lacked ground plane evaluation Moreover the enclosing environment can easily affect such antenna upon deployment Additionally slot placement on these locations can adversely affect yield due to the partial removal of the radiating element Most of these topologies were implemented on non transparent substrates such as Rogers Taconic and FR4 along with a simple rectangular radiating element and vertical slots for band notching Nonetheless the resulting resonant frequency depends on the length of the vertical slots This complicates its implementation at lower frequency as long slots are needed thus requiring a larger radiator footprint High material losses can lead to feeding line losses thus limiting the usage of such materials Water has been suggested with improved performance to use as patch antennas Yet optic transparent is incomplete as it demands a large underground metal ground surface Another good filter for transparent design is the usage of glass However the ground that supports glass is still a mineral that can affect true visual transparency to the entire antenna structure This limits its practical application as glass is not flexible The next challenge related to design refers to 2 achieving high speed network requirements for the antenna design Placing the antenna below solar panels on the main faces can hinder issues of light blocking This is to ensure that the solar panel above the microstrip antenna does not strongly affect its radiation performance into a challenge The efficiency of transparent microstrip patch antennas is measured at a few GHz frequencies Lastly the design of an acceptable antenna with well maintained performing characteristics has inspired researchers worldwide to work on the deployment of sophisticated modules It is challenging to design modern antennas that meet vehicular application demands such as impedance bandwidth gain and radiation performance Next the challenge related to design lies in designing 3 an optically transparent reflect array that must not substantially weaken optical illumination Impedance matched feeding network for mesh antenna design and the finite number of mesh wires for a given area have been identified as challenges related to design Microstrip patch antennas typically have high input impedance while feeding the antenna with a transmission line that has characteristic impedance is equal to the input impedance of the antenna may not be functional due to the finite number of mesh wires The sensitivity of the feed line s impedance increases as the number of mesh lines decreases Finally the optically transparent antennas is a rousing challenge that allows antenna printing on specific surfaces such as car windows and building glass For challenges related to design found in 4 size antenna installation and presence on small satellites bring some constraints for the antenna In fact mounting solar cells antennas and other scientific instruments through making satellites smaller have evolved into a challenge Numerous wireless devices are yet to be launched due to the physical limitations of locating antennas Wireless communication systems employ multiple antennas to facilitate applications that require high capacity bandwidth and gain Thus bulky structures increase spatial constraints for space applications The future of wireless communication depends on the system and its reliability to run independently One common way is to run the system using electrical power from varied sources However sources may have many disadvantages including cost environment unfriendly as well as high cost maintenance and repair As mentioned before reliable sources that are cost efficient environment friendly and less maintenance are highly sought One of these sources is the solar panel which has been used to run the wireless communication system by integrating the system with solar panel using transparent antenna system This integration however has led to many problems and challenges classified into several categories The first category is linked to 1 performance The integration of antennas with solar cells should not affect the aspect of performance thus posing as a primary challenge The second category refers to 2 materials An opaque metallic antenna incurs shade on the photovoltaic cells that can hinder access to light by the solar cells Most of the flexible antennas depicted in the literature were fabricated on non transparent conductors such as copper silver and gold thus placing them on an electrical circuit e g solar cell could reduce the system efficiency and degrade the system functionality The third category is linked to 3 design Due to their special properties most of the approaches are not applicable for meshed patch antennas combined with solar panels The integration of antennas with silicon solar cells of amorphous a Si and crystalline c Si was reported although minimizing antenna footprints and solar shadowing related designs still pose as challenges The fourth category is 4 size which refers to the limited surface area on small satellites The surface area of antennas test instruments and solar cells can affect the satellites due to size and weight A major challenge in small satellite is to fully utilize its limited surface area Transparent antennas are generally created to maximize the solar panel surface area which is vital for efficient harvesting of solar power and the space vehicle will be able to operate for a long term The last challenge category is 5 integrating solar cells antennas and other elements deployed in limited space such as CubeSats or even smaller satellites Integrating optically transparent antennas with solar cells is one of the toughest challenges to install wireless sensor nodes in modern settings as most small sensor nodes are run by battery The literature depicts many other challenges faced by researchers within the transparent antenna domain and some challenges cannot be contained into classification One of the toughest challenges in embedding wireless sensor nodes into modern gadgets is still power procreation such as achieving the galvanic approach to transparent conducting oxide TCO However designs with lower frequencies e g 900 MHz are more challenging and crave special measures to gain adequate radiation efficiency as well as to perform electrical connection to the antenna Designing optically transparent antennas and filters for smart city applications is also considered a challenge which includes infrastructure and network capacity Shorter broadcast distances and network dead zones would appear in frequencies for the next generation of wireless network 5G The requirement of strict network security for cyber physical system security in this forthcoming network poses a challenge Achieving an interconnected system among buildings electrical grids end users and transportation services is bound to alter the existing infrastructure that can potentially create communication dead zones within cities Reflection and scattering will inevitably cause signal degradation challenges to interconnections for open air systems as well as transitions between outdoor and indoor communications These create challenges for the upcoming 5G network Also there is the urgent need to incorporate green energy techniques as alternatives mainly because most integrated wireless systems rely on batteries that suffer from various disadvantages such as charging losses low charge and discharge cycle life overheat failure substantial maintenance costs and intelligent power management systems Due to the huge potential of WSN or IoT devices some being inaccessible battery replacement is often not feasible or time consuming Through integrating switches or varactors these periodic apertures are typically tuned electronically Electronic tuning however suffers from some disadvantages such as high cost and limited to military applications particularly in large scale applications Lastly some antennas are designed for varied locations such as an aperture in the rear end of the roof for FM radio and roof cavity Thus seeking a viable antenna position while reducing its visual perception and design consideration is challenging For many scientific reasons researchers are drawn to their respected fields Various meanings and motivations reflect the concern of researchers in this area in light of systematic transparent antenna over antennas Researchers are keen in transparent antenna and transparent wireless system due to the huge benefits offered by this field for deployment on this planet Earth and the outside space a breakthrough in space and earth sciences In the field of transparent electronics optically transparent antenna and transparent electrodes have garnered interest Researchers are also interested in transparent and conductive materials Novel materials for antennas are an appealing topic amongst technology companies and research institutions Figure 5 lists an overview of motivations for pursuing transparent antenna studies Overview of motivation Show All In light of transparent antenna and wireless communication system gain efficiency and radiation parameters are essential as they encourage interaction between system and users Researchers worldwide have proposed many strategies to enhance gain efficiency and radiation parameters in transparent antenna These strategies benefit the TCO in shaping multilayer electrodes such as organic light emitting diodes organic solar cells and thin film transistors due to its exceptional conducting efficiency The dual inverted F array increases gain while beam switching allows the array to sweep a wider coverage angle with larger beam widths The transmission line and the cell of a grounding strip with 6 mm width is sufficient to isolate the transmission line from cell lattice thus displaying improvement in gain efficiency and bandwidth Strained wavy NW networks created by the antennas display higher efficiency of radiation and smaller return loss when compared to antennas developed by straight NW networks The cyclic deformation tests show improvement in stability An antenna performed at 2 2 25 0 GHz band resulted in efficiency above 75 during the operating band apart from exhibiting adequate gain and good radiation pattern The high demand for wireless technologies has led fabrication of antenna to confirm its performance in terms of radiation pattern and reflection coefficient In order to raise the low radiation resistance of a single element antenna made of conductive oxide with low conductivity a dual band OTA array is introduced Finally an effective solution for a trade off between gain and transparency a heterogeneous structure was used on the basis of a practical approach to increasing the efficiency of a transparent UWB antenna using gold nanolayer deposition Different advantages related to the result are offered in the field of constructing transparent antennas Better concept and efficiency effects contain these Besides researchers have great interest in antennas designed with transparent materials The practical realization and empirical measurements for a single patch antenna element with higher optical transparency as reported in did not exhibit the common shortcomings noted in transparent materials for this particular method This is especially relevant for printing the design using the circuit in plastic CiP technology for embedded electrical components in which absence of visual clutter and network coverage are some advantages and pose as an efficient solution to transparent antenna as fitting of antennas is enabled For communication systems to cover more diverse locations visibly transparent antenna arrays create favorable circumstances due to their transparency and power steering ability Other benefits include a promising solution for solar panel applications and the introduction of a design paradigm for body worn applications One solution is to generate electricity from temperature gradients on a window In favor of making the antenna suitable two types of antenna have been proposed in addition to preparing spatial beam and directional diversity to receive power The optically transparent antenna enables the set up in a wider space The transmitarray TA is optically transparent and has the potential for beam steering in H plane as well as robustness mechanical A multi port antenna can also increase RF power Transparent antenna and wireless communication are imminent costs to meet users needs The literature of transparent antenna has listed varied benefits pertaining to cost including the step by step simple and low cost alternatives In designing low cost transparent antennas increment in multiband frequency helps to achieve various kinds of sensor nodes to create wireless sensor networks that perform different sorts of sensor nodes to pattern wireless sensor networks The proposed TA is optically transparent and low in cost The FTO can replace the costly ITO as it also has good conductivity and is useful in outdoor applications Nanotechnology has managed to decrease the production price of transparent antennas along with suitable transparency performance and inexpensive fabrication Due to its low cost compact and robust features any mission requirement can be easily fitted with the antenna Transparent antenna materials significantly contribute to the success and failure of evaluation especially in terms of antenna fabrication for a wireless communication system The materials that make up the system dictate the antenna performance Transparent polymer materials are a highly flexible fabric and transparent with the potential of integration As mentioned before the possibility of optical transparent antennas made of GZO is astounding In the design of an optically transparent reflectarray using TCO ITO film is the most broadly used material It is a perfect choice due to its light transmittance conductivity and processing technology of the four materials ITO AgHT ultrathin metal film and metal mesh antenna Antenna designs have intensively enhanced performance using emerging materials to minimize the size for compactness and for multiple wireless functionality In light of a reasonable trade off between optical transparency and conductivity the frequency agile antenna is the first optically transparent made from such material The antenna exhibited low visual impact at the near point viewing distance low electrical resistivity high optical transmittance and physically stable with ITO and FTO thin films used on polyimide substrates For different applications that crave transparency and conductivity FTO film material can be used Meanwhile graphene based optically transparent antennas can be used for practical applications Silver tin oxide is preferable in transparent antennas due to its acceptable value of conductivity and optical transparency over other conventional antennas Transparency of the antenna has a trade off with conductivity value where higher transparency means lower conductivity and vice versa Using the magnetron sputtering an AZO layer was deposited on spin coated AgNWs films to enhance the electrical performance of the AZO film Transparent conductive films TCFs are optically transparent and electrically conductive films thus their widely usage Wireless communication has become one of the big fields in the world especially the people need it for many uses and since everything now connects to the internet rase the need for more flexible system can adapt in anywhere One of the advantages the researcher looking for in their design is flexibility can contribute towards by bring more application while bending the transparent antenna show high mechanical flexibility without performance drop flexible and wearable electronic systems have promising potential especially for conductive polymers materials promising potential in the literature It can operate with different curvature angles at UWB frequencies In accomplishing a universal communication society wearable watches and glasses are anticipated in playing a crucial role Also allowing active RFID tag to be flexible low visibility compact modular and lightweight The capabilities of processing sensing and decision making will be equipped as well it potentially suitable for future flexible transparent electronics and wearable devices by using ITO indium tin oxide offers enhanced film brittleness and moderate sheet resistance The arrival of mobile communication has made antennas ubiquitous Traditional methods of manufacturing antennas will not be able to provide the increasing request for novel applications as it requires both flexible and transparent antennas The flexibility addition to these mentioned antennas ensures that it can be applied on the surface which will not have planar geometry The flexibility to such antennas makes sure that it can be used on the surfaces are not having planar geometry Manufacturing a transparent antenna ascertains that the quality of the antenna is in line with present antenna guidelines Antenna manufacturing is a mechanism aimed at ensuring satisfactory antenna quality and optimal prototype operation The proposed method which involves precipitation of homogeneous and heterogeneous gold nanostructures via DC deposition does not contain the common limitations of transparent materials The entire structure becomes optically transparent except for the small central feeding probe Due to water use the proposed transparent antenna is less expensive and easier to manufacture than previously reported transparent antennas The goal of the investigation is to find laser parameters that remove the Cu coating at maximum optical transparency of the ablated area as well as to identify the relationship among laser parameters designed antenna layout and generated antenna geometry At the optimized laser parameters a linear relationship between the designed layout trace width and the generated trace width was found The relationship enabled the production of antenna geometries with high precision that operated at the designed frequency Optically transparent antennas printed on transparent substrates present an alternative route to circumvent such problems These antennas need nevertheless to be fabricated from transparent and conducting layers This reflects an effortless method for the fabrication of transparent antennas instead of the costly direct copper spattering on the glass Minimum antenna size plays a worthy sign in the applications and a good predictor of appraising the quality of the antenna When a transparent antenna is electrically small it minimizes the unit volume enables portable deployment reduces space consumption and increases aesthetic values of the wireless system installed due to its high transparency Plenty of research work had attempted to make the size of the antenna smaller while establishing high efficiency and transparency using conductive oxide materials At the constant need to design transparent antennas in small sizes for multiband frequency application IoT addresses the requirements and offers capabilities for the modern concepts of wireless sensing such as tiny electronics components which are implemented for different types of sensor nodes to form wireless sensor networks Upon considering stringent size restrictions involved in CubeSat designs the proposed solution provides significant overall system size reduction and design flexibility The automotive industry has reported a significant growth in network protocols and miniaturized electronics in combination with vehicular communication modules Mobile communication advanced techniques are embedded into the automotive systems to give accurate information and road safety precautions to drivers at speedy data rates The future of wireless communication depends on the system and how it can reliably run independently One of the most comment ways to run the system is by using electrical power from different sources With the rapid development of autonomous communication systems antennas integrated with solar cells have received much attention from researchers as such a combination may save valuable real estate With the use of highly efficient and low voltage boost converter even in low light conditions the cell can still power the sensor and offer battery charge Another option of power source with solar cells and accomplished with antenna integration refers to A Si cells which are flexible low in cost and can be easily fitted by trimming it into any shape desired Transparent antennas have attracted a lot of attention these days due to their potential technical applications It can be integrated with solar cells SOLANT to achieve small but higher performance satellites also designed and integrated with solar cells One method to solve this problem is to use an optically transparent microstrip antenna on top of the solar cells Employing a meshed shape conductor is another method for transparent antenna applications The goal is to test the reliability of Wi Fi systems with solar panels So the solar source can be applied to supply the transparent Wi Fi antenna in a real environment that ultimately moves towards green energy and sustainable technology A Wi Fi device connects to the network via an access point A Wi Fi system provides wireless network communications between computers and other portable devices by placing fixed access points over a short distance through WLANs Currently the main source of power generation derives from fossil fuel which is not sustainable With the increasing price of fossil fuel more sustainable energy can be harvested from the sun solar energy signifying green energy The balance between antenna and solar cell functions is maintained Solar cell energy harvesting is not significantly affected by the presence of reflectarray antenna when the light source is normal to the reflectarray aperture surface The focus is hence an alternative design where the antenna is placed on top of the solar cells The work continues to propel the research on reflectarray antennas integrated with solar cells a Ka band reflectarray integrated with solar cells is presented to achieve better antenna and optical performances by integrating the external section of mobile devices on such components or by placing such structures below the solar energy harvesting components Such design enables the semi transparent feature which may be suitable for future solar energy harvesting feature in compact devices In order to fix the captured RF energy into a useful DC power that can be utilized by combining it with the solar panel output an RF rectifying circuit is created Transparent antenna design can be an option to achieve newly phased arrays on the solar panels and other beam steering systems However similar CP design has not been achieved for meshed patch antenna to be fully equipped within commercial solar panels for small satellite applications When it comes to transparent antenna and transparent wireless system performance linked to system using the antenna is important to both user and applications The literature depicts that antenna performance has garnered attention amongst researchers especially if the antenna made is transparent The resulting antennas show excellent performance under mechanical deformation due to the wavy configuration which allows the release of stress applied to the NWs and an increase in the contact area between NWs The TA unit offers 300 of phase shift and lower insertion loss at 28 5 GHz Both measurement and simulation results present a wide angular of beam scanning besides high gain and low sidelobe level The TA is designed from meshed double circle rings to avoid degradation transparency while keeping the good transmission characteristics at Ka band application For small RF passive components at low performance levels the antenna can be used as the measurement results reported in Lastly the worldwide researchers motivation increased as they try to make this world a better place to live in However some of this motivation cannot be grouped into categories One huge motivation refers to the main radiators made of water which is available everywhere One of the promising materials for the transparent antenna is water due to its merited advantages over the other transparent materials Besides being environment friendly and easily available water is optically transparent A transparent antenna takes advantage of spatial extensibility more so than the other antennas in terms of the wide range of usable area The number of installed antennas is indeed growing Transparent antennas give more degree of ability in the installation as they can be located in the window photovoltaic panels and even with light sources Research limitations are one of the most common academic dilemmas that a researcher can face Whether it is directly or indirectly related to the field of research more efforts will be needed to address it and enhance scientific knowledge in this field In the field of systematic transparent antennas research deficiencies are distributed through different groups and grouped according to specific general features which makes it easier to identify from potential researchers Some categories linked to research limitations are 1 materials 2 fabrication 3 design 4 flexibility 5 antenna characteristics 6 application 7 combination with solar cell and 8 others Figure 6 presents an overview of research limitations Limitation overview Show All The estimated average conduction efficiency of nearly 90 indicates that this type of antenna can overcome the limitations in conductivity and thickness of conductive polymers Integration of conformal antennas with solar cells is particularly valuable for CubeSat communication as the antennas when strategically integrated with solar cells do not compete for the limited surface real estate and dismiss mechanical deployment that is limited by the progress in material engineering Under similar conditions conventional TEGs are able to back up a low part of the temperature thus making it close to being non useful in applications where efficient heat sinks cannot be used Kapton CS colorless has limited availability as yellow Kapton NH has been used to make the TEG prototypes It has created certain limitations on possible antenna structures As mentioned before in order to reduce the conductivity limitation the distribution of electric current should be as even as possible on the antenna which means modest structures without meandering The fundamental limitations on the conductivity of the materials may result in lower microwave performance Considering how there is limited conductivity in AgNWs it is believed that AgNW lines will result in insertion loss depending on the frequency of its signal In metallic patch the gain is not high due to dielectric loss in distilled water as well as small sized the water ground plane Standard PCB printing technology limitation is to lower ohmic losses For all patch configurations the phase responses can be seen to match rather good at 26 GHz The simulated and experimental inconsistencies are mostly caused by manufacturing tolerance setup errors and prior limitations of the ITO surface impedance model Due to fabrication errors and resistivity difference of the semi transparent material the measurements indicate slight degradation at the upper frequency The reason of simulated and measured gain having a difference is because of the unavoidable measurement system and fabrication error It tends to also cause some unpredictable deviations likely due to the existence of possible air bubbles in distilled water The proposed antenna and conventional laminator antenna were compared and revealed limitations in the conventional microstrip patch antenna that had a narrow bandwidth and low gain The small satellite antenna design displayed some limitations such as limited surface area limited antenna mounting positions and a spreading mechanism A dual band CP antenna metamaterial based on the reported raw material was denoted as inappropriate because its second antenna for embedded materials contained within a traditional patch that occupies a large area and reduces antenna transparency This technology cannot be used for flexible and transparent antennas because there is no short flexible transparent pin and electrical conductor However MWCNT can overcome these limitations thus used to design a flexible and visually transparent microstrip patch antenna Due to its transparent property it can also be deployed without compromising aesthetic values or users noticing especially for covert operations Since this is a pulsed system the lower gains in a limited higher frequency band will not severely affect its overall performance in real applications The measured gain is slightly smaller than the simulation value due to some air bubbles in the water bearing the fabrication as some electromagnetic waves can leak through the water earth to the back direction Our technology also sets limits for higher operating frequencies 100 GHz and above In this case the degree of the network will need to be reduced at the expense of optical transparency The PET insulating polymer membranes are used for this research and the effect on the electrical properties of invisible antennas was limited to the thickness of the submillimeter film layer Meanwhile antennas made from glass which are created to use in military FM radios and inside aircrafts suffer from low gain This is especially practical for end fire antennas and may have limited applications for wide antennas Antenna integration at the top of the solar cells provides the required coverage for many applications but it may reduce the efficiency of the solar cells due to the effects of antenna shading The blockage that the antenna brought on the solar cells which reduced the amount of solar energy harvested into the system was the main limitation The receivers come in two basic structures direct detection and ultra high contrast Direct detection feeds an incident RF signal to a carrier frequency detector usually including a limited amplification stage Transparent arrays will be able to solve the demand of both improved system capacity and eradicate the location limits where antennas can be set up and viable set ups include windows of buildings and cars This Section offers suggestions for future attempts of transparent antenna The subsection is composed of four major aspects design and fabrication materials research as well as application and solar cell Figure 7 illustrates an overview of the recommendations and future work Recommendation future work overview Show All Future work includes improving peak gain by analyzing different types of network structures through which the antenna can be manufactured as well as examining the existence of mutual coupling between the two antenna elements when adapting into an array The antenna displayed overall good performance while the bending effects were determined via measurements and simulation The antenna elements were developed using a laser machine The research revealed that the planer dipole antenna can be fabricated using GZO Transparent antennas are more practical but it needs to be researched intensively This demands other deposition techniques to fabricate the employment of another transparent substrate material The future of transparent transmitters include the potential of GZO antennas as materials In near future PEDOT thin films with even thinner thickness are expected to improve efficiency conductivity and uniformity In future copper may be replaced as a metal layer in the traditional antenna fabrication process where AZO AgNWs stacked films can be used and could be extensively tested in communication antenna An impedance bandwidth 10dB of 10 13 and 7 6 was yielded respectively by ITO and TIO based transparent antennas This broadband width 45 allows future inter satellite communication system designs Optical and electrical applications of the transparent material may be incorporated into wearable devices in future There is an opportunity for printed multilayer antenna designs by making use of AuGL which is transparent and conductive material to accomplish efficient microstrip systems under the band of 60 GHz for indoor applications In order to rectify the wideband RF energy into DC output the wideband rectifier can be applied The solar panel and the rectenna systems DC output can be placed together to obtain a diverse dual source energy harvesting solution outcome This novel transparent patch antenna has plenty of potential applications on flexible electronics and transparent electronics design in the future due to its low cost and outstanding performances in radiation The TA unit is the kind of antenna that offers a visual impact that enables 5G application as a small unit and smart street equipment Scan loss and sidelobe level of the beams as well as the design are fully electronically switchable focal array that will be a hub for future work improvement In future work the microwave frequencies defined in this article aim to fabricate and examine different GZO filters that were formerly designed for military medical civilian applications as well as for near infrared and optical frequencies Upon anticipating the future TCO reflectarray will have plenty of interests and promising applications As the technology of TCO material is constantly improved integrating the TCO material technology into designs of high gain planar antennas will increase attention Transparent planar reflectarrays will become attractive options for plenty of future technology ventures as TCO reflectarray performances hold promising demonstrations as well as the increasing attraction of this technology in satellite and aesthetic applications As a result of its low thickness lightweight structure and high transparency characteristics many RFID applications have been enabled such as supply chains warehouses and access controls The designed antenna set up for its semi transparent feature is suitable for future solar energy harvesting feature in compact devices The proposed antenna is a good option for independent communication systems The reason is that it has potential advantages of non shading wide resistance flat structure and width of CP radiation ranges high radiation efficiency and high gains This leads to the conclusion that the transparent solar antenna is a reliable source for Wi Fi applications This is one step to achieving green and sustainable energy technology goals The antenna commercially has potential to use for other applications and for integration in smart devices In the near future it is expected that many applications will be linked with the proposed TCF antenna in transparent devices In general both space and design freedom needed to establish future automotive antennas can be offered by utilizing transparent antenna on the rear glass Its great features such as being light low profile highly transparent and flexible have made it a suitable candidate for applications where an antenna needs to be placed on non coplanar surfaces or wearable electronic systems Promising structures has been shown by the transparent microstrip antenna with FTO layers due to its ability to be have solar cell integration and usage in applications that require concurrent connectivity and visibility The solar ground plane has harmful and intrusive effects on the antenna performance as well as the dissipation in the transparent conductor which enables the integration of transparent correction antennas with solar cells In this case GaAs appears to be suitable as thinner cells Thus the design is a breakthrough in the realization of the IoT paradigm This will be especially suitable for unobtrusive integration within a clothing making use of extensive analysis between wearers and their environment interacts by wirelessly transmitting physical information about their wearers and their surrounding environment to the Internet through a question and answer protocol In the existence of human hand phantoms due to its change in resonant frequency it can be improved by future work of invisible antenna geometry developments There are many ways for using TCFs for transparent devices Generally both space and design freedom are needed to establish future automotive antennas by utilizing transparent antennas on quarter glasses For the sake of improvement of optical and electrical performances in transparent antennas it is vital that research work investigates the different materials composite of TCFs This improvement is applicable for the near future of transparent mobile devices In the realm of the systematic transparent antenna in wireless communication content is considered as a significant influencer within this domain radar imaging and localization applications have arisen significantly in wireless communication systems whereby reflectarray with solar cell integration radiation performance can be greatly enhanced to acquire optical region for its solar harvesting performance A persuasive effect on matching antenna impedance to the antenna can be found on the water patch antenna It has sufficient optical transparency and ensures the efficient performance of the photovoltaic cells along with an antenna that has promising bandwidth and radiation efficiency The design of the proposed TEG has a great advantage due to its ability to minimize heat leakage through its own module and in this matter under heatsink limited conditions the ability to maximize available temperature gradient The use of homogeneous gold nanolayer deposition design showed a significant improvement in gain but reduced transparency The transparent Silver Nanowire AgNW dependent antennas displayed an unusually high radiation efficiency of about 50 with 85 increment in transparency The only difference is the minimal use of metal in the top layer without significantly affecting radiation performance and antennas bandwidth while increasing its efficiency Design concepts are a tool that offers customers stronger and more reliable experience The two forms of concept are general and basic The methodology of design refers to the development of a single situation system or method To date the term is most often applied to technological fields about design software simulation and systems design The methodology can change depending on what the researchers want to achieve such as a single methodology using one method software and equation or hybrid methodology using multi method software and equations Figures 8 list the steps to design a transparent antenna using CST or HFSS Methodology Steps to Design the Antenna Using HFSS or CST Software Show All Both CST and HFSS are 3D simulators created with different mathematical techniques High frequency structure simulator HFSS is a finite element method FEM and more accurate in designing antennas while Computer Simulation Technology CST relies on FIT but popular with antenna designers due to simulation ease For frequency domain based simulation HFSS is better than time domain simulation Meanwhile CST is more user friendly and more applicable than HFSS Figure 9 Shows the simulation software depicted in the literature Bar chart of software simulation depicted in the literature Show All The 3D EM analytic package is a high performance analytical package for electromagnetic components and systems to design analyze and optimize outcomes The CST Studio Suite contains electromagnetic field solvers for EM specific applications in a single user interface The literature depicts that CST was used to design and simulate transparent antenna in 20 out of 81 articles 24 69 Figure 10 shows Bar chart of CST software simulation reported in the literature Bar chart of CST software simulation reported in the literature Show All The HFSS is one of many commercial methods used to design antenna systems as well as to design complex electronic circuit RF components such as filters and transmission lines The HFSS was used to design and simulate transparent antenna in 30 out of 81 articles 37 03 Figure 11 shows Bar chart of HFSS software simulation reported in the literature Bar chart of HFSS software simulation reported in the literature Show All Some researchers used multiple software packages to design and optimize their design of transparent antenna like HFSS and CST and HFSS with Analysis Systems Savant ANSYS HFSS with human tissue model simulator Sim4life DYMSTEC and HFSS with MATLAP Other software types had also been applied to design transparent antennas including commercial software ANSYS with MATLAB Analysis Systems Designer ANSYS and 3D electromagnetic simulation software Eight articles 9 87 had reported on the use of other software packages to design and simulate transparent antennas Meanwhile the remaining articles did not reveal the software used to simulate their transparent antennas This section lists the techniques and suggestions for the domain s future Most papers addressed the importance of further studies to explore various techniques in order for the transparent antenna to improve efficiency and application Some of these techniques are Beam Steering Antennas MIMO Antennas and Smart Antenna The fabrication of a transparent antenna is a process to ensure the quality of the antenna based on antenna regulations The manufacturing of antennas is a mechanism to ensure the successful quality of antennas and the implementation of experimental activities Usually thin film antennas are manufactured using physical vapour deposition wet etching photolithography and screen printing techniques Such procedures are complicated and costly other than that for antenna production with meshed conductors complex and costly manufacturing processes are needed for transparent conductive thin films Therefore the conventional conductors demonstrated do not meet the criteria of low cost or flexible transparent antennas Conductive options for the development of low cost conformal transparent antennas are inevitable there are many indicated challenge above in the fabrication challenge According to the literature many fabrication methods have been used to fabricate transparent antennas Sputtering Laser Photolithography Printing Process Spray Pyrolysis Technique Physical Vapor Deposition PVD Process and other fabrication methods Sputtering is a type of process that removes materials on a solid surface as a result of transferring momentum between the surface and an energetic particle that is usually an ion The surface bombardment is commonly acquired by a gas discharge in a confined space between two electrodes In situations like this the negative electrode is constantly bombarded by the positive ions produced in the plasma Generally the gas employed is argon as argon ions are species that intrudes the surface Some types of sputtering are as follows DC diode sputtering RF sputtering DC Triode Sputtering Magnetron and Reactive sputtering The following are reported in the literature pulsed DC magnetron sputter RF magnetron sputtering and spin coating DC magnetron sputtering DC pulse sputter instrument RF sputtering technique DC sputtering RF magnetron sputtering and sputtering Figure 12 shows the sputtering process Diagram of sputtering process redrawn from Show All Laser cutting is a type of technology that uses a laser to cut materials Schools small businesses and hobbyists have started to use them even though this method is usually used for industrial manufacturing applications Laser cutting functions by directing high power laser output through optics Laser and CNC optics computer numerical control are used to direct the material or the resulting laser beam The commercial laser cutting machine uses a motion control system to follow the CNC or G code for the model to be cut into the material The laser methods found in the literature are laser milling machine LPKF Proto laser S LPKF Proto Laser S 124102 laser machine laser cutter laser laser patterning and high precision laser cutter Figure 13 illustrates the laser process Diagram of laser process Show All Photolithography which is also known as UV lithography or photolithography is a procedure used to precisely manufacture the pattern of parts on a thin layer or bulk of the substrate known as foil Light is used to present a geometric pattern from a light mask optical mask to a light sensitive chemical photoresist i e photosensitive on the substrate The list of chemical treatments will etch the exposure pattern into the material or allow the deposition of a new substance in the desired pattern onto the material under photoresist In more complex integrated circuits the CMOS chip may pass through the optical lithography cycle up to 50 times The PHOTOLITHOGRAPHY methods depicted in the literature are STANDARD PHOTOLITHOGRAPHY photolithography advanced lithography process and standard microwave lithography process Figure 14 presents the photolithography process Diagram of photolithography process redrawn from Show All Printed electronics can point to a wide range of technologies used to print electrical devices on a substrate A substrate is a technical term for any material printed on paper glass or cover Electronic printing technologies are still largely untapped Some of the major technologies include screen printing printing and ink Different types of materials processes and equipment are utilized in 3D object via additive manufacturing production 3D printing is also called additive manufacturing as many 3D printing processes available contribute in being naturally additive with several key differences in materials and technologies used in the process The types of physical transformations used in 3D printing are light polymerization melt intrusion sintering and production of continuous liquid interface Some printing processes depicted in the literature are silver ink printing process screen or inkjet printing methods 3D print printing printed and high precision screen printer Figure 15 displays the printing processes Diagram of printing process redrawn from Show All The spray pyrolysis mechanism creates a spray of various precursor solutions which can be a solution of colloidal or mineral salts The solution aerosol droplets are then heated very quickly in the heating system at certain temperature that crosses through several stages 1 evaporation of solvent from the surface of the droplets 2 drying of drops containing the dissolved solvent 3 annealing the precipitate at high temperatures pyrolysis 4 formation of small particles from the formation of a specific phase 5 formation of solid particles and 6 solidification of solid particles As a result of the highly reactive particles obtained after pyrolysis internal sintering is needed In the pyrolysis process of spraying the arrangement of uniform and smooth droplets from the reactants and controlled pyrolysis demand processes that require it The low cost spray pyrolysis technology is a predominant method by which porous membranes and membranes with high density and high uniformity of particles are achieved It is also a great way to obtain ultra small powders of small molecular size narrow size distribution high purity high porosity and large area A regular pyrolysis device contains a spray a precursor solution a substrate heater and a temperature controller The spray pyrolysis technique is depicted in Figure 16 illustrates the spray pyrolysis technique Diagrams of spray pyrolysis technique redrawn from Show All The physical vapor deposition PVD sometimes especially in single crystal growth contexts known as physical vapor transfer PVT determines the different methods of vacuum deposition that can be used to create paint and thin films The PVD is defined by a process in which the material travels from an adsorbent phase to a vapor phase and then returns to a condensed phase with a thin film Evaporation and sputtering are the most common PVD processes The PVD is used to manufacture components that require thin films for optical mechanical electronic and chemical processing Semiconductor devices such as thin film solar panels food packaging of covered PET films and balloons as well as cutting tools coated with titanium nitride for metalworking are some examples of the use of PVD Smaller special tools generally for scientific purposes were created despite the manufacturing done by PVD tools The PVD methods found in the literature are PVD process in high vacuum and PVD process Figure 17 presents the PVD process Diagram of PVD process redrawn from Show All Fabrication antenna design is a mechanism that ensures satisfactory antenna quality and a working prototype Many methods have been introduced by researchers to design their first prototype table 1 Below shows the fabrication using other methods Besides fabricating the antenna design and making the first prototype to validate the design researchers have compared their prototypes with the previous designs within similar category application performance and different materials This is to add to the body of knowledge The literature depicts many comparisons as given below In order to validate the design another design with the same dimensions using solid copper was fabricated In order to determine the antenna performance antennas made from IZTO Ag IZTO and Ag were designed and measured to be compared with transparent antenna arrays and system output voltage In order to compare antenna performance two antennas using AgHT 4 and copper were examined as well as with copper based opaque patch antenna opaque antennas using thin silver Ag compared with transparent IAI antennas copper patch antenna counterparts fabricated microstrip patch antenna with copper sheet and comparison with an array of solid copper Upon comparing previous designs with reported various transparent antennas the proposed antenna had less transparency in the literature but displayed peak gains below 2 6 dB The viability of effective transparent antenna built of mesh silver film was validated and evaluated Plastic antennas are viable at microwave frequencies This structure not only allows full exposure of solar cells to sunlight but also can enhance the antenna performance Testing of antenna is the step after antenna fabrication and validation Many methods and devices can be used for testing depending on the parameters one seeks to test on a developed design A network analyzer is an advanced and sophisticated instrument to measure the network parameters of a device It commonly measures the S parameter at high frequencies since the reflection and transmission of the signals are easy to measure Table 2 shows the Summary of articles used Network Analyzer to test and validated transparent antennas Figure 18 Shows the Network Analyzer Network analyzer Show All Anechoic chamber environments An anechoic chamber an echoic meaning non reflective non echoing echo free is a room designed to completely absorb reflections of either sound or electromagnetic waves The size of the chamber depends on the size of the objects and frequency ranges being tested The literature has reported the use of anechoic chamber to test antenna Figure 19 Shows the Anechoic Chamber Anechoic chamber Show All The literature depicts other methods to test the antenna such as FEM based electromagnetic solver Satimo Star lab system standard waveguide method SATIMO complex antenna measurement system and electromagnetic chamber SATIMO complex antenna measurement system When it comes to transparent antenna and wireless system user and the applications have an important role in the performance connected to the system through the antenna Antenna performance is based on many factors such as frequency gain physical dimensions radiation efficiency bandwidth substrate patch materials and transparency Table 3 lists all the factors mentioned before for transparent antennas based on different materials designs and applications Figure 20 21 Shows the materials based on substrate and the application based on type of study Materials and substrate used for transparent antenna Show All Application based on types of studies Show All Antennas have been integrated with solar cells considering adopting green practice Studies have attempted to achieve this point for various reasons such as cost size rebuilding of communication features in areas hit by natural disaster solution to battery issues and to realize the new 5G concept Many articles have proposed new methods and designs that incorporate solar cells The developed in this study embeds articles that have probed into antenna or transparent antenna integrated with solar cells for varied applications First the solar cell types are described and followed by antenna application with solar cell Figure 22 Shows the literature on antenna with solar cells In total 20 out of 81 articles 24 69 had assessed antenna integrated with solar approaches Literature on antenna with solar cells Show All Antennas have been integrated with amorphous silicon solar for Wi Fi x band satellite RFID and indoor applications Antennas have been integrated with monocrystalline and polycrystalline silicon solar cells for the following applications crystalline silicon solar cell for emerging applications polycrystalline silicon solar cell for satellite communications monocrystalline and polycrystalline silicon solar cells for outdoor applications Antennas have been integrated with silicon solar cells for the following purposes satellite communications CubeSat applications solar cell application Bluetooth antenna communication 12 GHz applications and small satellites Antennas have been integrated with GA AS solar cell for satellite and terrestrial applications as well as for CubeSat deployment Numerous studies have integrated antennas or transparent antennas with solar cells to assess their suitability in promoting green communication We find in the literature some articles show and talk about some effect between the antenna and the solar cell The previous sections have discussed many transparent antennas designed for different purposes Various approaches were adopted by the past studies to explain in detail the antennas that they had proposed and assessed In fact the number of studies related to transparent antennas has risen The literature clearly depicts that the antennas proposed by many researchers served a specific condition or application However the evaluation of transparent antenna is an emerging and important topic that must be considered The main contributions of this paper are a comprehensive survey and classification of work related to the evaluation of transparent antenna applications as well as methods used in the process The reviewed articles were grouped into four categories The first included challenges motives limitations and recommendations Second it is related to design and development including method software and technology used The third is associated with testing evaluation and validation including antenna manufacture antenna verification antenna performance and data analysis Last final segment discusses the effect between solar cells and antenna notes gathered from articles about transparent antenna and antenna integrated with solar cell These contributions enable better understanding of this topic for future endeavor"}
{"title": "Improving Data Security, Privacy, and Interoperability for the IEEE Biometric Open Protocol Standard", "number": "9303431", "authors": "[{'preferredName': 'Eduardo Magalh\u00e3es De Lacerda Filho', 'normalizedName': 'E. M. De Lacerda Filho', 'firstName': 'Eduardo Magalh\u00e3es', 'lastName': 'De Lacerda Filho', 'searchablePreferredName': 'Eduardo Magalh\u00e3es De Lacerda Filho', 'id': 37089334832}, {'preferredName': 'Geraldo P. Pereira Rocha Filho', 'normalizedName': 'G. P. Pereira Rocha Filho', 'firstName': 'Geraldo P.', 'lastName': 'Pereira Rocha Filho', 'searchablePreferredName': 'Geraldo P. Pereira Rocha Filho', 'id': 37089330884}, {'preferredName': 'Rafael Tim\u00f3teo De Sousa', 'normalizedName': 'R. T. De Sousa', 'firstName': 'Rafael Tim\u00f3teo', 'lastName': 'De Sousa', 'searchablePreferredName': 'Rafael Tim\u00f3teo De Sousa', 'id': 37282208700}, {'preferredName': 'Vin\u00edcius P. Gon\u00e7alves', 'normalizedName': 'V. P. Gon\u00e7alves', 'firstName': 'Vin\u00edcius P.', 'lastName': 'Gon\u00e7alves', 'searchablePreferredName': 'Vin\u00edcius P. Gon\u00e7alves', 'id': 37089636452}]", "abstract": "Enhancing security, privacy, and interoperability of biometric networks and protocols has been a challenge for many research works for many years. The several proposed approaches still need to integrate these three characteristics while showing security evidence for biometric applications. Therefore, this paper proposes a probabilistic scheme to encrypt biometric database indexes and a novel appro...", "text": "Enhancing security privacy and interoperability of biometric networks and protocols has been a challenge for many research works for many years The several proposed approaches still need to integrate these three characteristics while showing security evidence for biometric applications Therefore this paper proposes a probabilistic scheme to encrypt biometric database indexes and a novel appro Security privacy and interoperability are three challenging biometric networks goals Biometric Service Providers BSP which are the entities that provide biometric services on the network are used by government services banks trade associations and health departments among other areas In this scenario a sector uses different technological systems without a security and privacy interaction between the data or otherwise uses the same technique Also insecure and compromised biometric networks and databases can irreversibly lead to identity theft unless they are properly security proof In light of growing privacy concerns with regard to data usage countries enacted laws to protect personal data by imposing solutions to mitigate the problem of attacks and data leaks For many years the main approach adopted to ensure biometric networks security and privacy was applying biometric template techniques The work of Ross et al pushes a lot of achievements to deal with template protection using biometric cryptosystems and cancelable biometric feature transformations Some of these works also address authentication network security privacy data and processing power Unfortunately some of them also have a few drawbacks Moreover even with the guidelines of the IEEE Biometric Open Protocol Standard IEEE BOPS it is not possible to use each of these schemes in different biometric databases and make them communicate in a way that is reasonably security proof and does not compromise a person s privacy In addition IEEE BOPS does not have a message exchange that allows an integrity check of transactions within the biometric network The problem then lies in enabling different biometric networks to communicate with integrity while ensuring individual data security and privacy This proposed work resolves this problem by taking the following measures introducing a probabilistic encryption scheme conducting complete security analysis and evaluation and establishing a set of new and complete communication protocols to improve the IEEE BOPS framework The evidence shows that it is not feasible for the cryptographic techniques used to encrypt all the records in the BSP network and databases to be broken down in polynomial time and still preserve a person s anonymity This research also compares other biometric cryptosystems and feature transformation methods with the IEEE BOPS framework and this shows the innovative aspects of this paper more clearly The contributions of this work are summarized as follows A probabilistic encrypted index scheme to address data security and privacy Using contribution 1 a new API to improve the workflow of IEEE BOPS by enhancing the integrity and control of the exchanged data The first contribution runs an algorithm that creates an anonymous index called IDN It uses a secret key k unique social identification of a citizen CPF and a Time Code Number TCN The IDN uses the AES 256 CBC encryption scheme slightly modified by a random secure and local initialization vector iv and a nonce parameters Using their own certified Federal Information Processing Standard FIPS HSM embedded in an audit environment the different accredited BSP in the network can calculate and verify the same IDN string that represents the holder CPF of that biometrics unequivocally without exchange iv or the nonce The algorithm s goal is to ensure any person s anonymity with secure evidence including semantic security given by the probabilistic outcomes that prove and maintain this condition Our second contribution creates a new interoperability protocol to improve the IEEE BOPS API The new proposal creates an API based on HyperText Transfer Protocol Secure HTTPS and JavaScript Object Notation JSON messages The new API ensures interoperability and integrity between the biometric systems by exchanging the index IDN It is important to notice that IDN within the network exchange and the databases is not decrypted anytime The security evidence of the network procedures and results proves that the protocol is secure enough to interoperate any biometric package within the BSP The rest of this paper has the following parts Section II discusses the related works more comprehensively and examines aspects of security privacy and interoperability We put forward the new scheme in Section III In Sections IV and V we conduct a security analysis and show the results of a running instance of the established framework Finally in Section VI we summarize our paper s conclusions and make recommendations for future work Several studies currently investigate the security privacy and interoperability of biometric records We will comment on some techniques that can enhance biometric protection but to the best of our knowledge none of them has been able to integrate security proofs privacy and interoperability simultaneously as is the case with this research Demonstrating how this problem of biometric data leakage is an issue in a given population research conducted by Li and Zhang showed that almost 80 of the participants were afraid that personal biometric information is liable to be stolen after being used in verification applications Only 17 thought the verification carried out in the banking application was safe About interoperability some reasonable attempts at standardization have already been made and as a further contribution made by our work we will suggest a means of improving the IEEE BOPS framework Some factors need to be taken into account when seeking to make biometrics networks and databases secure and ensure data privacy The work on differential privacy carried out by Dwork showed that there is a great deal of auxiliary information that an attacker can obtain without accessing the database Thus it is essential to narrow down our understanding of what research is required to establish security and privacy Our work is clear about these goals i e it is to enhance data security and privacy within the parameters of the biometric network and database using cryptographic techniques based on security evidence Parts I and II from Lai et al set out some fundamental theories The authors describe the trade off among security privacy and key protection in any biometric template security system when a single use case of biometrics and when facing the reuse of the same biometric information in multiple locations The works deal with specifics biometric measurements for different approaches when generating the key in biometric authentication systems a non randomized approach b randomized approach Nagar et al proposes a feature level fusion framework It bases on a transformation embedded algorithm which makes a biometric feature into a new binary or commitment vault representation a fused module that combines homogeneous biometrics and a biometric cryptosystem that generates a secure sketch in the enrollment procedures The proposed work presents some security analysis for the created biometric template and improving performance The work of Nassir and Perumal uses the user ID and password with the biometric data extracted converted to decimal numbers When using symmetric and RSA algorithms the work encrypts and signs the data into one package At the database they decrypted this packet for decision analysis The work shows some performance evaluation without security analysis Rathgeb et al work proposes a Bloom filter based transforms to protect templates in face and iris samples It builds two matrices arranged in two dimensional binary code divided into blocks of equal size consisting of bits The transform h applies to map the binary column to its equivalent decimal value which locations were within the Bloom filters A hash function applies leading face and iris to had the same transform length and in the last step the transformed face and iris are bit wise fused to improve privacy The paper from Kumar and Kumar proposes a multimodal biometric cryptosystem based on two modes a feature mode b decision mode The construction consists of three phases i e a Bose Chaudhuri Hocquenghem BCH applied in the biometrics creating parity code a locking stage hash code computation performed on the biometric modalities and the unlock stage where the parity code regenerates using XOR coding The experimental analysis confirms the superiority of multimodal cryptosystems and decision level fusion Li et al describes a new security analysis proposing a multibiometric construction by using a fingerprint to encrypt the key By combining information theory and security the work uses triangulations features extraction and two levels of encryption one with hash functions and fuzzy vaults to bind the transformed fingerprint template and the other with Shamir s secret sharing scheme to split and store the hash values A decision level fused obtains the identity of a sample Kaur and Sofat fuzzy vault work incorporates a fuzzy vault multimodal biometric template approach The fuzzy vault is a combination of extracted minutia points fingerprint and face using crossing number and principle component analysis where the fused is the input vault for encoding and Lagrange interpolation to recover the vault key for decoding The proposed scheme shows that the approach yielded satisfactory performance and provides the claimed security Zhou and Ren work proposes a Threshold Predicate Encryption TPE using a functional encryption scheme An encrypted plaintext and a secret key associate with a vector using Inner Product Encryption and Predicate Encryption instances that leave the decryption a function value and not the plaintext anymore No sensitive information about the vectors could not be passive or active attacks Toli and Preneel work uses a pseudo identity authentication recorder of a bank s client With a client s PIN code the device encrypts and stores the package discarding the biometrics and the PIN For security requirements the proposal uses ISO biometric financial and cryptographic device standards Kaur and Khanna proposes a random distance method Considering the multimodal cancelable biometric template approach it generates a discriminative and privacy preserving revocable pseudo biometric identities According to the user secrecy the method mapped on the Cartesian space biometrics features and calculates the distance for some points The security analysis presents some resistance to known attacks The ANSI NIST packages and IEEE BOPS API reference the interoperability of components within different biometric systems It should be noted that there are studies that attempt to establish interoperability mechanisms such as Tolosana et al and Mason et al However no comparative study will be made of them in this paper because they are only concerned with biometric devices and data but not all the transaction workflow We describe the IEEE BOPS owing to the improvement made by our scheme to this framework The IEEE BOPS assures multilevel access control identity claim auditing process and enables interoperability independent of the underlying system The proposed architecture is built for pluggable components using neutral languages such as Representational State Transfer REST JSON and Transport Layer Security Secure Sockets Layer TLS SSL providing a client server communication interface The BOPS mechanism includes software a trusted BOPS Server and Intrusion Detection System The BOPS uses an API for interoperability purposes that will be the subject of this paper Sections 6 through 9 of the IEEE BOPS document describe the interoperability considerations including the API format The API runs a 2 way SSL certificate dealing with replay attacks controls the authentication procedures and JSON messages among the BOPS server and the client device It creates five types of procedures Assertion Role Gathering Multi Level Access Control Assurance and Auditing that hold the communication by a triple association of user device and session The API starts with a JSON error code message for connection calls Afterward the BOPS documentation describes JSON messages about initial setup devices and session authentication by certificate exchange and communication security including a QROpportunity that identifies the client application The Role Gathering part includes a descriptive flow of input and output parameters about the session s construction creation status data and termination Between the status and data session is where biometrics are enrolled and sent to the network The control mechanism triggers a binary response for the JSONObject that includes or not the data returned related to the security connection issue Finally the API proposes an assurance and auditing JSON messages for group actions read write on any set of data Although BOPS deals with details on security and authentication the problem lies in the fact that IEEE BOPS has no effective integrity mechanism i e between the status and data session in the Role Gathering The multi level control does not offer acknowledged messages about the object of biometric transactions It does not include dealing with the pending operations time out or bad connection recovery among systems for the biometric identification procedure Our research uses the ANSI NIST package and proposes an integrity subsection in the API documentation improving IEEE BOPS In this section we outline the following The probabilistic encrypted IDN scheme How the secret key is protected The proposal to insert a subsection in the IEEE BOPS API documentation An extensive comparison of our work and others regarding security privacy and interoperability Figure 1 represents the implemented architecture It shows that the IDN algorithm runs on the HSM of each BSP after the enrollment process That same HSM is where the secret key is protected It also presents the interoperability calls that occur between BSP and as will be shown it can be within the IEEE BOPS documentation The implemented architecture Show All The IDN generation scheme shown in Algorithm 1 uses the social number CPF the Time Code Number TCN and the secret key k as follows The CPF is expanded by concatenation by itself until it gets a 256 bit length The iv parameter is calculated by concatenation of k and CPF expanded resulting 512 bit string called y then we get x by applying x SHA256 y a 256 bit string after we divide x in two halves letting a be the first 128 bits and b the trailing ones and finally iv a b The CPF is padded until it gets 128 bits and it is AES 256 CBC encrypted using k feasible to be calculated for the BSP but with a unknown iv for an adversary as the iv is not transmitted over the network This yields z an encrypted 128 bit string The nonce parameter is calculated similarly as iv i e the TCN is shrunken for 256 bit string then concatenated with k resulting 512 bit string called u then we get v by applying v SHA256 u a 256 bit string after we divide v in two halves letting c be the first 128 bits and d the trailing ones and finally nonce c d At last IDN z nonce feasible for only the BSP to reverse finding the same z for any tuple IDN and TCN since IDN and TCN are the only parameters to be exchanged in the network IDN Algorithm Show All The same CPF when enrolled at different times it will generate distinct because each transaction have a different However the same CPF enrolled e g at different times resulting the tuples and will generate the same z as follows Computes nonce with Do resulting z Using Algortithm 2 if z is the same e g for the stored tuple and the incoming tuple it means that it is the same CPF Only the BSP that have k CPF and TCNi can reach the same z for the same CPF All those cryptographic calculations are done within the cryptographic module of each BSP HSM Z Algorithm Show All We will describe how this proposal deals with the life cycle and protection of the secret key k For generation purposes we create a 256 bit random k into an offline HSM After creating it we export k by using RSA OAEP encryption operations with the public keys Kpu of the BSP HSMs in the network This procedure produces only one cryptographic envelope Kpu k per each BSP HSM containing the secret key k Only the private key Kpr of each BSP can decrypt the envelope Kpr Kpu k The security of these operations it is in section IV Figure 2 shows the workflow of the life cycle of the k Life cycle and security of the secret key k Show All To export and import the secret k Secret 2 key we use the following commands shown via OpenSSL script with RSA OAEP 2048 bit padding Certificate from BSP cer Secret 2 key key key Initialization openssl x509 pubkey noout in Certificate from BSP cer HSM pub inform DER openssl rsautl oaep encrypt inkey HSM pub pubin in Secret 2 key out Key key HSMprivate key Key key Secret 2 key Initialization openssl rsautl oaep decrypt inkey HSMprivate key pubin in Key key out Secret 2 key A local audit ceremony imports k into the BSP HSM with the feature non exportable It is not possible with this procedure to copy or export k The FIPS test certification requirements guarantees this HSM non exportable tool The conditions established in the FIPS tests documentation also deal with HSM s inviolability against penetration side channel physical chemical and other attacks Indeed one of the premises is that a certified HSM by internationally recognized mechanisms e g FIPS is necessary for our proposal All of these tools and procedures address the protection of the secret key k from violations and attacks In this subsection we will describe the API created to improve the proposal in the IEEE BOPS documentation As reported the IEEE BOPS API between sessionstatus and sessiondata does not have messages that guarantee client server biometric data transactions integrity We suggest inserting a subsection about Integrity including a session called sessionIntegrity between sessionstatus and sessiondata in the Role Gathering procedure considering the following commands Our API has two different approaches for HTTPS messages and five JSON transactions that improve the IEEE BOPS framework We are going to describe each of them Please refer to Appendix B for more details about the new API We propose two different services for the suggested session protocol i e for the IEEE BOPS API Role Gathering part The first one is the HUB service that takes care of the asynchronous transactions i e biometric identifying 1 n or verifying 1 1 transactions between client and BSP or BSP to other BSP The second one is the Directory service that takes care of the synchronous transactions i e bad connection responses pending operations time out among others The HUB service follows the asynchronous pattern i e all responses must be returned by the HUB that received the request when it has the available information All requests transaction in HUB service must use the POST method with the ANSI NIST biometric file in the request body and must contain the following headers NIST XML Content Type application xml NIST binary Content Type application octet stream The Directory service follows the synchronous pattern i e all responses must be returned in the same request response A reliable source of time synchronizes BSPs All requests transaction in Directory service must use the POST method and must contain the following headers Accept application json Content Type application json The response headers must contain the following parameters Content Type application json In Section V we will show the HUB service and Directory service working in the BSP network proposed for this work We will describe the JSON messages that create an integrity mechanism in IEEE BOPS API documentation As is done in Section 9 of the IEEE BOPS we will explain the integrity mechanisms created and in the API shown in Appendix B we show the calls request response in JSON Figures 3 and 4 present the workflow for these messages in the network proposed Please refer to Appendix B for more details Sequence of events to list pending operations between BSP Show All IDE and change status notification transactions Show All In case of an incident with a client or a BSP e g time out bad connection or maintenance the transaction shows a list of IDN and TCN indexes created in this work but could be any other that requires further processing that could not be done on line After the BSP requests the other to resend the missing operation only for IDNs that were not locally processed Hourly the procedure sends a JSON type to ensure that all processes have been executed holding integrity through the network After receiving the pending IDN list the BSP asks what operations should be performed This JSON message assures that the client or the BSP knows what was the missing transaction and enforce to operate the task This transaction intends to ask if a z code shown in Algorithm 2 is registered within the BSP local and cache database As reported the z parameter was created for this work but it could be any indexer on a biometric basis The same registered z is a JSON message response with a TRUE FALSE for existing or not If z exists it figures out which fingerprints and face are registered TRUE FALSE along with the related and TCN i A JSON message allows clients and servers to perform an identification or verification operation in the biometric network Client to BSP or BPS to BSP sends an IDE transaction The IDE transaction is the other remote entity s command to begin the identification or verification process It has attached the encrypted ANSI NIST package that can only be opened by a particular BSP which has the corresponding private key for additional security regarding the network The response is a VRE transaction with the value M or x It is an acknowledgment JSON message for the client or the BSP showing an identification process s termination status For this proposal it ensures that BSP can store the tuple IDN and TCN associated with the biometrics in the database Few techniques like our work refer to the security evidence and anonymity of the index register into biometric databases In Table 1 we show an informative outline of the existing approaches It is possible to realize that features transformations and cryptographic techniques among biometrics systems are not new However up to our best knowledge it had not been used to hold privacy with security evidence and also creating an interoperability protocol between different biometric database systems IEEE BOPS framework does not have any JSON messages that allow the client device to know with integrity and possibility of recovery the biometric transactions Our research shown in Table 2 improves IEEE BOPS by constructing JSON messages that allow the interoperability and integrity of biometric transactions pointing out if one did not complete the purpose task Also it allows us to use the IDN algorithm created ensuring the anonymity of the records We divide this section into three areas of security analysis The first and the second are focused on cryptoanalysis mainly on the randomness of the secret key semantic security SS forward by indistinguishability IND notation security The third one is based on network operations security For the security definitions of this paper Non malleability NM implies IND but for adaptive Chosen Ciphertext Attack CCA2 IND also implies NM SS is equivalent to IND in Chosen Plaintext Attack CPA model but not in CCA models The k is Random Number Generated Please refer to Appendix A The CPF is given by where is a decimal digit that is represented by one octet block In the first eight positions of CPF each octet block has 4 bit entropy The ninth represents a Brazilian state position The last two positions are checkers completing eleven digits and are calculated according to the first nine and ten i e and IDN is secure against Birthday Attack and Biclique Attack for any Towards proving that our encrypted IDN scheme is secure against the Birthday Attack and Biclique Attack we must explain the novel random and locally calculated iv and nonce created For iv we begin concatenating the 88 bit CPF string until 256 bit length with the 256 bit k resulting in a 512 bit length We use the entropy of a uniform random variable independent SHA 256 to one way 256 bit string converging to a entropy output effort For nonce we concatenate the 256 most valuable bits TCN string with the secret k We calculate the SHA 256 of this concatenation leading to a 256 bit length We achieve for nonce the same security level of the iv parameter The 128 bit CPF padded plaintext it is XOR ed with a 128 bit random iv for every entrance leading a random Instead of rebooting the encrypted AES 256 CBC with the previous outcome and an initialization vector we XOR ed the block entrance of the AES 256 CBC with a true 128 bit random and local iv derived from known parameters k and CPF only for the accredited BSP This process leads our encryption scheme to a 256 bit entropy effort The nonce is XOR ed with the outcome of AES 256 CBC resulting in IDN leading a 128 bit entropy effort cannot has control of the input bits calculated by XOR ing the random iv and also cannot computes nonce parameter The computational cost effort for our IDN scheme to find a collision is approximately n 256 k bit for birthday attacks Moreover the computational cost effort for a biclique key recovery is over 2250 bit under 14 rounds for 240 data and a preimage attack it is over 2120 bit under 14 rounds Therefore this computational effort leads to unfeasible known polynomial time attacks between the plaintext CPF to the encrypted IDN or IDN to CPF holding anonymity IDN is secure against Chosen Ciphertext Attack CCA Padding Oracle Attack POA and Chosen Plaintext Attack CPA for any As definition a well implemented AES CBC is security against POA and CPA but not CCA until our research For a general CCA an adversary sends to the Oracle a plaintext block After receiving sends is a random string By decrypting it obtains So calculates that leads to Now can compare with the original messages In our work the iv and nonce parameters are not sent over the network They work only within the cryptographic module of the BSP HSM Therefore e g 256 cannot be enforced by The same approach can be done for nonce parameter The only information sent through the network and stored in the databases is IDN ciphertext and TCN The calculations of iv and nonce in the IDN scheme are unfeasible for any The proposed PKE is SS forward by PKE IND CPA IND CCA and IND CCA2 notation against any adversary The RSA OAEP 2048 bit encryption scheme computes and outputing where is a bit integer random generated is a oneway trapdoor function of The RSA OAEP 2048 bit decryption scheme computes where is the inverse of using For LSB of and MSB of if PKE algorithm returns otherwise reject We use the RSA OAEP 2048 bit padding encode framework for the message in the cryptographic module of the HSM Recovering a message must compute is the Random Oracle RO and where is a RO The encode framework gives where Because OAEP does include a uniform random value in cannot recover which relies on the hardness of RSA 2048 bit problem cannot guess and is negligible for CPA Assuming that is a challenge ciphertext of is a random integer and follows the same iteration of the Oracle OAEP encryption For a not queried by the RO we have a uniform random distribution of and So there is a tiny probability that is queried by RO or refers to leading to a random and there is a minor probability that is computed leading Oracle decryption rejects By rejecting cannot combines the Oracle lists for a preimage of indicating that RSA OAEP 2048 bit is IND CPA IND CCA and also IND CCA2 encryption scheme From the exposition we can assume that our PKE is SS The difficulty of inverting the high exponents RSA function and the RO security model i e under the assumption that the hash functions used in the scheme behave as RO proves that our PKE scheme is secure considering the note found in RFC 8017 We use SHA 256 for our proposed scheme Our RSA OAEP 2048 bit implementation makes negligible Other significant attacks and methods must be considered For the AES 256 CBC algorithm a related key amplified boomerang attack has bit complexity in 14 rounds for data The hardness of exploiting RSA 2048 using index calculus Number Field Sieve NFS has a 2112 complexity The offline and BSP HSMs implement cryptographic calculations in a constant time with differential power analysis resistant cores and libraries embedded in a secure cryptographic module accredited with FIPS test suite This approach avoids side channels attacks e g timing attacks power analysis and fault analysis We will comment on some possible attacks on devices and networks pointing out the countermeasures for each of them The wrapped k is imported in a local ceremony at the BSP s security environment The offline HSM and BSP HSMs have a feature that does not allow any copy or misuse of the non exportable k The root master administrator can only create or destroy the logical space into the slot but will never gain the slot s ownership The BSP HSMs have other security features embedded e g multiple level authentication split access among operators Intrusion Detection System IDS non physical mechanical chemical violability and Structured Query Language SQL injection protection Mutually authenticated channels for all communication is done using a TLS SSL RSA 2048 bit By a signed trusted biometric service list each accredited BSP IP and URL end points are informed There are dedicated firewalls that only set the route IP of each BSP The biometric ANSI NIST packages have the name of the issuing and the destination BSP All names are retrieved from the certificate embedded in the trusted list In addition to every network part mentioned BSP uses time synchronization with a reliable time source with a timestamp for transaction exchange From this exposure replays attacks and Distributed DoS attacks can be mitigated We show the security evaluation of our work against known attacks and demonstrate the IDN scheme calculations Finally we present the new interoperability communication protocol with the built API working for an identification purpose IDE and the respective performance All results were obtained using data acquired from the operating BSP network Table 3 shows security evaluation of our proposed scheme Based on Section IV we show the complexity the unfeasible mathematical approach and the method applied for a negligible probability for each known attack Our work s main cryptographic contribution is the randomly calculated iv and nonce parameters The iv and nonce have 128 bit length and they are not sent through the network working locally and re calculated for each entrance Consequently cannot predict compute or enforce in a polynomial time the iv and nonce thus repealing significant attacks against the IDN proposed scheme including CCA against AES 256 CBC The complexity of breaking the proposed encrypted IDN is not feasible in a polynomial time attack OAEP implementation s complexity relies on finding a collision in the RO SHA 256 and in the hardness of breaking RSA 2048 bit All storage and calculus are done in an accredited HSM that generates random k applying side channels countermeasures with a non exportable asset feature The main result of our scheme is the enhanced security and privacy of biometric network and databases For network operation ensuring the interoperability protocol known practices have been implemented to repeal attacks Mutually SSL communication channels with a signed BSP Trusted Certificate List IPsec protocols dedicated IP URL endpoints synchronized BSP and timestamp message are some methods that can mitigate e g Distributed Denial of Service DDoS and replay attacks The IDN an OpenSSL script CPF TCN Key key IDN Initialization read CPF read TCN CPFhex echo n CPF xxd p CPFEXT CPFhex CPFhex CPFhex CPFEXT CPFEXT 0 64 K echo hexdump v e 1 02X Key key KCPF K CPFEXT KCPF KCPF 0 128 shaX echo n KCPF xxd p r sha256sum cut d f 1 A shaX 0 32 B shaX 32 32 IV for i 0 i A i 2 do Ai 16 A i 2 Bi 16 B i 2 xorAB Ai ^ Bi tmp printf 02x xorAB IV IV tmp done lData CPFhex lPadding 32 lData 2 blockCPF CPFhex tmp printf 02x lPadding for i 0 i lPadding i do blockCPF blockCPF tmp done z echo n blockCPF xxd p r openssl enc nopad e a nosalt aes 256 cbc K K iv IV TCNhex echo n TCN xxd p TCNEXT TCNEXT 0 64 KTCN K TCNEXT KTCN KTCN 0 128 shaT echo n KTCN xxd p r sha256sum cut d f 1 C shaT 0 32 D shaT 32 32 NONCE for i 0 i A i 2 do Ci 16 C i 2 Di 16 D i 2 xorCD Ci^Di tmpnonce printf 02x xorCD NONCE NONCE tmpnonce done IDN for i 0 i A i 2 do zi 16 z i 2 NONCEi 16 NONCE i 2 xorzNONCE zi ^ NONCEi tmpidn printf 02x xorzNONCE IDN IDN tmpidn done Table 4 shows the IDN calculations for the proposed scheme created Those are made following the script by using a possible hypothetical CPF and TCN with a test key k generated from the offline HSM for experimental purpose According to the results iv nonce z and IDN cannot be calculated without the knowledge of k associated with CPF and TCN i e only BSP in the network are able to generate z for the same plaintext CPF In the next subsection we will show a real CPF shown as IDN in this operation biometric network We present the results for the new interoperability communication protocol proposed The logging trail was extracted from BSP1 showing communication between Network 1 BSP1 and Network 2 BSP2 with a real and irreversible IDN The pending operation operation resend z query and an IDE transactions with a change status messages are shown DIR RECEIVED FROM BSP2 REQUEST requestType opending operations DIR SENT TO BSP2 RESULT opendingOperationsList operationType 1 n queue idnList idn 10d548f2fe7559d0a3162bc3db992ff2 tcn 0BB79BA3 2911 4297 9758 10E0BF975002 DIR RECEIVED FROM BSP2 REQUEST requestType operation resend idn 10d548f2fe7559d0a3162bc3db992ff2 tcn 0BB79BA3 2911 4297 9758 10E0BF975002 DIR SENT TO BSP2 RESULT response IDE idn 10d548f2fe7559d0a3162bc3db992ff2 tcn 0BB79BA3 2911 4297 9758 10E0BF975002 The pending operation mechanism ensures that every BSP processes all data By requesting it the BSP 2 received from BSP 1 all the tuple IDN and TCN that were not processed This command allows the network to be righteous i e without any missing identification processes After the BSP 2 which requested the pending operation and received the IDN list sent a operation resend that ensures BSP 1 that BSP 2 is ready to process the missing transactions Immediately after indicating that BSP 2 is ready the designated BSP 1 sent the IDE transactions HUB SENT TO BSP2 REQUEST requestType z query idn 66f5a1b3282da4ac74aabf28112c240a tcn 0C490874 B4F5 46ED B9CB BEB8E6F71081 HUB RECEIVED FROM BSP2 RESULT exists FALSE idn 66f5a1b3282da4ac74aabf28112c240a tcn 0C490874 B4F5 46ED B9CB BEB8E6F71081 HUB SENT TO BSP2 TRANSACTION transaction type IDE idn 66f5a1b3282da4ac74aabf28112c240a tcn 0C490874 B4F5 46ED B9CB BEB8E6F71081 timestamp 1575288144418 HUB RECEIVED FROM BSP2 TRANSACTION transaction type VRE idn 66f5a1b3282da4ac74aabf28112c240a tcn 02FA798D D81F 43DB 9E44 32489389C470 timestamp 1575288144418 reference tcn 0C490874 B4F5 46ED B9CB BEB8E6F71081 srf X HUB SENT TO BSP2 REQUEST requestType change status idn 66f5a1b3282da4ac74aabf28112c240a tcn 0C490874 B4F5 46ED B9CB BEB8E6F71081 After running the local process BSP 1 requests an IDE for the IDN 66f5a1 to BSP 2 The BSP 2 responded a VRE with X value i e no biometrics were found in the biometric database The BSP 1 sent a change status acknowledge message completing the registration process and BSP 2 can store in its cache database Every IDE request has attached the encrypted ANSI NIST package with the biometrics corresponding to each IDN Only the private key of each BSP can open the package and perform biometric identification processes These results prove that the proposed scheme is secure viable and could be incorporated into IEEE BOPS to have biometric network flow integrity mechanisms We will comment on the performance of the proposed scheme For this work performance is the efficiency of the network in processing online transactions The results are from a running instance between BSP 1 and BSP 2 First in Table 5 we show the principals components settings used for this work Then the HSM encrypts decrypts capacity Finally we show the IDE and pending operations numbers per day running the API proposed over a week The method applied to measure performance was to check how many IDE transactions BSP 1 sent and how many pending operations transactions BSP 2 had per day It means BSP 2 s ability to process demand transactions within the scope of this paper The HSM performs a Conditional Self Tests during its operation according to FIPS The nominal number to establish the script with AES 256 bits is over 1 000 per second This performance is much higher than when the transactions are carried out by the proposed network As will be shown the HSM encryption capacity using the suggested OpenSSL script is much greater than the network s ability to send and receive transactions We measured the overall performance of the network for a week regarding IDE and pending operations transactions In Figure 5 and Figure 6 we show the results 1 IDE over a week Show All BSP 2 pending operation over a week Show All The BSP 1 sent an average of 14 389 IDE transactions per day and received from BSP 2 an average of 4 284 for the same JSON requests The pending operations that BSP 2 requests for BSP 1 have an average of 266 transactions per day The proposed scheme s performance given by is approximately 98 15 Security privacy and interoperability should be the main requirements of all implemented biometric networks Biometric databases that communicate or intend to do so cannot guarantee a person s anonymity and yet complete the necessary processes to reliably identify an individual Operating the systems under these circumstances leads to privacy and security failures and this situation has to be changed Besides in light of recognition systems interoperability based on different technological frameworks no communication protocol fulfills the requirements for exchanging biometric data through the networks anonymously with a method that ensures transactions integrity We put forward a novel scheme that guarantees everyone s anonymity within the biometric databases and still allows them to communicate securely by following all the identification procedures A probabilistic encryption scheme and a new communication protocol were used to achieve privacy together with security evidence and interoperability to ensure the integrity of messages sent between biometric networks We successfully produced an anonymous index for all the databases representing only one person for the same input data and secret key among the systems However we believe that the IEEE BOPS standard can be improved by establishing an additional framework for JSON messages between systems including a way for networks to maintain their operations regardless of contingencies In future work it should be possible to expand the communication protocol for updated biometric packages Moreover the IDN algorithm that was created can provide security privacy and interoperability for general purposes e g for any communication network or database segmentation particularly those that use encryption schemes like 5G security architecture This future work will be a logical outcome of the contributions we have made to the new interoperability protocol which involves sharing an encrypted index enhancing security and ensuring privacy for the biometric network In order to perform formal analysis automatically of keys that are generated by the offline HSM we used the NIST test suite We compiled the make iid and make non iid tests using the libdivsufsort dev libbz2 dev dependencies with a Ubuntu 18 04 operation system with the following results NIST IID test ea iid i keys bin Calculating baseline statistics H original 7 886548 H bitstring 0 998301 min H original 8 X H bitstring 7 886548 Passed chi square tests Passed length of longest repeated substring test Beginning initial tests Beginning permutation tests these may take some time Passed IID permutation tests NIST hbox Non IID test ea non iid i keys bin Running hbox non IID tests Running Most Common Value Estimate Running Entropic Statistic Estimates bit strings only Running Tuple Estimates Running Predictor Estimates H original 7 718814 H bitstring 0 932005 min H original 8 X H bitstring 7 456043 This result proves that the official NIST test suite approves the randomness of the keys k that are generated from the proposed scheme We present the API code that can be emerged in IEEE BOPS Each step determines which type of call is required in requests and responses for integrity purposes swagger 2 0 info description API version 1 0 0 title contact email emlacerdaf gmail com tags name directory description Synchronous Pattern name hub description Asynchronous Pattern schemes https paths directory zquery get tags directory description zquery consumes application json produces application json parameters in query name z type string required true description zcode responses 200 description z was found in the base schema ref definitions zquery 204 description z not found 400 description Bad request 401 description Request without certificate 403 description Certificate not recognize directory PendingOperations get tags directory description Pending operations listing operation consumes application json produces application json responses 200 description Return from pending operations schema ref definitions PendingOperations 400 description Bad request 401 description Request without certificate 403 description Unrecognized certificate directory operation resend get tags directory description Operation resubmit request pending operation consumes application json produces application json parameters in query name tcn type string required true description TCN code responses 202 description Accepted 400 description Bad request 401 description Request without certificate 403 description Unrecognized certificate directory idn list get tags directory description IDN list consumes application json produces application json parameters in query name startDate type integer description UNIX Timestamp UTC in query name endDate description UNIX Timestamp UTC type integer responses 200 description OK schema ref definitions HubError 401 description Request without certificate 403 description Unrecognized certificate hub post tags Hub description Hub operations consumes application xml application octet stream produces application json parameters in body name NIST description ANSI NIST transaction schema type object example responses 202 description Accepted 400 description Bad request schema ref definitions HubError 401 description Request without certificate 403 description Unrecognized certificate definitions ZQuery type object properties idn type string example IDN code timestamp type integer example 1234567890123 exists type string example TRUE or FALSE t 14 013 1 type string example Corresponding TCN or blanck t 14 013 2 type string example TCorresponding TCN or blanck t 14 013 3 type string example Corresponding TCN or blanck t 14 013 4 type string example Corresponding TCN or blanck t 14 013 5 type string example Corresponding TCN or blanck t 14 013 6 type string example Corresponding TCN or blanck t 14 013 7 type string example Corresponding TCN or blanck t 14 013 8 type string example Corresponding TCN or blanck t 14 013 9 type string example Corresponding TCN or blanck t 14 013 10 type string example Corresponding TCN or blanck t 10 type string example Corresponding TCN or blanck PendingOperations type object properties pendingOperationsList type array maxItems 1000 items type string example IDN of the pending transaction TCN of the pending transaction IDN of the pending transaction TCN of the pending transaction idnList type array items properties idn type string example timestamp type integer example 1234567890123 t 14 013 1 type string example Corresponding TCN or blanck t 14 013 2 type string example Corresponding TCN or blanck t 14 013 3 type string example Corresponding TCN or blanck t 14 013 4 type string example Corresponding TCN or blanck t 14 013 5 type string example Corresponding TCN or blanck t 14 013 6 type string example Corresponding TCN or blanck t 14 013 7 type string example Corresponding TCN or blanck t 14 013 8 type string example Corresponding TCN or blanck t 14 013 9 type string example Corresponding TCN or blanck t 14 013 10 type string example Corresponding TCN or blanck t 10 type string example Corresponding TCN or blanck HubError type object properties errorCode type integer example 999 errorMessage type string example error"}
{"title": "Extended State Observer-Based Nonlinear Terminal Sliding Mode Control With Feedforward Compensation for Lower Extremity Exoskeleton", "number": "9316728", "authors": "[{'preferredName': 'Yi Long', 'normalizedName': 'Y. Long', 'firstName': 'Yi', 'lastName': 'Long', 'searchablePreferredName': 'Yi Long', 'id': 37089273868}, {'preferredName': 'Yajun Peng', 'normalizedName': 'Y. Peng', 'firstName': 'Yajun', 'lastName': 'Peng', 'searchablePreferredName': 'Yajun Peng', 'id': 37089273234}]", "abstract": "This paper presents and experimentally demonstrates an extended state observer (ESO) -based nonlinear terminal sliding mode control strategy with feedforward compensation (ESO-F-NTSMC) for lower extremity exoskeleton. Since the lower extremity exoskeleton (LEE) is a coupled human-exoskeleton coordination system, the internal or external disturbances and uncertainties affect its performance. A nonl...", "text": "This paper presents and experimentally demonstrates an extended state observer ESO based nonlinear terminal sliding mode control strategy with feedforward compensation ESO F NTSMC for lower extremity exoskeleton Since the lower extremity exoskeleton LEE is a coupled human exoskeleton coordination system the internal or external disturbances and uncertainties affect its performance A nonl Many diseases and injuries such as stroke and spinal cord injury have a considerable impact on these patients normal living abilities and mental health It is useful and necessary to develop intelligent wearable exoskeletons for rehabilitation training to regain mobility These devices can help users practice repetitively and free physical therapists from heavy work In past decades many kinds of wearable exoskeletons are designed to help patients rehabilitate from the jury As the exoskeleton is a typical human machine interaction system it is essential to design and develop control strategies to track the defined human gait safely and robustly From the perspective of training purpose and controller design progress the control strategies for exoskeletons can be separated into trajectory tracking control adaptive control impedance based control and bio signals based control These control strategies can also be divided into model based and model free methods from the perspective of model dependence or not The model based techniques acquire a full dynamics knowledge and their performance relies on the accuracy of the system model Nevertheless it is not easy to obtain an accurate and complete dynamic model in practice since many diverse sensors need to be utilized to obtain and identify system parameters The model free strategies are designed without considering the exoskeleton s dynamics model and their performance is influenced normally by the external disturbances The disturbances existing in the system have effects on the performance of the exoskeleton system To improve the robustness and accuracy of gait training numerous strategies for wearable robotic exoskeletons have been substantially developed such as robust control and adaptive control However those mentioned control approaches can only ensure asymptotic convergence when dealing with uncertainties and disturbances To achieve consistent high dynamic tracking control and better convergence performance the finite time control strategy such as sliding mode control SMC has been studied for the robotic exoskeleton control SMC has two specific features of disturbance rejection and insensibility to uncertainties by designing the sliding mode surface Different types of SMC strategies are employed for robotic exoskeletons such as terminal SMC nonsingular terminal SMC and fuzzy SMC Nevertheless the performance of sliding mode control is susceptible to the existence of a chattering phenomenon which may increase control effort and excite high frequency oscillation The boundary layer saturation method is used to attenuate or eliminate the chattering Meanwhile this method will increase the error and reduce the speed of response In the real exoskeleton control the internal such as chattering in the SMC or external disturbances exit universally and need to be compensated or suppressed for good performance The unknown disturbances can be estimated by designing a disturbance observer and canceled in the feedback control law or be compensated by applying a disturbance compensator To ensure a more transparent effective and robust control strategy for robotic exoskeletons one of the ordinary solutions is to use extended state observers ESOs which is designed to estimate the system states as well as total disturbances The ESO based control approach is first to assess the disturbance in terms of the output and then canceled by its estimates The ESO is the core of active disturbance rejection control ADRC The central objective of active disturbance rejection control ADRC is to treat the internal and external uncertainties as a total disturbance and eliminate them actively ADRC has been employed successfully in the robotic exoskeleton control The SMC combination with disturbance observer is efficient and useful for the disturbance rejection and the chattering phenomena elimination simultaneously Chen et al employ the ADRC and terminal SMC into following the human gait trajectory in the lower extremity exoskeleton This method applied FTSMC to replace the ordinary control law of the ADRC and increase the fast response However this designed ADRC based SMC strategy is only restricted to the swing phase and only the experiments of the swing leg are conducted To control the robotic system safely and robustly the nonlinear terminal sliding mode control N TSMC is employed to control the robotic system fast and robustly Meantime the ESO can estimate the internal or external disturbances including the chattering or disturbance caused by the N TSMC The disturbances estimation can be eliminated or alleviated as the negative feedback in the control strategy design The feedforward compensation control is added into the combination strategy to provide a fast response of the system Given this a novel ESO based nonlinear terminal sliding mode control with feedforward compensation control ESO F NTSMC is employed to control the wearable exoskeleton with a robust response and fast convergence Unlike the design principle in the F NTSMC is the primary and fundamental control law The usage of ESO is helpful to estimate the total disturbance including that caused by chattering The proposed control strategy is not restricted only to the swing leg and valid for the whole lower extremity in the stance swing and the swing phase In this paper the lower extremity exoskeleton system is specified in the second section In the third section the proposed control strategy is presented Experiments and results analysis are given in the fourth section Conclusions are drawn in the final quarter Exoskeletons are worn by humans and work parallel with the human body as anthropomorphic devices Human motion analysis is usually used to design and optimize an assistive lower limb exoskeleton The designed exoskeleton needs to meet the natural movement s basic requirements including enough range of motion dexterous workspace and lightweight A single leg is composed of three joints hip knee and ankle and has seven degrees of freedom DoF for a human user in which the hip joint has three DoFs the knee joint has one DoF and the ankle joint has three DoFs To achieve and improve the comfortable wearing feeling the designed lower extremity exoskeleton should have the same or close DoFs as the human legs have The lower extremity mainly helps human people walk in the sagittal plane To maintain the wearer s balance in the sagittal plane and take consideration of lightweight design the flexion extension DOFs of the hip joint and the knee joint are powered by DC motors The mechanical structure of the lower extremity is shown in The powered hip joint or knee joint is actuated by a DC motor system encompassing an actuation motor and a gear pair with embedded encoders The thigh segment the shank segment and the waist part can be adjusted as required The attachment straps on the thigh and the shank are made of soft materials to prevent the human leg s improper pressure from causing wearing discomfort The wearable shoe is connected with the shank segment through the ankle joint and is similar to the real sports shoes to enhance flexibility There are three pressure sensors to collect ground reaction force for phase identification The designed exoskeleton can walk at the maximum speed of 1 25m s and the weight of the whole system is less than 12kg The exoskeleton can be worn by people whose weight is less than 80kg The structure of the exoskeleton HAA hip adduction abduction HFE hip flexion extension HML hip medial lateral rotation KFE knee flexion extension APD ankle plantarflexion dorsiflexion APE ankle pronation external rotation WLA waist length adjustment TLA thigh length adjustment SLA shank length adjustment The HFE and KFE DoFs are powered by DC motors and other DoFs are passive The waist part can be adjusted up to 38cm The length of the thigh and shank segments can be adjusted to fit the human user in the height of 155cm 185cm Show All In the dynamic modeling the lower extremity mainly moves in the sagittal plane The powered DoFs are the flexion extension movement of the hip joint and the knee joint As the ankle joint s movement range is small in the sagittal plane it can be regarded as a fixed joint in the model Some parameters such as the mass and length of the leg segment are assumed to be constant The leg segment of the exoskeleton can be simplified as a 2 DoFs link with the trunk as the fixed platform Without loss of generality the mathematical model of a single leg of the exoskeleton can be expressed as follows where is the symmetric definite inertial matrix is the Coriolis and Centrifugal force matrix is the gravitational force matrix is the control input torque vector denotes un modeled dynamics and external disturbances where and are angles for the hip joint and the knee joint respectively is input torque vector is driving torque of the hip joint while is driving torque of the knee joint The specific expression of matrices of and can be shown as the following The lower extremity exoskeleton is a human in the loop system which needs to adjust the robot system and the human user in coordination The exoskeleton system needs to understand how the human user moves The exoskeleton s control process has two significant aspects one is to generate human gait trajectory based on sensors measurement information The other is to shadow the human gait by designing a control strategy taking care of the user s properties Human gait trajectory generation is the essence and the first step of exoskeleton control In this paper the human gait is obtained by the combination of the pressure sensors and IMUs The proposed human gait generation method is based on walking phase identification In this work the walking phases are separated into the stance phase and the swing phase Those two walking phases can be distinguished by a concise threshold method The ground reaction force GRF measured by pressure sensors can be recorded as 0 0 0 0 0 and 0 for the left leg and right leg respectively in the initial stance phase The sums of GRF in those two shoes are expressed as follows where 0 and 0 are the initial sums of GRFs when the human user stands upright In real time the GRF sums in the th interval are shown as below Several positive weight parameters are defined as and to satisfy and The phase identification process is divided into three cases which is shown as the following double stance for two legs stance phase for left and swing phase for right swing phase for left and stance phase for right As discussed above in  the walking phases for a single leg can be identified as the stance phase and the swing phase The proposed gait generation method is built on the foundation of phase identification Except for double stance the gait trajectory in other phases can be acquired by the IMU measurement directly as following and are the hip joint angle in the interval and the interval respectively while and are the knee joint angle in the interval and the interval respectively and are the increment angular position obtained from the IMU attached on the thigh and the shank respectively The IMU s original output has been processed by the extended Kalman filter to improve accuracy According to the placement of IMUs or in  is the yaw angle relative to the upright posture where and can be measured by the IMUs attached to the thigh and the shank respectively If the phase is identified as the stance phase and will be set as zero Based on the previous work in the process of ESO derivation is expressed in detail In general a second order single input and single output SISO with known knowledge can be expressed as the following where and are the acceleration velocity and position of the system and are the known knowledge of the system is the external disturbance and the un modeled dynamics is the input of the controller is a parameter dependent on the system The system state can be defined as and The system can be rewritten as the following expression Defining the observation states as and then the formulation 11 can be rewritten as follows The observation error is defined as where is the state estimation The ESO can be expressed as below where and are constant are positive constant parameters relative to the bandwidth of the controller and observer Through the usage of ESO in  the total disturbances can be estimated and then eliminated in the feedback law design Similarly the mathematical model of the exoskeleton is a second order multi input and multi output MIMO system Based on   the dynamics model of the exoskeleton can be rewritten as below Based on  the following expression in state space can be obtained  can be expressed as where Then define a positive matrix as The system  can be rewritten as where Given that is known the control input can be obtained as Then  can be split into two equations as the following Based on the general formula of ESO and  two ESOs can be obtained for the hip joint and the knee joint respectively as follows and are the positive observer gain matrices can be obtained from as the estimated disturbances by ESO The designed ESO can estimate total disturbance by the extended state calculation In general the SMC for systems can be achieved by the sliding surface definition and the control law design The proposed control approach aims at improving the whole performance with high accuracy and fast coverage The tracking error in the nonlinear SMC strategy can converge to zero in finite time As discussed previously the designed target trajectory is the human gait as in  and the actual position feedback is The tracking errors of the hip joint and the knee joint can be defined as below To ensure that the tracking errors converge within finite time and avoid the singular problem the sliding mode surface can be designed as and are positive parameters Based on  in order to improve robustness and fast convergence of the SMC a deigned nonlinear fast terminal sliding surface is represented as follows where is a positive definite matrix ts is the sampling cycle time The time derivative of the sliding surface is expressed as the following The control laws should be chosen to satisfy the existence condition of SMC and the exponent reaching law is given as follows where and are positive definite matrices is a symbolic function which is shown as the following Combining  and  Based on  and  then  can be rewritten as Based on  the control vector can be obtained as below The proposed controller should guarantee that the tracking error and the sliding surface can converge to zero asymptotically If then the control system is globally stable i e when The Lyapunov based stability proof is presented as the following The Lyapunov function can be defined as The differentiation of can be denoted as follows Combining  and  Substituting  into  we can obtain As discussed above and are positive then and are also positive finally Therefore the control system is globally stable which means when as then and as According to the stability proof the trajectory tracking error can converge to zero by the proposed control law Based on  the control input to the lower extremity is shown as below where is expressed in  In the robot control the feedforward control can provide a fast response When the walking phase and the motion direction of the leg segment change discontinuous friction or inertial will encounter affecting the human gait tracking performance To cope with these erratic changes and assure a smooth transient response a feedforward control loop is added into the nonlinear SMC expressed in  The framework of the proposed ESO F NTSMC strategy is shown in The proposed ESO F NTSMC strategy for the lower extremity exoskeleton The input part mainly includes information from pressure sensors and IMU The walking logic part describes the walking phase identification and the human gait generation The ESO estimates the disturbances as negative feedback into the master control signals The main function of the designed control strategy is to shadow the human gait trajectory Show All The control architecture of the lower extremity exoskeleton system is shown in As shows the central controller is an embedded PC whose function is to produce control signals receive the sensor feedback information and transfer relevant information to other modules A single leg has two actuation systems each of which is composed of a driver and a DC motor There are two kinds of sensors employed in the control framework i e inertial measurement unit IMU and position sensor encoder The IMU is embedded in the attachment point and the encoder is integrated into the DC motor There are three pressure sensors in the wearable shoe and one IMU sensor attached to the shoe s side The improvement of measurement accuracy can be achieved by the compensation of the information fusion algorithm In this kind of used IMU the extended Kalman filter is used to deal with the problem of time drift and accurate output data in real time All sensors are transferred back into the central controller through the CAN bus The control command signals are also transmitted through the CAN bus The power supply is provided by the 24V lithium battery Besides some digital sensors are used to start on off the system or as safety switches A wearable watch is also utilized as a measurement tool to obtain the kinematic information of the lower extremity exoskeleton Some safety precautions are taken into consideration in mechanical design and electrical design In the mechanical design physical status indicator switches are placed to prevent leg segments from excessive excursions In the electrical system a sizeable e stop button shuts off full power and the motor control torque is limited by using a saturation function Hardware architecture of the exoskeleton control system All distributed slaves communicate with the central PC through CAN bus Those sensors are plugged into the CAN network to transfer information Show All Experiments are performed to evaluate the performance of the proposed approach Three human volunteers participated in the experiments and the Laboratory Management Council authorized ethical approval Those three human volunteers have an average height of 1 70 0 5 m an average weight of 65 4 5 kg and an average age of 33 5 years old The lower extremity functions of all human subjects are normal The lower extremity exoskeleton is used with crutches help to imitate the usage scenario of patients which is shown in The human volunteers are asked to wear the exoskeleton by himself as the following process 1 wear the exoskeleton and adjust the leg segment to fit the human user s body shape 2 fasten all belts to make the user feel comfortable especially the attachment points on the leg 3 adjust the exoskeleton until to prepare to walk 4 prepare the assistive crutches 5 press down the turn on off button to start on the actuation system 6 walk on the ground at a natural mode During the experiments the kinematic and kinetic data are collected in real time through the user software interface Experiments of wearing the lower extremity exoskeleton for straight walking on the floor The crutches were used for walking safety Show All We compare three kinds of control strategies traditional terminal SMC TSMC ESO based traditional terminal SMC ESO TSMC and the proposed approach ESO F NTSMC In the TSMC the sliding surface is defined as  shows and the control law can be expressed as follows The formula of ESO TSMC can be shown as below In this experiment three kinds of strategies namely TSMC ESO TSMC and the proposed ESO F NTSMC are compared from the perspective of the target trajectory tracking performance To ensure the performance comparison s validation all the parameters are predefined as the same including parameters of ESO and TSMC Those parameters in ESO can be obtained from the observer s bandwidth and all matrices in TSMC are symmetric To make the difference more explicit the root mean square error RSME is utilized to evaluate the strategy s performance as follows where is the kth sampling tracking error and is the size of the error vector The wearers are asked to walk at a natural speed Based on the online gait generation method by   the target human gait trajectory can be obtained The position encoder monitors the actual angular position of the hip joint and the knee joint Three subjects carried out the wearing experiments and the target trajectory may be different because of individual differences The target trajectory can be generated based on the proposed online gait generation method as shown in shows the mean joint trajectories of the hip joint and knee joint for the three subjects and gives the mean value and standard deviation for all three subjects In a whole gait cycle the gait trajectory can automatically be transferred from the stance phase to the swing phase The human gait trajectory tracking performance by using the proposed method ESO F NTSMC is illustrated in The human gait generation method obtains the target gait trajectory and the embedded encoder receives the actual trajectory feedback As depicts the gait tracking is quite good and the target gait trajectory can be followed It is evident that the proposed strategy can shadow the human gait trajectory in time and accurately for the lower extremity exoskeleton also illustrates the consistency of the proposed method for different users The target trajectory for the three subjects under the proposed human gait generation method Show All Trajectory tracking performance by the proposed method Show All To illustrate the validity of the proposed method well the RSME and tracking errors are compared among TSMC ESO TSMC and ESO F NTSMC The comparisons of tracking performance are conducted as shown in and As shows the RSME is regarded as the comparison index for those three strategies RSMEs of two joints in TSMC are larger than 0 4 degrees while those in ESO TSMC and ESO F NTSMC are smaller than 0 3 degrees The RSMEs in ESO F NTSMC are smaller than those in ESO TSMC obviously From the perspective of the tracking error depicted in the ESO F NTSMC can obtain the most accurate tracking compared with the other two strategies In and the tracking errors are the mean values for three subjects A more detailed comparison is conducted in Table 1 Table 1 gives the tracking errors range of hip joint and knee joint and provides performance improvement from the perspective of RSMEs Compared with the traditional TSMC the ESO TSMC and ESO F NTSMC have 41 49 and 47 04 performance improvement for the hip joint and 42 04 and 44 02 for the knee joint respectively To some extent the proposed method is effective for different wearers and the exoskeleton system can be adopted to help different users The usage of ESO can provide more accurate and robust trajectory tracking for the LEE The ESO F NTSMC has a small improvement than ESO TSMC The RSME performance comparison for three strategies Show All The tracking error comparison for three strategies Show All In addition the control torque comparison is also depicted as shown in The control torque is estimated by the control current feedback and the relationship between the control torque and the control current is assumed to be approximately linear As shows the chattering phenomenon of ESO TSMC or ESO F NTSMC is weaker than that of TSMC The control torque of the ESO F NTSMC is smaller than that of TSMC and ESO TSMC The ESO F NTSMC has smoother movement and a more powerful ability to deal with the disturbances Therefore the proposed method can improve the gait tracking accuracy and overcome the total disturbances to promote the smoothness of movement The control torque comparison for three strategies Show All The proposed method ESO F NTSMCT is used to track the human gait in real time Compared with TSMC the proposed method is more robust and smoother The proposed control method has the potential to be a practical general control framework for the lower extremity exoskeleton because of its adaptation to different users and good application performance Meantime there is one significant issue for the proposed method in real usage which is too many parameters need to be set and adjusted by empirical In this work we contributed to the development of a control approach for lower extremity exoskeleton namely ESO based nonlinear terminal SMC with feedforward compensation The lower extremity exoskeleton under study and its mathematical model is given in detail The control strategy design process is also presented encompassing the human gait generation the ESO derivation the SMC design and the stability proof Finally the experiments are conducted on three human volunteers The results show that the proposed method called ESO F NTSMC can obtain more precise tracking performance and smoother movement The ESO F NTSMC strategy has the potential to achieve promising performance and can be implemented and extended to other rehabilitation exoskeletons control Looking towards the future we will study how to tune those parameters online to improve adaptability The dynamic model will be identified by an online calculation method such as a neural network The metabolic cost of wearing the exoskeleton will also be envisioned in future work"}
{"title": "Fault Reconfiguration Strategies of Active Distribution Network With Uncertain Factors for Maximum Supply Capacity Enhancement", "number": "9321405", "authors": "[{'preferredName': 'Xiaoqin Wang', 'normalizedName': 'X. Wang', 'firstName': 'Xiaoqin', 'lastName': 'Wang', 'searchablePreferredName': 'Xiaoqin Wang', 'id': 37089453294}, {'preferredName': 'Xudan Sheng', 'normalizedName': 'X. Sheng', 'firstName': 'Xudan', 'lastName': 'Sheng', 'searchablePreferredName': 'Xudan Sheng', 'id': 37089452547}, {'preferredName': 'Weixing Qiu', 'normalizedName': 'W. Qiu', 'firstName': 'Weixing', 'lastName': 'Qiu', 'searchablePreferredName': 'Weixing Qiu', 'id': 37089453206}, {'preferredName': 'Wanli He', 'normalizedName': 'W. He', 'firstName': 'Wanli', 'lastName': 'He', 'searchablePreferredName': 'Wanli He', 'id': 37089453114}, {'preferredName': 'Jing Xu', 'normalizedName': 'J. Xu', 'firstName': 'Jing', 'lastName': 'Xu', 'searchablePreferredName': 'Jing Xu', 'id': 37089452804}, {'preferredName': 'Yang Xin', 'normalizedName': 'Y. Xin', 'firstName': 'Yang', 'lastName': 'Xin', 'searchablePreferredName': 'Yang Xin', 'id': 37089451669}, {'preferredName': 'Jiabin Jv', 'normalizedName': 'J. Jv', 'firstName': 'Jiabin', 'lastName': 'Jv', 'searchablePreferredName': 'Jiabin Jv', 'id': 37089451203}]", "abstract": "With a large number of distributed generation connected to the distribution network, the total supply capacity of the distribution network has been improved to a certain extent. However, due to the randomness of its output, it also brings some challenges to the reliable operation of the distribution network and the restoration of power supply after failure. Based on this, this paper proposes an ac...", "text": "With a large number of distributed generation connected to the distribution network the total supply capacity of the distribution network has been improved to a certain extent However due to the randomness of its output it also brings some challenges to the reliable operation of the distribution network and the restoration of power supply after failure Based on this this paper proposes an ac With the construction of active distribution network a large number of DG access not only can balance part of the load nearby but also has an impact on the load prediction of distribution network side Therefore the construction and access of DG must be fully considered in the planning of future active distribution network With the trend of DG penetration increasing in the future it is very important to accurately predict the DG output in the planning area Reference introduces the total supply capability TSC model of distribution system in which reference gives TSC model based on main transformer and reference gives TSC model based on feeder On the basis of TSC in reference a calculation method of available supply capacity ASC of distribution network is proposed The meaning is that under the condition of ensuring the N 1 safety constraint the distribution network can increase the load on the basis of the existing total load If the power supply capacity is reached each load node needs to achieve a specific and ideal load distribution At present some studies have considered the influence of DG access on the calculation of total supply capacity In reference a calculation model of maximum total supply capacity considering DG location and network reconfiguration was established to maximize the expected load magnification under typical DG output scenarios The calculation results show that DG access is conducive to the improvement of maximum total supply capacity Reference established a two level optimization model of maximum total supply capacity considering the uncertainty of DG output In the process of maximizing the total supply capacity the economic operation of active distribution network was considered but the N 1 safety criterion was not included in the constraint In reference a model of maximum power supply capacity including user classification and interaction between user side and power grid is established The demand response load is considered as interruptible load and emergency demand response load Simulation results show that the interaction with user side can improve the maximum TSC of distribution network Both DG and DG can improve the total supply capacity to a certain extent Among all the control strategies the distribution network fault reconfiguration can adjust the network topology structure in real time according to the system operation status which has important practical significance There are a lot of researches on distribution network fault reconfiguration at home and abroad in normal operation state distribution network fault reconfiguration aims to minimize network loss or load balance in emergency state distribution network reconfiguration is a means of fault recovery with the minimum number of switching operations or maximum load bearing total supply recovery as the goal However the current distribution network fault reconfiguration strategy has the following shortcomings Whether the network optimization and reconfiguration in normal operation state or fault recovery reconstruction in emergency state are often passive which can not play a full warning role for the security of system operation which is not in line with the development needs of active distribution network The current distribution network reconfiguration goal is single which can not fully take into account a large number of external uncertainties in the operation of the distribution network thus causing potential security risks in the operation of the system Therefore this paper proposes an active reconfiguration strategy for distribution network after fault which is oriented to the improvement of maximum total supply capacity Then the models of wind power photovoltaic power and electric vehicle power generation are established and the Latin hypercube sampling method is used to obtain the power flow distribution of wind and light under such conditions Then taking the maximum total supply capacity of the system as the objective function an active distribution network model for maximum total supply capacity improvement is established Finally the total supply capacity of the distribution network before and after the reconfiguration is compared by an example It is verified that the reconfiguration strategy can effectively improve the power supply recovery ability of distribution network after failure The maximum total supply capacity of the distribution network is defined as the maximum load that can be carried by the distribution network when the N 1 calibration of all feeders and the N 1 calibration of the substation s main transformer are satisfied In the N 1 test the load transfer between the main transformer and the feeder the contact relationship between the main transformer and the feeder in the network the capacity of the main transformer and the feeder the overload coefficient of the main transformer and other practical operation constraints of the distribution network should be considered In addition to the overall TSC of distribution network the TSC model and calculation also need to give the load distribution on each main transformer and feeder when reaching TSC The maximum total supply capacity model based on feeder interconnection takes the maximum load of each feeder as the objective function In the formula is the maximum load of feeder and there are altogether N feeders in this distribution network The constraints of the distribution network s maximum total supply capacity model should meet the N 1 safety criterion N 1 safety criterion is that when the distribution network is in operation when a component in the distribution network such as the main transformer feeder etc failure occurs there will be no redundant user power outages in the network N 1 verification of distribution network mainly includes feeder N 1 and main transformer N 1 The N 1 check of the feeder is to test whether the feeder load can be transferred to other connected feeders when the outlet of a single feeder fails as shown in Equation 2 The main transformer N 1 check examines whether the load can be transferred to other main transformers in the station or to other substations through the network when a single main transformer exits as shown in Equation 3 The following figure 1 is a simple distribution network structure diagram as an example to analyze the security constraints of model N 1 Schematic diagram of simple distribution network structure Show All The constraint conditions of the maximum power supply capacity model which satisfy the verification conditions of feeder N 1 and main variant N 1 are as follows In Equation 2 the right side of the inequality represents the feeder capacity that is related to feeder represents the feeder load that is connected with feeder The right side of the equation 3 represents the rated capacity of the contralateral main variant represents the feeder load connected to the same main transformer after feeder failure represents other feeders or feeder segment loads connected with in the same main transformer after the failure of In Formula 4 is the load lower limit of a certain overloaded area and Z is the set of all main variables in the overloaded area most of the load in the area cannot be transferred off the grid so the total load of the main variable of the overloaded area is not less than the load of the overloaded area As shown in Figure 1 there is no overloaded area in the simple distribution network structure diagram If then the contralateral host becomes T3 Feeder 4 5 and 6 are of feeder 1 Feeder 2 is of feeder 1 Feeder 4 is the of feeder 1 so the N 1 safety check of feeder 1 shall meet the following requirements At present the more common wind speed model is the two parameter distribution model which can well describe the probability distribution of wind speed and its probability density function can be expressed as In the formula is wind speed and are the shape parameters and scale parameters of Weibull distribution respectively which can be calculated according to the historical wind speed data The output power of the fan can be obtained from Equation 7 In the formula is the output power of the fan and are respectively the starting wind speed cutting wind speed and rated wind speed of the fan is the rated capacity of the fan The output power of photovoltaic power generation is most affected by the intensity of the sun s light The Beta distribution is the most widely used probabilistic model to describe light intensity In the formula denotes the Gamma function and are the actual light intensity and the maximum light intensity respectively and are the shape parameters of Beta distribution can also be calculated using historical data The expression of photovoltaic output can be expressed as In the formula is photovoltaic output is the number of photovoltaic panels and represent the area and photovoltaic efficiency of the kth solar panel respectively The load has time series characteristics its active power and reactive power obey normal distribution respectively and the probability density functions are respectively where and are active load and reactive load respectivel and are expected value and standard deviation of active load respectively and are expected value and standard deviation of reactive load respectively In this paper it is assumed that only the load characteristics of EV are considered and the daily mileage satisfies the lognormal distribution and the probability density function is where is the daily mileage of the electric vehicle and are the expected value and standard deviation of the daily mileage respectively The charging time of EV is where is the charging time of the electric vehicle H is the power consumption per 100 km is the EV charging power is the charging efficiency LHS based uncertainty simulation includes two steps First LHS generates initial sample data according to probability distribution The second is to sort the samples according to the correlation of multiple random variables Among them obtaining the order matrix which can reflect the correlation is the key to simulate the correlation uncertainty Suppose you want to extract E random variables F represents any variable is the cumulative probability distribution function as is shown in Figure 2 Schematic diagram of LHS Show All As shown in FIGURE 2 the interval 0 1 was divided into F cells and the random value was selected in each small range the midpoint value of the interval was usually selected Then the corresponding sampling value can be obtained by calculating the inverse function as shown in the figure A sample matrix of order can be obtained by sampling E random variables In order to reduce the correlation between random variables and improve the accuracy of sampling the Cholesky decomposition method is used to sort the initial sample matrix It is defined that is the correlation coefficient matrix among E random variables and the sequence is carried out according to the following steps Generate a matrix L with an arbitrary array of 1 2 F elements in each row and calculate its correlation coefficient matrix Obviously it is a positively definite symmetric matrix The Cholesky method can be used to decompose it where is the lower triangular matrix Eliminate the correlation of L where the correlation coefficient matrix of is the identity matrix Break down where is the lower triangular matrix The correlation coefficient matrix of is approximately equal to by the following formula Update and rearrange the corresponding row elements in according to the size order of each row element in until the correlation coefficient matrix of meets the specific requirements According to the above steps according to the probability density functions of wind optical power generation and load LHS can be used to get the sampling matrix which takes into account the correlation among all DG and the random fluctuation of load among them In the formula and are the number of grid connected access points of wind turbine and photovoltaic power generation system respectively and are the th sampling data of wind speed and light intensity After the sampling matrix is obtained the wind speed and light intensity are used to calculate the wind turbine output and photovoltaic output respectively and the power flow distribution of the distribution network under the operating conditions is further obtained The correlation between DG and load can be fully considered in the optimization process and the results are more accurate The specific steps of LHS based uncertainty model are shown in Figure 3 Flow chart of simulation of uncertainty factors based on LHS Show All Schematic diagram of load transfer Show All Distribution network reconfiguration DNR is to change the topology of distribution network by changing the structure and switch position of distribution network so as to make the system reach the optimal operation state In the actual distribution network the transformer supplies power through single bus connection double bus connection and bridge connection The change period of this kind of network structure is often long but the position of Section switch and contact switch can be adjusted according to the dispatching needs which has greater flexibility in operation In comparison the change of grid structure can be properly ignored in a certain calculation period so it can also be properly ignored in the algorithm and the flexibility of Section switch and interconnection switch is mainly considered After the Section switch and tie switch are adjusted the total load between each group of feeders and feeder sections with connection relationship is not lost but is transferred between different feeders or feeder sections As shown in 1 2 and 3 show the tie switch Section switch and tie switch respectively When the tie switch between feeder F1 and feeder Section F2 changes from 1 to 1 the load change of F1 is and the load change of feeder Section F2 is The results show that the load variation of the two feeder lines which are related to each other satisfies the sum of 0 when the position of the sectionalizing switch between feeder 3 and feeder 4 changes from 2 to 2 the load variation of feeder F3 and feeder F4 can also satisfy that the load variation of the two feeder lines related to each other satisfies the addition sum of 0 According to this principle it can at when the position of Section switch or tie switch changes the load change of each group of feeders or feeder sections with connection relationship should meet the following formula In the above formula the subscript represents a group of feeders or feeder sections and with connection relationship represents the load variation of feeder or feeder Section after adjusting the Section switch or tie switch between and It should be noted that in reality the load variation of feeder or feeder Section is regulated by Section switch and tie switch so the load change is not continuously adjustable but a group of discrete numbers so is actually a group of discrete variables Then the discrete value of load variation can be expressed as In this paper the optimization goal of distribution network fault reconfiguration is to TSC of distribution network Considering the randomness of new energy and EV access the probability distribution function can also be used to approximate the actual situation Therefore the objective function of this paper is to maximize the expected value of distribution network power supply capacity In the formula is the total expected value of TSC of the system is the probability of occurrence of scenario is the expected value of TSC of random optimization distribution network considering the randomness of distributed generation and electric vehicle charging in scenario S is the total number of scenarios in formula 7 is the maximum total supply capacity of feeder when scenario occurs In the formula in scenario the right side of the inequality represents the feeder capacity with connection with feeder represents the feeder load with connection with feeder represents the output power of the fan in scenario indicates that the access point of the fan is in the area of the feeder represents the PV output in scenario indicates that the PV access point is in the feeder area represents the load change that may be caused by the switch change on the feeder formula 25 indicates that in scenario the right side of the inequality is the rated capacity of the opposite main transformer represents the feeder load connected to the same main transformer after feeder fault refers to the load of other feeders or feeder sections connected to the same main transformer with after the fault of main transformer where is located represents the load change possibly caused by the switch change on feeder represents the load change possibly caused by the switch change on feeder where represents the lower limit of the load of the heavy load zone Z in scene S and is the set of all main transformers in the heavy load zone In order to verify the effectiveness of the reconfiguration model and method proposed in this paper wind turbines are connected to feeder F1 and feeder F20 and photovoltaic is connected to feeder F10 and feeder F16 respectively The system structure before reconfiguration is shown in Figure 5 Schematic diagram of calculation example Show All In there are 2 substations 4 main transformers 20 feeder outgoing lines which are respectively recorded as F1 F22 20 feeder outlet switches numbered 1 20 10 tie switches numbered 100 110 and 21 Section switches numbered 200 221 The allowable capacity of feeders is 11 5MVA and the rated capacity of substation is 40MVA There is no heavy load area The existing original load curve is shown in Figure 6 In this example the parameters of distributed generation connected in the distribution network diagram are as follows wind power s s s MW Photovoltaic power generation each array has 100 modules single area 2 photoelectric conversion efficiency The initial output of distributed generation in each period is shown in Figure 7 below Original load curve Show All Initial output of distributed generation in each period Show All It can be seen from Figure 7 that the initial output of typical wind turbine is mainly concentrated between 1 00 8 00 and 17 00 24 00 and the initial output of typical photovoltaic is mainly concentrated between 7 00 20 00 From a macro point of view the DG output of 8 00 10 00 in a day is significantly lower than that in other periods The total DG output of each period is uneven with great randomness and volatility This example analyzes and compares TSC of distribution network before and after connecting to DG without considering distribution network reconfiguration The results are shown in Table 1 below It can be seen from table 1 that without considering the distribution network reconfiguration TSC of each main transformer will be improved to a certain extent after the addition of DG but the increase range is slightly smaller than that before DG access On the one hand it shows that the access of DG can improve the TSC of the distribution network On the other hand compared with the total output of DG the improved TSC of distribution network only accounts for a small part of it and the network loss is consumed Most of the DG output so the distribution network reconfiguration aiming at reducing network loss and improving the TSC is of great significance The topological structure of distribution network is changed by changing the structure of distribution network and switch position so as to realize the distribution network reconfiguration with the maximum total supply capacity as the goal The value range of load variation of each feeder is a group of discrete variables as shown in Table 2 below Adjusting the switch position of each feeder in the original distribution network can change the load of the feeder so as to achieve the purpose of load balance For example the switchable range of feeder F1 is 200 201 202 203 100 and the corresponding load variation is 1 5 1 2 1 1 0 6 0 00 and the switch selection range of feeder F11 associated with feeder F1 is 200 201 202 203 100 and the corresponding load variation is 1 5 1 2 1 1 0 6 0 00 The active distribution network reconfiguration model in Section 3 2 is used to calculate the distribution network reconfiguration strategy as shown in Figure 8 The mathematical essence of the reconstruction model is a linear programming model with discrete variables to find the optimal value The solution of the model can be solved by using Lingo programming tool In Figure 8 after reconstruction the switches on feeder F1 are selected between 201 and 202 and the switches on feeder F4 are between 210 and 220 Compared with before reconstruction the load of the system is more balanced at this time which improves the TSC of the distribution network to a certain extent Reconstructed grid structure Show All Available power supply capacity is a common index in the evaluation of TSC of distribution network It refers to the difference between the maximum load power provided by the distribution network and the current existing load when the local power grid meets the conditions of all bus node voltage constraints and all branch power constraints under the specified operation mode which is determined by the operation mode and load growth mode of the regional power grid According to the above description the mathematical expression of the available power supply capacity ASC of the distribution network is In the formula ASC is the available power supply capacity is the existing load of the distribution system According to this formula the ASC of each feeder Section can also be obtained Compared with TSC and ASC which are commonly used in traditional evaluation of total supply capacity of distribution network TSC represents the maximum total supply capacity of distribution network under the N 1 safety criterion without considering the real time load change When the load of distribution network changes its total supply capacity also changes with it TSC can not reflect this change and ASC is defined as the maximum power supply capacity and existing load Therefore ASC can better reflect the change of power supply capacity before and after the actual distribution network reconfiguration Table 3 above shows the comparison of total supply capacity of distribution network before and after reconstruction According to the results in the table the maximum total supply capacity before reconstruction is 123 08MW which is 132 96MW after reconstruction with an increase of 9 88MW On the basis of considering the original load the available power supply capacity is increased by 9 89MW It can be seen from that when the distribution network reconfiguration is not considered the ASC of the system will be improved by connecting with DG while the available power supply capacity will be improved on the original basis when the DG is connected to the reconstructed system It can be seen that the reconfiguration can greatly improve the TSC of the distribution network Available power supply capacity curve including DG output after reconstruction Show All Aiming at the problem of how to improve the maximum total supply capacity when DG is connected to the distribution network this paper defines the maximum total supply capacity index and establishes the maximum total supply capacity model with N 1 safety criterion as the constraint condition Then the random model of wind power and photovoltaic is established and the Latin hypercube sampling method is used to generate the initial sample data and the Cholesky method is used to rank the samples to obtain the power flow distribution of the distribution network under the operating conditions Based on the practical model of distribution network a post fault reconfiguration model of active distribution network for maximum power supply capacity enhancement is constructed The calculation formula reflects the randomness of DG output and load power and the influence of grid structure of power supply area on load backup power supply The proposed post fault reconfiguration optimization algorithm takes into account the extensive access of DG and fault reconstruction of distribution network It provides a theoretical basis for quantitatively evaluating the power supply reliability of the distribution network with DG as well as the formulation and evaluation of the fast recovery strategy Finally combined with the actual situation the changes of distribution network total supply capacity before and after DG connection and reconfiguration are compared The main conclusions are as follows The access of distributed generation can improve the total supply capacity of distribution network to a certain extent but due to the uneven load distribution the output of distributed generation can not be completely transformed into the system output In order to maximize the power supply capacity the optimal tie switch position is selected to minimize the system loss After adopting this strategy the power supply recovery ability of distribution network has been greatly improved which proves the effectiveness of the reconfiguration strategy"}
{"title": "Distribution Network Topology Identification Considering Nonsynchronous Multi-Prosumer Data Measurement", "number": "9328870", "authors": "[{'preferredName': 'Lizong Zhang', 'normalizedName': 'L. Zhang', 'firstName': 'Lizong', 'lastName': 'Zhang', 'searchablePreferredName': 'Lizong Zhang', 'id': 37088773610}, {'preferredName': 'Fengming Zhang', 'normalizedName': 'F. Zhang', 'firstName': 'Fengming', 'lastName': 'Zhang', 'searchablePreferredName': 'Fengming Zhang', 'id': 37089545339}, {'preferredName': 'Xiaolei Li', 'normalizedName': 'X. Li', 'firstName': 'Xiaolei', 'lastName': 'Li', 'searchablePreferredName': 'Xiaolei Li', 'id': 37089452082}, {'preferredName': 'Chunlei Wang', 'normalizedName': 'C. Wang', 'firstName': 'Chunlei', 'lastName': 'Wang', 'searchablePreferredName': 'Chunlei Wang', 'id': 37089451355}, {'preferredName': 'Taotao Chen', 'normalizedName': 'T. Chen', 'firstName': 'Taotao', 'lastName': 'Chen', 'searchablePreferredName': 'Taotao Chen', 'id': 37089451150}, {'preferredName': 'Qingqing Wang', 'normalizedName': 'Q. Wang', 'firstName': 'Qingqing', 'lastName': 'Wang', 'searchablePreferredName': 'Qingqing Wang', 'id': 37089452969}, {'preferredName': 'Huilin Hu', 'normalizedName': 'H. Hu', 'firstName': 'Huilin', 'lastName': 'Hu', 'searchablePreferredName': 'Huilin Hu', 'id': 37089451705}]", "abstract": "Accurate topology is the basis of fine management and safe operation of distribution network. With the development of distributed generation, more and more users participate in the distribution network, which makes the flow direction of distribution network more complex and brings some difficulties to the topology identification of distribution network. The existing distribution network topology i...", "text": "Accurate topology is the basis of fine management and safe operation of distribution network With the development of distributed generation more and more users participate in the distribution network which makes the flow direction of distribution network more complex and brings some difficulties to the topology identification of distribution network The existing distribution network topology i Distribution network is usually designed in a closed loop and operated in an open loop It stays radiant during normal operation When fault occurs or optimal control is implemented the segmented switch and contact line act and the topology of the distribution network changes Due to the problems such as missing and false information in the remote transmission data of distribution network or even the absence of communication channels at some nodes manual inspection and reporting are required which results in the failure of timely update of the network topology stored in the system thus affecting the safe and economic operation of distribution network In recent years the acquisition of accurate topological structure by installing measuring equipment or investing more labor costs has seriously affected the economic benefits of power grid companies Therefore topology identification of distribution networks has attracted more and more researchers attention Reference considering the outage of some nodes a hybrid integer quadratic programming MIQP topology identification model is constructed with branch power as variable In reference based on the line current data measured by the new equipment line current sensor a mixed integer linear programming model with line current as variable was constructed An optimization model with the minimum error of state estimation as the objective is established with the variables of node voltage amplitude branch power and branch switch state in Based on the measurements of node voltage and branch current the topology identification problem is modeled as a regularized alternating optimization problem and the online topology identification is realized In reference a linearized three phase power flow model is established and a learning framework based on node voltage reconstruction network topology is proposed Reference reconstructed the network topology by analyzing the correlation between node voltages in the historical measurement data of smart meters in medium and low voltage distribution networks Reference established a topology identification model based on matrix theory which can locate and detect the network topology only by inputting limited node voltage amplitude In reference firstly the bus of substation is partitioned and then the topology of each area is identified PMU has been widely concerned because of its high precision Based on a large number of historical voltage data provided by high precision PMU references mine the correlation between distribution network topology and voltage and realize the topology identification of distribution network All of these methods require the distribution network to be equipped with high cost PMU and provide a large number of historical data such as voltage amplitude and phase angle which are difficult to obtain in the actual distribution network These methods do not take into account the existing multi prosumer fusion data in the distribution network With the smart grid construction more and more measurement terminal layout in distribution network distribution network operation is becoming more and more complex different types of measuring equipment to provide important basic data for distribution network operation control the traditional power system measurement data through data acquisition and monitoring SCADA system for measurement and with the development of smart grid power distribution network in SCADA and advanced measurement system advanced metering infrastructure AMI became the primary source of measurement data sampling data may be out of sync in SCADA measurements at different locations which affects data accuracy AMI is an extension of the automatic meter reading system and the load side data is collected by smart meter SM among the three types of data mentioned above SCADA coverage rate is good but data accuracy is low The accuracy of PMU data is high but the coverage is poor AMI coverage rate is high data accuracy is high but the sampling period is long cannot meet the real time requirements of power system At present in most cases SCADA data PMU data and AMI data generally cannot meet the requirements of distribution network In order to solve the above problems this paper proposes a topology identification method of distribution network based on multi prosumer fusion data First combining with the existing distribution network measurement system analysis of the characteristics of different measurement data and then based on time scale PMU measurement data to other data alignment then consider different measurement system of the data acquisition frequency based on the linear extrapolation method for data acquisition of low frequency measurement system data is lacking based on the multi prosumer data fusion has developed an optimized model of distribution network topology identification The main contributions of this paper are summarized as follows This paper proposes a time scale alignment method for no homologous data based on PMU data In order to ensure that the data used in the topology identification model comes from the same time this paper considers the time and elevation accuracy characteristics of the measured data of PMU device and corrects the time stamp of measured data of AMI and SCADA system based on PMU so as to improve the data accuracy of topology identification model A pseudo measurement data generation method based on linear extrapolation is proposed in this paper Considering that the acquisition frequency of AMI and SCADA system measurement data is too low it is not conducive to topology identification The proposed linear extrapolation method is based on the data of two consecutive known time points in the past to obtain the measurement data of the next desired time point This method has a good effect on the short term trend prediction of power system with gradual change A topology identification model based on multi prosumer measurement is established in this paper The model takes adjacency matrix and some branch power as variables and aims at minimizing the weighted error between the branch power measurement and estimation of multiple measurement sections The most possible topology of the group of measurement data is found out from all possible operation topology of distribution network planning The rest of this paper is organized as follows The existing measurement system of distribution network is analyzed in Section II The time scale alignment method of multi prosumer data based on PMU and the pseudo measurement generation method based on linear extrapolation are proposed in Section III The distribution network topology identification model based on multi prosumer fusion data is established in Section IV The method and model are simulated and analyzed in Section V Finally some conclusions are drawn in Section VI The existing measurement system of distribution network is mainly composed of SCADA AMI and PMU The normal operation of the distribution network is a complex tree network radiation power supply mode which is composed of trunk branch lines The substation feeders are supplied by sections of trunk lines and the load is connected to the nearest through branch lines The spatial configuration diagram of the measurement system of distribution network is shown in Figure 1 Spatial configuration of distribution network measurement system Show All The SCADA measurement data is collected through RTU PMU is installed at the root node dynamic load access node contact switch and other important nodes of the distribution network 10 kV main line to collect dynamic data and the node voltage at the substation outlet can be observed so the accurate data generated based on PMU can synchronize the SCADA data and add time coordinates to it AMI The measurement data is collected by the intelligent meter of special transformer of load branch line and AMI measurement of branch and branch node is fully covered SCADA system is an important subsystem of energy management system which is installed in feeder switch distribution transformer outlet and open closed loop cabinet Its measurement includes node injection power branch power node voltage amplitude and branch current amplitude SCADA can also collect remote signal information such as switching amount and its data is collected once 2 5s The accuracy of data acquisition reaches level 2 with low accuracy and the data delay reaches the level of millisecond An important feature of SCADA is that its time of entry the time of entry into the database after the data is sent to the master station rather than the time of data acquisition Therefore when SCADA data is used for topology identification identification the time scale information needs to be processed to obtain the full section state information of the system PMU takes PMU as the basic acquisition unit and is installed at the root node dynamic load access node contact switch and other important nodes of the 10kV main line of the distribution network to collect dynamic data Its measurement includes the node voltage phasor and the branch current phasor The PMU is generally used at an interval of 10ms or 20ms and can receive a global positioning system GPS signal add time scale for synchronous measurement data calculation can obtain the power phase power Angle and other information The accuracy of the obtained data is up to 0 05 the data delay is up to the level of microseconds and the precision and sampling frequency of PMU measurement are better than the conventional measurement methods The functions of each measurement system are shown in Table 1 AMI is the data collected by the intelligent electricity meter installed on the client which can obtain the user s measurement value with rough time scale and carry out the remote measurement and energy consumption analysis of the electric energy Its measurement includes node voltage amplitude branch current amplitude node injection power and branch power AMI acquisition speed is relatively slow and the sampling cycle of smart meters for Chinese users is 15min or 30min AMI data acquisition accuracy can reach 0 5 level high accuracy data delay to reach the second level A high redundancy data source is provided for the topology identification of distribution network by using the current measurement data of distribution network As the time scale of AMI SCADA and PMU data is different it cannot accurately reflect the real situation of the distribution network at that time Therefore it is necessary to select the standard data source at that time as the benchmark and synchronize the measurement data of the remaining data source The sampling period and transmission delay of PMU data are millisecond and the sampling period and transmission delay of PMU devices are strictly synchronized under GPS time reference thus ensuring that all measurements of PMU have accurate time scale Therefore this paper chooses PMU data as the reference AMI data sampling period is long but high accuracy and with a time scale according to the time scale can be measured with PMU section data alignment However there is no unified time scale for SCADA data so the PMU measurement and SCADA measurement are simultaneously used for topology identification to align the data and consider the delay so as to obtain more time sectional measurement data in the gap between the synchronous time with SCADA data After the measuring device has different sampling speeds and ensures the same time section the data of sampling period length is interpolated and the data of sampling period length is interpolated in turn according to the period length so that the sampling period of measurement of multi prosumer data quantity is consistent From the previous analysis AMI data with a time scale can be aligned with PMU data only according to the time scale thus making the two measurements aligned with PMU data as the baseline Therefore we only need to discuss the alignment method of PMU and SCADA SCADA measurement data with large time delay measurement even the same time the measurement data SCADA measurement data to the dispatching control center of the main point to slower than PMU data thus can be spread to the main moment of PMU data set back a time window if received after the SCADA data then this SCADA measurement data of the sampling time agree with PMU data point and were used to express the direction finding of PMU SCADA and AMI respectively sampling period is sampling period is sampling period is according to the previous sampling period expression samples s times between two consecutive sampling points Between two consecutive sampling points is sampled r times and the data acquisition frequency of the three measurements is shown in Figure 2 Data acquisition frequency diagram Show All The sampling period of PMU data is fast while the sampling speed of SCADA data and AMI data is slow Therefore in many time sections there is no SCADA data and AMI data it is necessary to supplement high precision pseudo measurement data thus making the synchronization of PMU data SCADA data and AMI data Two methods linear interpolation and linear extrapolation can be used to generate pseudo measurement data When linear interpolation is carried out the data of the next time break point needs to be predicted and high precision prediction requires a lot of time However the linear extrapolation method is very effective for short time trend prediction of power system with gradual change In this paper the linear extrapolation method is adopted and its basic principle can be expressed as follows where is the direction finding quantity of AMI at any time between time and time is the direction finding quantity of SCADA at any time between time and time As shown in Figure 3 data at any time between and can be extrapolate using and Schematic diagram of linear extrapolation Show All As shown in figure 4 in distribution automation system database terminal for multi prosumer measurement data fusion assuming for millisecond PMU data refresh rate starting time per mu interval PMU measurement data among them Multi prosumer measurement data fusion Show All Flow chart of the algorithm Show All Synchronizes SCADA measurements of the system based on accurate data generated by a PMU with a timestamp In the same distribution network partition the same sampling pulse is used for SCADA data collection so that SCADA data can be guaranteed to be at the same time Assuming that the voltage of node is observable the instantaneous voltage value of node is where is the effective value of voltage is angular frequency is the primary phase angle These three quantities can be obtained by measurement so the of at any time is known For SCADA assuming that the sampling period is the SCADA voltage measurement value of time coordinate where is the number of sampling period Therefore in the time interval according to the values can be found within a certain time makes Therefore it can be assumed that one time coordinate of SCADA data within the distribution network partition is so that the time scale of SCADA data within the partition can be added by using the time marking property of PMU namely Hypothesis for second level SCADA data refresh rate the each time section measurement data including and SCADA measurement data one of them The data refresh frequency of SACDA is seconds and the multi period measurement data is obtained in the gap at the time of synchronization with the AMI data section so as to increase the measurement equations for topology identification AMI measurement data can be through own time scale and PMU data synchronization assuming that rs for minutes of AMI data refresh rate per rs time section measurement data including and measurement data from the AMI among them To sum up for a distribution network with multi type measurement data because the data refresh frequency of various measurement systems is very different the measurement equation is jointly established by PMU SCADA and AMI measurement data with multi period so as to meet the requirements of topology identification The problem of topology identification of distribution network based on multi prosumer fusion data can be understood as finding the most possible topology corresponding to the set of measurement data from all possible operation topologies of distribution network planning based on a small amount of multi prosumer measurement data In this paper based on weighted least square method a topology identification model is constructed to minimize the weighted error between the measured and estimated values of the branch power of multiple measured sections and take the branch power as a variable Based on this the following objective function is established where T is the number of measured sections is the weight of the section measured in unit t represents the set of all measurement data including real time measurement data and pseudo measurement data represents the measurement data of the t th section including node voltage amplitude node injection power and branch active and reactive power represents the variance of the i th measurement on the t th section represents the measurement equation of the t time section y is the selected state variable including the amplitude of node voltage and branch active and reactive power The constraints of topology identification model are mainly composed of power flow constraint and radial constraint Based on the linearized DistFlow flow model this paper deduces the flow constraints suitable for topology identification For a containing distributed photovoltaic power distribution system the power balance equation is as follows where and respectively represent the active and reactive power flowing into node i represents the output power of photovoltaic K represents the child node set of node i and represent the active and reactive load of node respectively and and represent the resistance and reactance of line ij respectively In practical problems the line loss is smaller than the line power so it can be ignored In addition in the topology identification problem the above equation of power flow is only valid when the line is in running state Therefore the element in the matrix is considered to be set as the switching state variable in the power flow constraint If is 1 it means that the line ij is put into operation and the power flows from to Based on the above analysis and the comprehensive consideration of multiple measured sections the power flow constraint in the topology identification problem is as follows where and respectively represent the active and reactive power of the branch i j flowing through the section at time represent elements in the Time matrix In addition as node voltage has little influence on the accuracy of topology identification model and will increase the solution time of the model the influence of node voltage is not considered in the constraint conditions of the model For topology identification problems generally known is the injected power of the node and part of the power with measuring device branches Considering that the measurement error of the device generally conforms to gaussian distribution the power flow constraint can be specified as follows where and respectively represent the measurement of the active power and reactive power of the branches of the measurement section at unit and respectively represent the estimation value of the active power and reactive power of the branches of the measurement section at unit and respectively represent the measurement of injected active and reactive power at the node of the measurement section at unit represents the node connection relation at time represents the error of each measurement By using not only the power on the branch of distribution network can be expressed but also the flow direction of the branch can be expressed In this way although a large number of prosumer groups participate in the distribution network through distributed generation which makes the flow direction of distribution network become complex can correctly express the complex branch power flow In addition this paper only considers the topology identification of the radial distribution network so it is necessary to add the constraint of the number of branches which is equal to the number of nodes minus the number of source nodes Equation 17 indicates that the connection relationship between nodes is 0 1 variable 0 means break and 1 means connection Equation 18 ensures that the distribution network topology is radial Since equations 13 16 contain terms of 0 1 variable representing switching state and multiplication of continuous variable representing branch power which belong to nonlinear constraints the following two inequalities are introduced to transform nonlinear constraints into linear constraints In which M is an arbitrarily large positive number so and in formulas 13 16 can be directly expressed by and In order to verify the validity of the proposed data fusion method and topology identification model experiments were carried out on a computer equipped with Intel Corei5 8500CPU to program the fusion of multi prosumer data of distribution network and an optimization model for topology identification was established to solve the model In this paper the feeder system as shown in Figure 6 is adopted to verify the effectiveness of the proposed method The feeder system has a reference voltage of 12 66kV and a reference power of 10MVA It has 33 load nodes 32 normally closed branches and 5 normally open tie lines Photovoltaic devices are added to some nodes of the system to indicate that prosumer groups participate in the distribution network through distributed generation IEEE 33 bus feeder with measurement placement Show All Assume that nodes 1 6 13 20 24 and 30 are SCADA measurement nodes and sampling is conducted every 10s Nodes 2 8 11 16 and 32 are PMU measurement nodes and sampling is conducted once every 1s Nodes 18 22 25 and 33 are AMI measurement nodes and are sampled every 1min In the optimal operation or fault switching the corresponding normally closed segment switch is opened and the normally open tie line is closed All the measured data are simulated by adding the results of power flow calculation to gaussian noise distribution The measured data follow the gaussian distribution as follows where is the measure measured at the unit at the time cross section is the true value of the tidal current measured at the unit at the time cross section is the variance of the measure at the unit at the time cross section For a measurement device with the maximum error the true value the standard deviation of the measure is The data obtained by the above method is used as the measurement data of the initial measurement device and the data generated by the time scale alignment and pseudo measurement are used as the multi prosumer fusion data Figure 8 is PMU SCADA AMI in 0 180s for the fitted with three kinds of measurement device node 2 injected active power measurement of the initial results figure 9 node 2 into the active power of the data fusion results three kinds of device of measurement data better but SACDA data error is bigger in the initial data the measurement error of AMI will be big but still within acceptable precision The improvement effect of PMU on the quality of measurement data is shown in the following picture Initial mixed measurement data Show All Mixed measurement data after fusion Show All Topology identification results corresponding to the initial data Show All As can be seen from Figure 7 among the initial data collected by the measuring device the sample data of PMU is very rich and the measurement accuracy is also high Because a large number of users participate in the distribution network through distributed generation the distribution of power flow in distribution network has changed greatly The power and voltage data collected by distribution automation system fluctuate with the output fluctuation of distributed generation Due to the low frequency of data collection of SCADA and AMI the sample data in the same time is relatively scarce and due to the time required for data uploading of SCADA system the data stored in SCADA system is not only not rich enough but also of poor quality As shown in Figure 8 after multi prosumer data fusion the sample number and accuracy of SCADA and AMI are improved which lays a good foundation for the subsequent rapid topology identification of distribution network The simulation assumes that the line 2 19 is disconnected and the tie line 12 22 is closed In this scenario topology identification of the distribution network is carried out In the simulation the influence of branch measurement power and node injection power combinations with different measurement errors on topology identification results is considered For each error combination topology identification is performed for 100 times respectively The accuracy of topology identification is defined as follows where represents the number of topology identification and represents the number of correct identification Considering the measurement data with different time sections and different degrees of errors the test results are as follows Among them the abscissa represents the real time measurement error 1 3 and the pseudo measurement error is 1 5 from left to right in each group of histogram By the figure 9 and 10 as you can see it can be seen from the above experiment results under the condition of the measurement error is smaller the topology identification results obtained based on the original data accuracy is above 90 and topology identification based on multi prosumer data fusion accuracy basic stood at 100 it can be seen that multi prosumer data fusion can significantly improve the distribution network topology identification accuracy However when the measurement error of node injected power data increases to more than 30 the accuracy of topology identification in both cases decreases significantly In addition it can be seen from the figure that the injected power measurement data of nodes with large measurement errors is the key to affect the accuracy of topology identification The corresponding topology identification results of multi prosumer fusion data Show All In addition the topology identification model based on multi prosumer fusion data does not need to wait for the measurement system to upload more measurement data for a long time so the results of topology identification can be obtained faster in the same time because the multi prosumer fusion data is more redundant than the original data In order to solve the problem that the existing topology identification methods are greatly affected by the quality of distribution network measurement data this paper proposes a distribution network topology identification method based on multi prosumer hybrid measurement The density of measurement data of distribution network is improved by linear extrapolation method and the topology identification model of distribution network is established based on branch power the following conclusions are obtained through simulation analysis The quality of data obtained from the measurement system of distribution network is chaotic The research on the fusion method of multi prosumer data can improve the quality of measurement data and provide high quality data for topology identification or state estimation of distribution network The accuracy of topology identification is mainly affected by the measurement data of node injection power with large measurement error so improving the accuracy of pseudo measurement can greatly improve the accuracy of topology identification A large number of prosumer groups participate in the distribution network through distributed generation which makes the flow direction of distribution network more complex By improving the measurement level of distribution network such as measurement redundancy data accuracy or data acquisition frequency the accuracy of distribution network topology identification can be improved and the influence of prosumer groups can be reduced To sum up the example carries out topology identification for different configurations of distribution network measurement system From the results it can be seen that topology identification using multi prosumer and multi time section measurement data can expand the application scope of topology identification and the accuracy of topology identification is related to the layout of measurement equipment and the accuracy of measurement data With the development of smart grid technology the large scale deployment of measurement terminals will provide high redundancy data sources for distribution network topology identification The method proposed in this paper has practical value and application prospect"}
{"title": "IoT Equipment Monitoring System Based on C5.0 Decision Tree and Time-Series Analysis", "number": "9334978", "authors": "[{'preferredName': 'Biaokai Zhu', 'normalizedName': 'B. Zhu', 'firstName': 'Biaokai', 'lastName': 'Zhu', 'searchablePreferredName': 'Biaokai Zhu', 'id': 37085858934}, {'preferredName': 'Xinyi Hou', 'normalizedName': 'X. Hou', 'firstName': 'Xinyi', 'lastName': 'Hou', 'searchablePreferredName': 'Xinyi Hou', 'id': 37089357941}, {'preferredName': 'Sanman Liu', 'normalizedName': 'S. Liu', 'firstName': 'Sanman', 'lastName': 'Liu', 'searchablePreferredName': 'Sanman Liu', 'id': 37089357477}, {'preferredName': 'Wanli Ma', 'normalizedName': 'W. Ma', 'firstName': 'Wanli', 'lastName': 'Ma', 'searchablePreferredName': 'Wanli Ma', 'id': 37089356182}, {'preferredName': 'Meiya Dong', 'normalizedName': 'M. Dong', 'firstName': 'Meiya', 'lastName': 'Dong', 'searchablePreferredName': 'Meiya Dong', 'id': 37086523693}, {'preferredName': 'Haibin Wen', 'normalizedName': 'H. Wen', 'firstName': 'Haibin', 'lastName': 'Wen', 'searchablePreferredName': 'Haibin Wen', 'id': 37089356545}, {'preferredName': 'Qing Wei', 'normalizedName': 'Q. Wei', 'firstName': 'Qing', 'lastName': 'Wei', 'searchablePreferredName': 'Qing Wei', 'id': 37089356617}, {'preferredName': 'Sixuan Du', 'normalizedName': 'S. Du', 'firstName': 'Sixuan', 'lastName': 'Du', 'searchablePreferredName': 'Sixuan Du', 'id': 37089357093}, {'preferredName': 'Yufeng Zhang', 'normalizedName': 'Y. Zhang', 'firstName': 'Yufeng', 'lastName': 'Zhang', 'searchablePreferredName': 'Yufeng Zhang', 'id': 37089356761}]", "abstract": "Abnormal traffic and vulnerability attack monitoring play an important role in today\u2019s Internet of Things (IoT) applications. The existing solutions are usually based on machine learning for traffic, and its disadvantage is that a large number of manual operations are needed in the classification process, and the adaptability is poor. Moreover, for unknown attacks, the system cannot make a relativ...", "text": "Abnormal traffic and vulnerability attack monitoring play an important role in today s Internet of Things IoT applications The existing solutions are usually based on machine learning for traffic and its disadvantage is that a large number of manual operations are needed in the classification process and the adaptability is poor Moreover for unknown attacks the system cannot make a relativ Driven by the rapid development of big data artificial intelligence and information communication technology the scale of the Internet of things is growing rapidly Internet of things technology and its related applications have also made innovative breakthroughs With the increasing number of IoT devices the IoT industry ushers in the golden development stage With the large scale application of the IoT attackers can also take advantage of it The attacker will access the device with vulnerability into the target network to hide and launch an attack at any time The architecture of IoT has its own characteristics once attacked it may lead to network paralysis causing great threat and loss to the country and individuals Compared with the traditional Internet this situation is more complex and difficult In the field of network security abnormal traffic monitoring and vulnerability scanning are always difficult to study There are many existing methods such as face recognition fingerprint feature and characterization acquisition RFID technology car networking smart grid etc Shangguan et al broke the disadvantage of using special hardware for phase determination in the past and propose a new face image recognition method The recognition image of face features is divided into several regions and the LBP data distribution is obtained from them so that the system can recognize faces more accurately With the continuous development of science and technology mobile devices and smart phones are widely used Various commercial applications came into being With the increasing deployment of large scale sensors unknown measurement errors are more likely to occur Based on this Xiang et al propose to use a two stage iterative algorithm to estimate the source existence source parameters and sensor noise iteratively so as to reduce the error Wang et al package and bundle tasks when collecting data and use the incentive mechanism of personnel scheduling to solve the problem of unbalanced participation Taking joint learning as an example mengkai song et al systematically combines the working principle of privacy preserving learning technology There are still many security threats in maintaining privacy preserving learning technology Fauri D et al define roles through the semantic information of the BACnet protocol realize the situational awareness of threats by constructing a knowledge graph of equipment information and achieve the purpose of abnormal monitoring of equipment Wu et al achieve the purpose of network security defense by optimizing the black and white list technology With the wide deployment of intelligent brain in cities wireless monitoring system involves the security and privacy of unauthorized video Cheng et al design a detection system called dewicam Arias O et al take Google nest thermostat and Nike fuelband as research cases to do in depth research on the performance of wearable devices in order to improve its security and privacy settings The system can detect the wireless camera based on the lightweight smartphone Xixi Li et al propose a method of automatically extracting information based on time series technology Because traditional data monitoring methods can no longer reflect the value and change range of data in real time some scholars propose adding time series to flow monitoring People have added time series technology to applications such as temperature currency exchange rates and stocks Taking RFID tag technology as an example many methods are still limited by software and hardware conditions and experimental scenarios First the error of the results obtained in most experiments is still relatively large and some are not suitable for the positioning of related objects Second in order to achieve absolute target positioning many methods need to use special hardware or pay attention to experimental deployment and invest more tags and antennas In order to accurately track the object in all directions Jiang et al add a specific phase model to the RFID system to obtain its position in three dimensional space Finally the system can accurately track the object in three dimensional space Guo et al propose to apply RFID in industry to detect liquid leakage By combining coarse grained RSSI and phase the system can detect liquid leakage by using the characteristics of inductive coupling between adjacent tags Based on the above research we propose a research method for monitoring of IoT devices based on C5 0 and timing analysis On the basis of IoT monitoring combined with decision tree C5 0 to build a feature classification framework The decision tree C5 0 is used as a feature classification module to process the collected data information which improves the efficiency of data processing and makes the feature classification more accurate This feature based time series analysis technique can optimize the real time monitoring system By establishing time series model the system analyzes and processes the attack information that has occurred This makes the whole monitoring system more stable and real time We design a matching system between data and equipment The system can match the processed data information with the device model one to one and judge whether the data is abnormal and infer whether the device has loopholes Our method is nested from data to equipment which greatly improves the accuracy of IoT equipment monitoring as shown in Threat model of internet of things Show All The design of the IoT equipment monitoring involves the following challenges High precision processing In machine learning classification the classification results are easily affected by noise and data redundancy which requires us to do a good job in data preprocessing and noise reduction The balance between local feature selection and computational complexity Traditional time series monitoring uses global known features but this paper uses computer view technology to transform data features into GAF At the same time the CNN LSTM combination model is added to this part for feature extraction and classification which not only improves the accuracy and real time performance of the system but also increases the difficulty and complexity of monitoring Data self adaptive collection This paper designs a data and equipment matching system This link is to realize the conversion of data to equipment The system constructs the data matching of IoT devices through white list technology The system corresponds the processed data information with the equipment model However building a matching system requires a lot of equipment data In terms of data collection and processing we should consider how to achieve adaptation We summarize the main contributions of this article as follows We propose a monitoring and forecasting system for IoT devices At present with the increasing number of IoT devices and large scale applications attackers take this opportunity to connect vulnerable devices to the target network and launch attacks at any time In this paper aiming at the current situation of network security the Device Hive IoT virtual platform is used to collect traffic data of IoT devices The system uses decision tree C5 0 and time series monitoring model as the information classification module and data module of the system respectively which improves the classification accuracy and improves the real time monitoring of traffic We use the CNN LSTM hybrid model for time series monitoring We add a time series model to the monitoring of IoT devices convert the time series data into a GAF graph and use it as a data feature of the attack information After that the pictures are input into the CNN LSTM combined model for training Therefore it is possible to obtain the monitoring results of IoT devices that may have vulnerabilities or attacks in the future which is more stable and real time for the entire monitoring system We design a matching system between IoT devices and traffic For the identification of Internet of Things devices most of them use deep learning for classification and the experimental process is relatively abstract and complicated Moreover the experiment can only detect whether the equipment is abnormal and cannot identify the vulnerabilities and attack types of the equipment This article introduces whitelist matching technology to filter out traffic that does not meet the rules At the same time the system can identify the models of IoT devices and realize the conversion of analysis results from data to devices The remainder of this paper is organized as follows Section II describes the system overview Section III introduces the system structure in detail Section IV describes the experimental design and performance of the system Section V reviews the related research works Finally Section VI concludes this work With the increasingly fierce network attacks traffic intrusion and vulnerability attacks against the IoT emerge endlessly This paper improves on the traditional abnormal traffic detection research and proposes an IoT device monitoring system based on C5 0 and timing analysis The main parts are as follows as shown in Overview of the system Show All The system uses the Device Hive IoT virtual platform to collect traffic data of IoT devices At the same time the system uses the KDD 99 data set for simulation experiments and randomly selects 10 data to form the data set of this experiment Data is preprocessed through data identification data cleaning data standardization and data normalization The system inputs the preprocessed data into the C5 0 decision tree model to realize the classification and recognition of attack types At the same time boosting and pruning methods are used to improve the accuracy of the model reduce the problem of over fitting of the decision tree and improve the accuracy Using visualization technology the time series data is transformed into GAF graph which is input into CNN model as feature information The key features obtained are combined with the selected features in the decision tree training process to form a new feature set which is input into LSTM model and finally the data monitoring information and abnormal trend are obtained Use visualization technology to convert time series data into GAF graphs input them as feature information into the CNN model combine the key features obtained with the selected features in the decision tree training process to form a new feature set and input the long and short term memory network LSTM model to finally obtain data monitoring information and abnormal trends The monitoring information is filtered twice through whitelist technology overlapping data is extracted the range of early warning retrieval is narrowed and the final early warning data is obtained then the early warning data is matched with the IoT device and the abnormal IoT device model is identified and the result is from data to Conversion of equipment After testing identify the IoT devices that are vulnerable to attacks and potential vulnerabilities and give warnings to them so that users can obtain the status information of the IoT devices in advance By comparing the characteristics of different data sets we believe that the KDD Cup 1999 data set is more suitable for our simulation experiments and it can prove the superiority of our method to a greater extent The KDD Cup 1999 data set is a new data set formed by Professor Sal Stolfo and Professor Wenke Lee through data mining and data preprocessing of the DARPA 98 data set It is more authoritative as the data set used in the KDD competition held in 1999 The data set contains approximately 5 million pieces of training data and 300 000 pieces of test data The data is divided into two categories normal and abnormal The abnormal types are divided into four major types of attacks DoS attacks U2R attacks R2L attacks and Probe The attack types are further subdivided into 39 types of different types of attacks There are 22 types in the training set and the test set contains the remaining 17 unknown attacks This paper randomly selected 10 from the KDD Cup 1999 data set as the training data sample for the experiment We use Device Hive platform to obtain IoT data information At the same time in order to ensure the accuracy of the experimental results we conduct simulation experiments on the KDD Cup 99 data set In order to detect abnormal traffic we need to mine the data However the data is incomplete and inconsistent This also brings many problems to data mining In order to obtain high quality data samples and lay the foundation for the smooth development of the subsequent work of network traffic detection the data preprocessing part is particularly important Therefore we perform data processing on KDD 99 data the processing process is as follows After opening the data set a plain text file will be obtained The function of data identification is to classify and identify the data according to the attack type The data that does not conform to the specification is checked and eliminated The redundant and repeated data are removed by using the text string processing method and the useful information is extracted These include centralization and normalization that is panning and zooming eliminate the differences between features and remove the data unit limit We need to calculate the average value of each attribute and standard deviation Among them is the average value of attributes and is the standard deviation of each attack type The data in the data set after data cleaning is standardized centered by the mean and then scaled by the standard deviation and finally a normal distribution with a value range of 0 1 is obtained The standardized formula is We use KDD 99 dataset to preprocess the data through various steps The simplified data set provides reliable data source for the subsequent training of classifiers and provides convenience for the follow up work as illustrated in KDD 99 data set pretreatment process Show All The C5 0 decision tree algorithm is used to classify the collected IOT data and at the same time the C5 0 training set is formed The working principle of C5 0 decision tree algorithm in this experiment as a method of classifying decision tree model C5 0 algorithm takes information entropy as the measurement to generate decision tree Based on information gain degree splitting method it iteratively splits until the sample subset cannot continue to be split and finally obtains the optimal classification result The faster the information entropy decreases the faster the segmentation threshold will also change In this paper C5 0 decision tree is used to distinguish the characteristics of IOT data and identify the IoT data with malicious attacks and vulnerabilities The decision tree algorithm constructs the decision tree and uses the method of approaching the discrete function value to achieve the purpose of data classification From ID3 algorithm C4 5 algorithm to C5 0 algorithm after continuous optimization and improvement the classification efficiency and accuracy have been greatly improved Therefore this paper finally selects C5 0 algorithm as the classification model In this paper a feature classification model based on decision tree C5 0 is designed After preprocessing the obtained data it is input into the training set of decision tree C5 0 model which is used as the feature classification module to classify the data information This provides a good data base for the subsequent time series model to monitor the traffic information of the Internet of things And it can improve the efficiency of data processing Gain S A is the information gain divided by attribute A which means that the information gain obtained by dividing the data set S by attribute A represents the degree of information eliminating random uncertainty under one condition represents a subset of the sample set obtained when the attribute A takes the value in the data set S When the attribute A takes the value the larger the characteristic intrinsic value SpliInformation The formula for the characteristic intrinsic value is Information gain rate calculation method The pruning method is used in the C5 0 decision tree model to improve the accuracy of the model As a regularization technique pruning can be calculated to make the model close to the optimal structure prevent overfitting and improve the accuracy index Based on the data set and its information gain rate build a decision tree where is the leaf node where the number of leaf nodes is and are the attributes of the node on the decision tree and can be regarded as one Parameters are used to adjust the size of the tree and the trade off between the fit of the tree and the data Then is the complexity of the tree and pruning is regarded as the calculation of the model loss value Then the loss function can be defined as With the rapid development of the network the data gradually presents the characteristics of quantification diversification and fluctuation at any time Traditional data monitoring methods have been unable to reflect the range of data values in real time so people have added time series technology to the daily temperature currency exchange rate stock and other applications In this paper time series model is added to the safety monitoring of Internet of things Firstly the data is transformed into GAF graph by visualization technology and it is used as the data feature of attack information Then one dimensional convolutional neural network CNN is used for training to obtain the key features of detection After that a new feature set is formed by combining the original features processed manually and the hidden features generated based on convolutional neural network The abnormal trend of Internet of things device traffic is obtained by inputting long term memory network LSTM Finally the system combines whitelist matching technology to filter out traffic characteristics that do not meet the rules as depicted in Flow chart of white list building based on time series Show All After preprocessing the time series data of 39 types of attacks GAF images are obtained We randomly selected 6 as shown in Mapping constellation of tags transmission Show All It is difficult to build a monitoring model by directly adding deep learning into time series Therefore we use Python library to convert one dimensional image into two dimensional image GAF Gram Angular Field GAF Its working principle is to transform one dimensional time series into polar coordinate system in Cartesian coordinate system and then generate GAF matrix by trigonometric function We assume that and are vectors in two dimensional space then the inner product is defined as The matrix is formed by the dot product of each set of vectors which is expressed as Assuming that the time series is we need to scale it to 1 1 to make the inner product more accurate The combination model of CNN LSTM is used to predict the devices that may produce vulnerabilities or be attacked in the future This can make the whole monitoring system more stable real time and adaptive Long short term memory network can solve a series of problems that may arise in long term prediction At the same time the visualization technology is introduced into the system which makes the global features localized and improves the accuracy of the model as shown in The workflow of CNN LSTM model Show All Neural network is a network structure composed of single neurons Convolution neural network can train and learn pixels audio and other objects through the connection of each layer and finally achieve the function of imitating biological vision and perception Generally speaking convolutional neural network is given an unknown picture or audio by building a model for feature extraction and the final output results can determine its type shape etc Moreover with the decrease of the level the accuracy of the extracted features will also decrease The closer to the bottom the more detailed classification features will be extract In one dimensional convolution the function of convolution is to extract data and obtain translation features in a certain direction as shown in We use one dimensional convolution to predict time series and extract sequence features We use one dimensional convolution to predict time series which is expressed as follows One dimensional convolution flow Show All represents a new sequence obtained from convolution of and Assuming that the element in is then the convolution is as follows It can be seen that the essential principle of convolution operation is to extract features accurately from cyclic product and addition operation It can be expressed as where is the convolution degree are time series and is the length of LSTM Long Short term Memory is a kind of long short term memory network which belongs to a type of time recurrent neural network RNN By training the data features it can solve a series of problems such as gradient disappearance and gradient explosion in the process of RNN training The core problem of LSTM lies in its information transmission path and the selection and processing of information to be preserved and forgotten in the training process that is the problem of cell state and gate LSTM has a total of three gates input gate output gate and forgotten gate which are expressed as In this paper the experimental environment is as follows the operating system is windows 10 the memory is 32 GB the Intel R core TM i7 8550u CPU is 1 80 GHz and the software environment is Pycham 2020 We use KDD 99 data set for simulation experiment Before classifying the data it is still necessary to process the KDD Cup 1999 dataset The tag item is the statistical diagram of the total traffic of attack type as shown in Figure 8 Through research and experimental analysis we put forward a research method of Internet of things equipment monitoring based on C5 0 and time series analysis which provides a new idea for the monitoring of IoT equipment We use KDD 99 data set for experimental test as shown in Table 1 Distribution of attack types in KDD 99 dataset Show All In the following we extract 7 graphs from 19 statistical distribution charts of attack traffic and normal traffic as shown in to These include the following attack types src bytes srrror rate dst bytes cont access control file times login failure times wrong segments and continuous Comparison of statistical distribution between attack samples and normal samples in type src bytes Show All Comparison of statistical distribution between attack samples and normal samples in type srrror rate Show All Comparison of statistical distribution between attack samples and normal samples in type dst bytes Show All Comparison of statistical distribution between attack samples and normal samples in type cont Show All Statistics of access control file times Show All Statistics of login failure times Show All Statistical chart of number of wrong segments and continuous type Show All Because decision tree C5 0 has the advantages of low data preparation requirements it can analyze large data sources in a relatively short time and obtain relatively accurate results Therefore this paper uses decision tree C5 0 method to establish a classification model for traffic information The system reduces the dimension of the data input from feature extraction so as to build a model and identify the attack type At the same time the system introduces boosting technology which can effectively improve the accuracy and achieve the purpose of model data pruning and optimization The basic principle of boosting technology is to give each sample an initial weight value improve the training set and construct a decision tree Among them the system selects samples to establish a new decision tree model It should be noted here that the larger the weight is the more likely it will be selected According to this rule the system iterates the error samples repeatedly until the classification error is less than the specified threshold as shown in Comparison of statistical distribution between attack samples and normal samples in type src bytes Show All In this paper open data set KDD 99 is used for simulation experiment The system combines C5 0 decision tree and timing series technology to monitor the data of IoT The results show that the proposed method has some advantages over the traditional methods in monitoring accuracy Therefore the application of this system opens up a new idea for the traffic analysis of IOT devices In the IoT monitoring we can collect data through the device hive IoT virtual platform Most of the devices used in the monitoring platform are representative and commonly used in the market including 43 kinds of intelligent temperature management system household appliances etc In order to make all the tested devices generate enough training data we repeat the process 10 times The experimental architecture is shown in Experimental architecture diagram Show All The experimental operation process is as follows After getting the data of IoT devices through Device Hive and training the initialization settings are set to activate the device At the same time with the help of the application software provided by the supplier the device is connected to WiFi or Ethernet and the credentials will be synchronously transmitted to the user network After a series of operation the equipment will be forced to restore the factory settings which is convenient for repeated testing of equipment data Decision tree C5 0 is adopted as a training model and the system inputs processed data for classification training At the same time the system uses methods such as pruning to improve the accuracy of the model This can solve the problem of overfitting of the decision tree thereby improving the accuracy index Then the system builds a time series model to analyze the classification results and obtain data monitoring and analysis results Compared with the traditional Internet of things equipment identification and monitoring methods the accuracy and stability of this paper are improved The system optimizes the processing of data duplication and redundancy At the same time we introduce C5 0 decision tree to split the sample subset until the sample subset cannot be split In this way the optimal classification results can be obtained Moreover time series analysis will affect the real time performance of feature classification and data monitoring The system adds time series technology to predict abnormal traffic which makes traffic monitoring more timely and can avoid many unnecessary losses caused by malicious attacks We take the accuracy detection rate and false positive as the measurement of decision tree C5 0 classification evaluation is used to evaluate system performance is regarded as the ratio of C5 0 model to detect intrusion attack is the proportion of false positive rate After testing and evaluating the classification effect Table 2 shows the accuracy rate detection rate and false alarm rate of the four attacks in the overall attack classification The experimental results show that the overall detection rate and accuracy of the data samples are improved We review the related works in three aspects Doshi R et al carry out a distributed denial of service DDoS attack on the Internet infrastructure to avoid a series of security risks that may be caused by its connection with insecure IoT devices This method promotes innovative research on the direction of IoT attack traffic under automatic detection Gendreau A et al believe that in a heterogeneous network comprehensive monitoring of information flow will become more difficult This means that the ability to detect intruders in each type of device will directly determine the smooth spread of system information Doshi et al take ios devices as the research object and conduct in depth research on data filtering of IoT devices Trilles S and others borrow IoT platforms or software to coordinate device connections and program applications in the network Among them the microservice architecture and serverless computing are used as examples to avoid potential vulnerabilities of the Internet of Things that may cause a series of hidden security threats Machine learning can realize data mining and information acquisition as a method widely used in classification detection and data processing It can provide a new idea to solve the security problems of the IoT Majumdar et al deeply analyze a variety of machine learning technologies and conclude that machine learning can be used not only in intelligent monitoring traffic analysis model training and classification but also in cloud computing big data and artificial intelligence Based on machine learning Doshi and others build the overall framework of the IoT security system and realize a reasonable security management method Tarek Salah et al design and implement a classifier based on C5 0 decision tree build a hardware based intrusion detection system model This method detects most network attacks with FPGA technology and greatly improves experimental detection rate In the field of machine learning classification and detection the application of various data sets in experiments makes the detection results more accurate Shone N et al propose a new deep learning technology for intrusion detection This technique utilizes an unsupervised feature learning asymmetric depth automatic encoder The deep learning classification model is evaluated by KDD 99 and NSL KDD data sets Devi et al classify and compare the KDD 99 and NSL KDD datasets This detection system can classify and process different data With the overall situation of network attack becoming more and more serious the research of intrusion and traffic monitoring is particularly important Based on this the method adopting time series analysis technology is widely used in the field of network security maintenance Li et al use deep learning technology to obtain features from time series convert time series information into images and improve self system adaptability and reduce human intervention Taking stock selection as an example Zhang et al construct a new stock selection model by combining decision tree C5 0 algorithm with factor analysis Experimental results show that the model can effectively help investors avoid risks Different from the traditional way Shang et al study nonlinear time series analysis and process traffic management data through simulation experiments The experiments show that it is feasible to apply time series technology to traffic data monitoring and processing and improve the accuracy and real time of data Taking joint learning as an example mengkai song et al systematically combines the working principle of privacy preserving learning technology There are still many security threats in maintaining privacy preserving learning technology Nanda et al use sensor data for smart city data monitoring They propose a method to build a recurrent neural network model by using two attention mechanisms to establish a connection between intelligent buildings and intelligent transportation In this work we introduce a monitoring system for IoT devices based on decision tree C5 0 and timing analysis The system is based on the KDD Cup 99 data set for simulation experiments The experimental results can achieve an average error rate of 3 22 and can monitor unknown attacks with 96 accuracy By comparing with traditional flow monitoring methods excellent performance can be obtained It is proved that this method has its superiority in the monitoring of IoT devices"}
{"title": "Decentralized Control of Multiple Strict-Feedback Systems With Unknown Control Directions", "number": "9343855", "authors": "[{'preferredName': 'Junmin Peng', 'normalizedName': 'J. Peng', 'firstName': 'Junmin', 'lastName': 'Peng', 'searchablePreferredName': 'Junmin Peng', 'id': 38520269300}, {'preferredName': 'Shenping Xiao', 'normalizedName': 'S. Xiao', 'firstName': 'Shenping', 'lastName': 'Xiao', 'searchablePreferredName': 'Shenping Xiao', 'id': 37577363400}, {'preferredName': 'Chao Huang', 'normalizedName': 'C. Huang', 'firstName': 'Chao', 'lastName': 'Huang', 'searchablePreferredName': 'Chao Huang', 'id': 37088473652}]", "abstract": "This paper is concerned with the decentralized control problem of networked high-order nonlinear systems that can be transformed into strict feedback forms with parameter uncertainties and unknown control directions under a directed communication graph. A decentralized controller is designed recursively for each agent to realize output synchronization and guarantee the overall system to be bounded...", "text": "This paper is concerned with the decentralized control problem of networked high order nonlinear systems that can be transformed into strict feedback forms with parameter uncertainties and unknown control directions under a directed communication graph A decentralized controller is designed recursively for each agent to realize output synchronization and guarantee the overall system to be bounded Due to its wide application collaborative control of networked systems has attracted considerable attention among the control communities since the 21st century The early relevant work focuses on networked agents with linear dynamics or first order nonlinear systems Currently much attention has been paid to network agents with nonlinear dynamics such as fractional order systems chained form systems stochastic systems lower triangular systems nontriangular systems Euler Lagrange systems and systems with quasi one sided Lipschitz nonlinear dynamics Besides uncertainty and disturbance have been considered to reflect the actual situation of the agent including unmeasured velocities input constraints unmodeled dynamics stochastic disturbance When the agents have high order dynamics and uncertainty people always change the objective from state consensus to output synchronization It is worth noting that the control direction or the sign of the high frequency gain determines the characteristics of feedback negative feedback or positive feedback which is important in controller design Usually it is assumed to be positive and known However this assumption becomes unrealistic in some cases such as uncalibrated visual servo control in and autopilot design of surface vehicles In these cases the Nussbaum function has been proposed to solve the problem of stabilization or regulation for a single system with an unknown control direction However rare work has been done for multiple systems with unknown control directions because it is difficult to show cooperative behavior without sacrificing distributivity Another basic requirement is that the networked system should remain bounded Since the entire system may have multiple Nussbaum type functions interacting simultaneously analysis becomes extremely difficult Our previous work has studied the consensus of multiple first order integrators that is the agent has a model of with unknown high frequency gain After that we have extended the agent s dynamic to a high order lower triangular system but the signs of all are assumed to be the same Recently has applied the same hypothesis about the unknown control directions but with prior knowledge of their bounds Motivated by the above discussion this paper aims to solve the output synchronization problem in more complex situations i e networked agents with unknown control directions and uncertainty but no additional assumptions about their control directions are required The objective of this paper is to design a decentralized controller for each agent whose dynamic is in the form of high order strict feedback with parameter uncertainty and unknown control coefficient sign and amplitude to achieve output synchronization and the entire system remains bounded The main contributions of this brief are summarized as follows Existing works have assumed that agents have known control directions or unknown but identical control directions or unknown and nonidentical control directions but with a restriction on its bounds In this paper all these restrictions are removed that is these agents can have nonidentical unknown control directions and the controller is designed without prior knowledge of the The analysis function is crucial for recursively designing the decentralized controller Usually the Nussbaum function appears in the derivative of the Lyapunov function In this paper the Nussbaum function appears in the analysis function itself rather than its derivative which not only facilitates proofs based on contradictions but also expands the scope of application of the schedule The rest of the paper is organized as follows In Section II we present some basic notions and preliminaries of the graph theory Then the problem formulation and control objective are proposed in Section III In Section IV the decentralized controller is proposed A numerical example is proposed in Section V to illustrate the effectiveness of proposed decentralized controllers Finally the conclusion is drawn in Section VI Throughout this paper denotes the family of real matrices means that is a positive negative semi definite matrix means that is a positive negative definite matrix denotes the null space of matrix denote the least upper bound and the greatest lower bound respectively and is the classical signum function For a continuous differentiable function the row vector of is Here we introduce some graph terminologies that can also be found in A weighted graph is denoted by where is a nonempty finite set of nodes an edge set is used to model the communications among agents The neighbor set of node is denoted by means that there is no information flow from node to node A sequence of successive edges in the form is defined as a path from node to node For an undirected graph it is said to be connected if there is a path from node to node for all the distinct nodes A weighted adjacency matrix where and if and 0 otherwise In an undirected graph where the information exchange is uniformly balanced In what follows we set when without loss of any generality In addition we define the in degree of node as and is thus the in degree matrix Then the Laplacian matrix of graph is It is well known that is the null space of Laplacian matrix when the communication graph has a spanning tree with is some constant and Barbalat Lemma Consider the function If is uniformly continuous and exists and is finite then A Nussbaum type function is the one with the following properties Commonly used Nussbaum type functions include and Consider a network of agents with the dynamic of agent described by where are the state input and output of agent respectively is the control direction i e high frequency gain whose sign and amplitude are both unknown is a vector of unknown parameters are sufficiently smooth known functions bounded for all where Control objective Our goal is to design for all agents in graph such that their outputs are asymptotically synchronized i e while the overall system is guaranteed to be bounded In existing works the coefficients sign and amplitude are assumed to be known or unknown but identical or some assumptions have been made on In this paper the signs of can be nonidentical which means that some agents may have positive high frequency gains while others may have negative high frequency gains In the circumstance the voltage equalization control of power source which is composed by multiple ultra capacitors can be formulated where the reversal of positive and negative poles may happen occasionally either by misoperation in assembling or inappropriate use Next we have the following main result of this paper For the sake of simplicity in what follows is used to denote the information of i e is a state vector of for all For agent described by 2 there is a decentralized controller with the Nussbaum function and is updated by where with and are calculated by parameter updaters are updated by with and tuning functions are such that output synchronization of the network can be achieved i e and the overall system is guaranteed to be bounded provided that the digraph has a spanning tree Now we start the step by step design procedure The proof is carried out at the same time Step 1 Define The time derivative of along 2 is Rewrite 10 as with where is the neighborhood error of agent Define and 11 is then equal to Step 2 Note that it is shown that with and are chosen as 6 and Then 13 is reduced to with and Define By 14 and 12 one has with are control gains Step m Note that hence where and are chosen as 6 and 8 such that 17 is equal to with Define We can see that Final step n Similarly by one has Define Noting that the derivative of is with and are chosen as 6 and 8 Choose the control protocol as 3 where and are updated by 4 and 7 Then 24 is reduced to Next we will prove the existence of the closed loop system solution on the time interval Let us denote by that the maximum interval of existence of the closed loop system solution First assume on the contrary that at the state escapes to the positive infinity no matter what the sign of is there is a strictly increasing infinite sequence with the property such that which means On the other hand 25 shows that which contradicts Therefore is within a compact set on i e is bounded on Similarly if we assume that at the state escapes to the negative infinity the same conclusion can be given due to the property of the Nussbaum function see 1 Second assume on the contrary that at the state escapes to infinity In that case where is some constant number while the escape of to the positive infinity after a certain time which contradicts Hence is bounded on By inductive argument are bounded on i e the closed loop system is bounded Integrating 25 one has Moreover Hence is integrable Notice that the derivative of is a polynomial that composed by the states of the closed loop system Hence the derivative of is also bounded i e is uniformly continuous By Barbalat Lemma one has i e or No matter which case when the directed communication graph has a spanning tree one has which completes the proof Compared with the existing works the main contribution of this paper is that the unknown in agent s model can be with different signs without other additional assumptions It is a significant progress for many practical problems For example in the circumstance the voltage equalization control of ultracapacitor type power source can go on wheels even in fault mode Usually when combined with backstepping design the Nussbaum item appears in the derivative of the Lyapunov function see But in this paper the Nussbaum item has been added in the analysis function itself see 22 There are two reasons for this Firstly it facilitates the contradiction argument when the Nussbaum item appears in rather than since we can get the conclusion that if is unbounded which contradicts 24 Secondly it is essentially an extension of the known control direction case A stabilizer for a single system is firstly proposed in our previous work in this way To the best of our knowledge it is the first time that this method is applied in a multi agent system By applying the above scheme the outputs are driven to the equilibrium eventually If we alter the controller slightly outputs of agents can be synchronized to other desired trajectories as well For example by setting then where is the desired trajectory Distributed control and decentralized control are applicable to large scale systems composed by multiple subsystems Each subsystem can only acquire partial information The main difference lies in their control objectives In distributed control researchers mainly focus on subsystems behaviors such as consensus or synchronization of the multi agent system But in decentralized control not only subsystem s behavior but also the overall system s behavior are considered just like the control objective of this paper i e output synchronization as well as the boundedness of the overall system are the points of focus Typically in traditional decentralized control the partial information needed for subsystem i is not formulated by a communication graph i e subsystem i may receive redundant information which does not appear in its controller Motivated by the cooperative control in this paper we formulate the information exchange by the graph theory such that not only a graph condition has been added in the result but also the information subsystem i received is used in its controller In this section an example is presented to verify the effectiveness of the proposed controller in Theorem 1 To this end we consider the output synchronization problem of a group of three agents denoted by 1 3 in Fig 1 For the sake of simplicity are set to be 1 when The dynamic of agent is the same as that in with and are the state output and input of agent respectively is the unknown parameter Communication topology Show All To be more specific the initial values of agents are chosen as and all the initial values of updaters are set to be 0 Then we set control gains and system parameters is chosen here as the Nussbaum function By Theorem 1 a decentralized controller 3 together with parameter updaters 7 are designed for each agent The simulation results are shown in to It can be seen in Fig 2 that the outputs of all agents in the network are asymptotically synchronized i e shows that are bounded and shows that all are bounded It can be seen in and that both parameter updaters i e for agent itself and for its neighbors are also guaranteed to be bounded So the overall system is bounded Thus the simulation results well confirm the theoretical issues in Theorem 1 Compared with the related references the proposed controller results in faster converge of the parameter estimators Trajectories of output Show All States of Show All Trajectories of Show All Parameter updaters Show All Parameter updaters Show All Our future research will focus on the convergence speed of such a networked system with time varying delays and agents in non strict feedback forms This paper investigates the output synchronization problem of multiple strict feedback systems with parameter uncertainties and unknown control directions in the circumstance of directed communication graph and agents may have different control direcions which is the first highlight of this paper A decentralized controller is designed recursively for each agent such that output synchronization can be achieved Meanwhile the overall system maintains bounded A new analysis method is proposed in which the Nussbaum item appears in the analysis function itself directly rather than its derivative such that the proof can be simplified significantly which is the second highlight of the paper The simulation example shows the efficiency of the presented scheme"}
{"title": "Research on Cluster Economy Operation Model Considering Customer Satisfaction", "number": "9347469", "authors": "[{'preferredName': 'Wei Zhao', 'normalizedName': 'W. Zhao', 'firstName': 'Wei', 'lastName': 'Zhao', 'searchablePreferredName': 'Wei Zhao', 'id': 37577627300}, {'preferredName': 'Wenqi Xue', 'normalizedName': 'W. Xue', 'firstName': 'Wenqi', 'lastName': 'Xue', 'searchablePreferredName': 'Wenqi Xue', 'id': 37087410030}, {'preferredName': 'Peiqiang Li', 'normalizedName': 'P. Li', 'firstName': 'Peiqiang', 'lastName': 'Li', 'searchablePreferredName': 'Peiqiang Li', 'id': 37673373300}, {'preferredName': 'Xuexian Tang', 'normalizedName': 'X. Tang', 'firstName': 'Xuexian', 'lastName': 'Tang', 'searchablePreferredName': 'Xuexian Tang', 'id': 37088847706}, {'preferredName': 'Ertao Lei', 'normalizedName': 'E. Lei', 'firstName': 'Ertao', 'lastName': 'Lei', 'searchablePreferredName': 'Ertao Lei', 'id': 37085784437}, {'preferredName': 'Hanbin Diao', 'normalizedName': 'H. Diao', 'firstName': 'Hanbin', 'lastName': 'Diao', 'searchablePreferredName': 'Hanbin Diao', 'id': 37088529480}, {'preferredName': 'Xiaoxiu Lv', 'normalizedName': 'X. Lv', 'firstName': 'Xiaoxiu', 'lastName': 'Lv', 'searchablePreferredName': 'Xiaoxiu Lv', 'id': 37088654016}]", "abstract": "A Real-time electricity market transaction is an important method that is used to realize the interactive balance of source-network-load in a new generation power system. Based on the multi-camp economic choice and the needs of national conditions, the emergence of cluster operators is inevitable. Due to stability and development of photovoltaic users, it is important to evaluate the importance of...", "text": "A Real time electricity market transaction is an important method that is used to realize the interactive balance of source network load in a new generation power system Based on the multi camp economic choice and the needs of national conditions the emergence of cluster operators is inevitable Due to stability and development of photovoltaic users it is important to evaluate the importance of Photovoltaic power generation can effectively alleviate the current energy shortage and environmental pollution problems Its major application is distributed household photovoltaic which has advantages of being close to the user side flexible in construction scale simple in installation and wide in application range The implementation of policies and cost reductions has improved the development of power generation Presently the grid connected power operation mode is self use surplus electricity access to the Internet Most of the producers and sellers trade with the power grid in cluster mode to gain more from the electricity utility Based on China s national conditions and the state of the multi party economy it will be historically inevitable to construct operators that are within the electricity selling market and the market structure regulations It is therefore necessary to execute the cluster economic operation model that is widely dominated by intermediate operators This is to improve on market vitality The operation model of cluster economy has three elements of pricing benefit and ranking Scholars have established several pricing strategies and benefit models LIU et al outlined a benefit cost model of user participation in Demand Response DR LI et al used fuzzy maximum satisfaction method by applying the 1 n master slave game to construct the user benefit function the price response and power supply pricing optimization of micro grid Referring to the domestic policy of household photovoltaic Mary and others put forward a power cost model associated with the net load of the system and then put forward a DR model for photovoltaic users based on game theory and proved that there is a unique Nash equilibrium strategy for this game problem Ma et al outlined a game model between community operators and consumers using the master slave game theory The study verified the advantages of improving the benefits of both parties and optimizing the load characteristics of the system Tushar et al applied the energy management scheme of wireless city based on the non cooperative Stackelberg game which reduces the total operating cost of the community energy management center and improves the residential units with maximum distributed energy Wang et al and Liu et al researched on the users photovoltaic output power and electricity consumption and found that it has an accurate prediction of at least one hour The former puts forward a pricing model of photovoltaic users group electricity price based on Stackelberg game The latter continues to study the influence of user behavior on operators revenue and finally proposes an economic penalty model aiming at the difference between the actual output of photovoltaic power and the predicted value of load power Ma et al studied the joint operation mode of cogeneration and photovoltaic users and proposed an internal pricing model of microgrid Lin et al applied fuzzy double objective method to normalize user s cost and dissatisfaction However literature used energy transaction model based on double Stackelberg game for power supply companies micro energy networks and users while literature used an optimization model and solution method based on non cooperative game for CCHP energy supply network The daily electricity consumption of users mainly considers three factors electricity utility economy and satisfaction User benefit model proposed does not consider the utility of electricity Literature did not consider the user s satisfaction with electricity consumption thus lacking its comprehension Several studies have focused on photovoltaic user groups on its economic operation but limited on the evaluation of the importance of users in the group However to establish a stable and prosperous photovoltaic user group it is necessary to solve the technical difficulties and the evaluation of user contribution From the perspective of user group managers it was found out that distinguishing significant users and giving incentives usually economic rewards is beneficial to the stability of the group From the user s point of view motivated users are likely to consume more electricity thus promoting the development and improvement of photovoltaic user groups The PV photovoltaic user group has a reasonable importance evaluation and reward system which will attract other PV users to join the group and make the PV user group grow and develop Therefore this paper used the users electricity consumption benefit model that takes into account the utility economy and satisfaction retrieved from the user s point of view For the photovoltaic users the optimal operation of internal electricity price included First computing the contribution value of users in the group to operators revenue This was done by studying the revenue essence of group operators using the cooperative game theory Second the evaluation index of users operators and power grid was defined and the photovoltaic user group evaluated and ranked using the TOPSIS method Finally using the Stackelberg game a 1 n electric energy trading model of operator photovoltaic user group was constructed to complete the design of cluster economic operation Photovoltaic user group is composed of users and cluster operators Users are both producers and consumers The transaction relationship and price setting are shown in Figure 1 Cluster trading mechanism framework Show All The cluster operator formulates the cluster tariff and and It uses to acquire the surplus energy of surplus power users and uses to sell energy to the consumers who are short of electricity If the overall power consumption of the cluster is greater than its photovoltaic capacity the cluster operator will purchase the power of the large grid with and the cluster operator will purchase power from the large grid with if the overall power consumption of the cluster is greater than its photovoltaic capacity In this process it maximizes its own income through price difference Therefore is not lower than the grid price and is not greater than the grid price Users follow the cluster operator and respond to the demand according to the electricity price to maximize their own electricity use efficiency All electric energy transactions in the cluster are realized by cluster operator A day is divided into periods on average to form period set The number of users constitutes the PV user group set In the period the cluster price set by the cluster operator is and When user n trades with the grid separately the energy consumed by user is which is also known as the original planned electricity consumption The actual energy consumed in the photovoltaic user group is and the energy generated by the photovoltaic system is then the amount of electricity the user needs to trade with the cluster operator is When user n needs to sell electricity to cluster operator when user n needs to purchase electricity from cluster operator Therefore it is not necessary for users to participate in the transaction According to the purchase and sale behaviors of consumers and power grid at that time all users are divided into power purchase users and power sales users In the existing literature the difference between the actual power consumption and the original planned power consumption is used as a function of power consumption comfort or satisfaction When the actual consumption of electricity is less than the original plan the satisfaction function of electricity consumption is negative otherwise the reproduction is positive Equal is zero For PV users the power consumption satisfaction function has the following characteristics If the actual power consumption is less than the planned power consumption that is the power consumption satisfaction value is negative which indicates that the power consumption satisfaction of consumers decreases and the function value decreases rapidly with the decrease of actual power consumption When the actual electricity consumption is equal to the planned power consumption that is the satisfaction function of electricity consumption is If the actual power consumption is greater than the planned power consumption that is the power consumption satisfaction value is positive which indicates that the power consumption satisfaction of consumers increases The function value increases slowly with the increase of actual power consumption and the increase speed decreases gradually In this paper the power consumption satisfaction model of PV user in the i th period is as follows In formula and are the satisfaction parameters of user in i th period and and the requirements of different types of users for electricity consumption satisfaction can be described by adjusting and Let the planned power consumption of user in the i th period be and by adjusting the actual power consumption the user satisfaction function under different parameter values is shown in Figure 2 User satisfaction function with different parameters Show All The use of logarithmic function to describe the utility of electricity consumption has achieved positive results The overall electricity consumption benefit of users is composed of the utility of prosumers and the electricity transaction cost The cluster operator makes the maximum profit by setting the cluster price and users respond to the demand according to the cluster price In the existing power consumption benefit model the sensitivity of user power consumption adjustment to cluster electricity price is too high which will lead to excessive adjustment rate of user power consumption This new function is used to solve the problem of user satisfaction In the formula the overall electricity consumption benefit of user in the i th period Utility coefficient whose value is related to user demand and time period It can be seen from formula 4 that when the actual electricity consumption increases the electricity utility and satisfaction of electricity sales users increase and the electricity sales income decreases the electricity consumption utility and satisfaction of power consumers also increase and the electricity purchase cost increases and shows a negative decrease The actual power consumption is determined by balancing the electricity sales revenue or purchase cost customer satisfaction and electricity utility so as to maximize the overall power consumption benefit The first derivative of benefit function is as follows By continuing to derive the user benefit function with respect to the actual electricity consumption the second derivative a of the benefit function can be obtained as follows In this formula and so thus the benefit function is a strictly convex function about For the maximum benefit of user in the i th period the optimal power consumption is unique Suppose that when the cluster electricity price the photovoltaic users use electricity according to the original plan that is and the benefit is the most When the utility coefficient can be obtained by calculation In which is a positive value When the cluster electricity price formulated by CO is and and other parameters are known so that The theoretical optimal electricity consumption of user in the i th period can be obtained by using fzero function in MATLAB In the i th period the role of purchasing and selling electricity in the cluster of photovoltaic users is determined by the net power under the initial situation When the power price and set by the operators meet the conditions of the users will not change their roles and their benefits will increase The proof is as follows For electricity consumers When the price of internal electricity sales is then the user s electricity consumption benefit function and its first derivative are as follows When and the derivative of utility function is a decreasing function of so makes and the benefit of users is maximum Therefore the user s role in selling electricity does not change and the amount of electricity sold increases When the electricity selling user trades with the power grid at the price and the electricity consumption of the user is the benefit function of the user reaches the maximum According to formula 4 the value of the benefit function at this time is When the electricity selling consumers trade with the operator CO at the cluster price assuming that the actual electricity consumption according to formula 4 the value of benefit function of power selling users is as follows It is known that and by comparing equation 10 with equation 11 we can know that And because is not the optimal power consumption under the cluster electricity price According to the foregoing it can be known that there is an optimal electricity consumption which makes and so means that the benefits will increase after the sales users trade with operators When the optimal power consumption satisfies the condition the optimal power consumption of users is When the solution does not satisfy the condition that is because the power consumption benefit of users decreases in the interval In addition because is adopted at this time which can maximize the power utilization benefit of users under the condition of satisfying the power consumption restriction In the same way the power consumers will not change their role in purchasing electricity and with the increase of purchasing power the benefits of users will increase When the power consumption restriction condition is satisfied but when it is not satisfied that is then The proof is over In order to study the relationship between the overall electricity use efficiency and the cluster electricity price Suppose that the electricity consumption The power consumption The power consumption satisfaction parameters are the power grid price is the relationship between power consumption efficiency and electricity consumption under different electricity prices is shown in Figure 3 Relationship between user s benefit with power consumption in different price Show All It can be seen from the figure that when the cluster selling price increases the optimal power consumption of power selling users decreases and its power consumption benefit increases when the cluster purchase price decreases the optimal power consumption of power purchasing users increases and its benefit also increases As the leader of cluster power trading cluster operator guides users electricity consumption behavior by setting internal transaction price and it is also the settlement center of all users income and expenditure Users who buy electricity and sell electricity then the total amount of users who buy and sell electricity in the i th period is When the operator sells the remaining electric energy to the grid at the price of when the operator purchases power from the grid at the price of to meet the load demand of users In order to maximize the revenue CO needs to formulate appropriate cluster tariff At this time the operator s revenue can be expressed as follows The internal price psi and pb are decision variables and the solution of the optimal cluster price of period i can be expressed as follows The above formula is a nonlinear programming problem and the optimal cluster price can be solved by using the interior penalty function method For one cluster operator N photovoltaic users who can respond to the demand of Stackelberg game The game rules are as follows a leader cluster operator formulates and publishes its strategy SL cluster tariff and from its strategic space SL in the period and the other N players PV users who become followers observe it sl Select its optimal response power consumption and ensure that The leader maximizes his own income and the follower adjusts the electricity consumption according to the cluster electricity price to maximize its own power consumption benefit function The cluster price set by operators in the I period is and which satisfies The game process is shown in Figure 4 Game process flow chart Show All If the Stackelberg game is to reach Nash equilibrium solution the profit of the leader cluster operators and the benefits of the following photovoltaic users should reach the maximum Theorem 2 in and Theorem 1 in prove in detail that the corresponding optimal cluster electricity prices and exist and are unique when the operator s income is maximum Using the interior penalty function method to solve equation 15 the optimal cluster price can be obtained When the internal optimal electricity price is determined let use the fzero function of MATLAB software to solve the ideal optimal electricity consumption and then according to the user s power consumption constraints determine the only optimal electricity consumption of user in the i th period so as to maximize the benefits of users At this time Stackelberg game reaches Nash equilibrium The demand response process that is the optimal pricing process of photovoltaic user group is shown in Figure 5 Game process flow chart Show All Detailed description is as follows In the i 1 time period the electricity consumption and photovoltaic power generation of each user in the next time period are predicted and they are determined as user types according to the net power Substituting the optimal power consumption xnio when photovoltaic users trade with the power grid the power consumption satisfaction parameters of users at the i th moment and the power grid price into formula 7 and calculating the utility parameter of the next optimization section Each user sends information such as and the role of participating in the transaction to the operator Operators can get the optimal internal electricity price that can maximize their profits according to user information and formula 15 Each user determines the optimal electricity consumption that maximizes its utility according to the cluster electricity price issued by the operator and the demand response process in the next optimization period is completed From the previous part it can be seen that adjusting the electricity load in response to the user s demand will not change their participation role From the user s point of view the main benefit of photovoltaic users is the power consumption cost saved by their own use When the users have surplus electricity to share with the users in the group it can reduce the power purchase price within the group and save the power consumption cost of the power shortage users Moreover users are more willing to join the photovoltaic user group with relatively abundant surplus power so the users who share more energy will be loved by other users From the operator s point of view the revenue of operators is related to the mutual use of electricity by producers and sellers Whether users are surplus or short of electricity they contribute to its income The greater the contribution value is the more important it is From the perspective of power grid the larger the total installed capacity of the whole photovoltaic user group the more attention will be paid by the power grid and the greater the installed capacity the greater the potential of providing electric energy Therefore the installed capacity of users is also a factor to be considered in evaluating the importance of users Therefore the criteria for evaluating the importance of users are as follows a shared photovoltaic energy b contribution to operators revenue c installed capacity of users The power sharing behavior of residual power users is the fundamental reason for operators and other users who are short of electricity The power shared by user in a calculation cycle is The photovoltaic user group led by the operator is essentially the operator alliance photovoltaic users and then trade with the grid which is equivalent to the alliance game only increases the alliance revenue Most of them are obtained by operators and other photovoltaic users get the remaining part of the increased revenue The contribution value of user to the operator s revenue is the user contribution value in the alliance game minus the revenue in the cluster The contribution value of user in a calculation cycle is calculated as follows The larger the installed capacity of the whole PV group is the more attention the grid attaches to the PV user group Therefore the installed capacity of user should also be considered in order to rank the importance of users in the cluster Power sharing can not only save the cost of electricity purchase but also increase the profit of cluster operator In addition in order to encourage users to share electric energy this paper puts the evaluation index A in the first place Indicator B can not only reflect the profit contribution of residual power users to the cluster operator but also indicate the revenue contribution of power shortage users to the cluster operator Therefore the evaluation index B is ranked second and the evaluation index C is ranked third The ranking results are as follows 1 sharing photovoltaic energy 2 contribution to operators revenue 3 installed capacity of users The higher the ranking of evaluation indicators the greater the weight coefficient According to the weights from ranks method the weight coefficient of evaluation criteria was calculated In the formula represents the total number of evaluation criteria and represents the weight coefficient of the evaluation criteria The basic principle of TOPSIS evaluation method the idea of approaching the ideal solution Based on the normalized original matrix the optimal scheme and the worst scheme expressed by the optimal vector and the most column vector respectively in the finite scheme are found out and then the distance between the evaluation object and the optimal scheme and the worst scheme is calculated respectively to obtain the relative closeness between the evaluation object and the optimal scheme This is the basis of evaluation This paper defines the value of user under After preprocessing the user s data by linear transformation the normalized decision matrix Z is obtained In the formula represents the maximum value of all users corresponding to the standard The weighted normal matrix X is obtained by multiplying Z by the weight coefficient Defining ideal solutions and negative ideal solution by The distance from user i to ideal user and negative ideal user is calculated Calculate how close the user is to the ideal user in the cluster Rank the importance of photovoltaic users from big to small by Five users with photovoltaic power generation system in a region constitute a photovoltaic user group They respond to the demand through Stackelberg algorithm under the guidance of cluster operator When the distributed generation output of the cluster is 0 all the electricity is purchased directly from the grid by the operators At this time Therefore this paper only studies the time period of distributed photovoltaic power output The installed capacity of each user in the photovoltaic user group is shown in Table 1 The typical solar volt output power curve and the daily load consumption curve of users are shown in figures 6 and 7 Prosumer photovoltaic output power in each period Show All Prosumer power consumption in each period Show All According to the distributed photovoltaic grid price in most regions of China the benchmark grid price of coal fired units is 0 4 yuan kW h for parameter and is the commercial electricity price which is 1 0 yuan kW h Users electricity satisfaction parameters and can take any value under the condition of satisfaction according to the requirements of users in each time period For the convenience of calculation here we uniformly take It is assumed that the requirements of five users on electricity consumption satisfaction are the same in each period Due to the need to meet the profit demand of operators and power grid in real life as well as the power supply line demand of power load the supplementary conditions of simulation are as follows and The shared photovoltaic energy in the user calculation period T is as follows Table 3 and table 4 show the user s income within cycle T under the cooperative alliance mode and the internal tariff mode respectively Based on the data in tables 3 and 4 the revenue of users to operators is calculated according to formula 17 as shown in Table 5 When the internal electricity price is implemented the revenue of cluster operator in calculation period T is 473 07 yuan and the sum of users contribution to cluster operator s revenue is equal to that of cluster operator which verifies the correctness of calculation of contribution value According to the data in Tables 1 to 3 and equations 19 20 the normative decision matrix Z is calculated According to formula 18 the weight coefficients of evaluation criteria are calculated as follows According to formula 21 the weighted gauge matrix X is obtained The value of the closeness between the user and the ideal user in the photovoltaic group calculated by formula 22 26 is shown in Table 6 According to table 1 table 2 and table 5 the installed capacity of user 1 is the largest and that of user 4 is the smallest User 5 has the largest contribution to the operator s revenue and user 4 has the smallest contribution to the operator s revenue User 2 shares the most electric energy while user 5 shares the least and 0 energy It needs to purchase electricity from the cluster operator at each time There is no research on the importance evaluation and ranking of users in the photovoltaic user group in the existing literature so this paper assumes that the importance evaluation and ranking is based on the single attribute of users Photovoltaic user group is operated under the management of operators assuming that operators start from their own revenue and rank according to the contribution value of users to their own revenue Compared with the comprehensive evaluation method proposed in this paper the comparison of user ranking results is shown in Figure 8 User importance ranking Show All As can be seen from the above figure the ranking results of user 4 are the same under the two evaluation methods Other users rank differently under the two evaluation methods User 5 ranks first in the evaluation method based on contribution value but ranks fourth in the comprehensive evaluation method The ranking results of the two evaluation methods are quite different Based on the analysis of user 5 s electricity consumption behavior it does not share energy with other users during the day s operation of photovoltaic user group From the original intention of establishing photovoltaic user group encouraging users to share electricity has poor performance In the comprehensive evaluation method users are encouraged to share electric energy and their ability to absorb excess photovoltaic energy and installed capacity are also considered Therefore the ranking under the comprehensive evaluation is more reasonable and fair The operator formulates the internal price to maximize its own revenue The user carries out demand response according to the cluster electricity price to maximize its comprehensive benefit The calculation formula of the electricity consumption adjustment rate of user in the period is as follows In this paper The comprehensive benefit model considering the satisfaction degree of electricity consumption and the original benefit model without introducing the satisfaction degree function are used to calculate respectively The adjustment rate of electricity consumption in each time period after each user s demand response is shown in Figure 9 10 Electricity adjustment rate considering electricity satisfaction Show All Original electricity adjustment rate Show All According to the figure the maximum load adjustment rates of users 1 2 and 3 all occur at the time of selling electricity and the maximum load adjustment rates of users 4 and 5 occur at the time of buying electricity The five users are reduced by 49 54 50 96 and 68 respectively Compared with the above figure the new power consumption benefit model can effectively reduce their own power consumption adjustment rate thus ensuring users power consumption comfort It is assumed that users in the group use the same satisfaction parameters in one day means that the user does not consider the power consumption satisfaction and the other three groups of satisfaction parameters meet the requirements of In this way the utility coefficient of electricity consumption is the same when the satisfaction parameters of users are different The absolute value of maximum adjustment rate of users under different satisfaction parameters is shown in Table 7 It can be seen from table 1 that the smaller the customer satisfaction parameter the lower the absolute value of electricity adjustment rate of residual power users and power shortage users The smaller the of the residual power users it indicates that the consumers have high requirements for electricity consumption and will not easily reduce their own power consumption The smaller the of the power shortage users is the higher the power consumption will not significantly improve the power consumption comfort so they are not willing to use more electricity In order to maximize the profit operators will adjust the internal price The internal price of the typical day is shown in Figure 11 below Photovoltaic user group internal price Show All It can be seen from the figure that after taking into account the user s satisfaction with electricity consumption the price of electricity purchased by power consumers through operators is higher than that before The price of electricity sales is lower than the original internal price except 12 00 According to the above analysis it can be seen that the income of cluster operator is increased secondly compared with the non cluster mode users can reduce the electricity consumption cost of power consumers and improve the electricity sales income of residual power users and the comprehensive benefit of electricity consumption of users is increased Finally compared with the electricity consumption benefit model without considering the satisfaction degree the model users can effectively reduce the adjustment rate of electricity consumption and implement the optimal power consumption according to the different requirements of different users To solve the problem of high electricity consumption adjustment rate when the PV users respond to the cluster electricity price demand the paper introduces the electricity consumption satisfaction function and proposes an optimal pricing model for PV users using the master slave game Moreover to promote the stability and development of photovoltaic user groups the paper analyzes the nature of operators revenue and uses cooperative game theory to calculate the contribution value of users in the group to operators revenue From the three aspects users operators and power grid the importance evaluation index of users is defined and a strategy for evaluating the importance using TOPSIS method is proposed with the aim of completing the comprehensive ranking of producers and sellers The proposed model revealed that comprehensive ranking of users is more fair and reasonable which can effectively encourage users to install household photovoltaic equipment and share electricity hence promoting the virtuous circle of household photovoltaic industry On electricity consumption adjustment rate the proposed model showed that the electricity sellers are reduced by 50 and electricity buyers are more than 50 which effectively guarantees the user s electricity experience and is more suitable in real life It can be implemented according to different requirements of different users On Optimal electricity consumption aspect the electricity utility showed that cluster mode is beneficial where by it reduces the electricity consumption cost of users improves the electricity sales income of surplus users and increases the comprehensive users electricity consumption In terms of operators revenue it was proven from the price comparison chart that there is increase in profitability The developed example verifies the superiority of optimized pricing model the effectiveness of the user benefit model the feasibility of the calculation method of the contribution value and the rationality of the evaluation method Compared with the existing mode the electricity price calculation mode and the ranking strategy of users in this model are more reasonable and its operation mode can more powerfully encourage users to generate electricity stimulate the development of household photovoltaic increase the installed capacity and further move towards carbon neutrality In conclusion the model proposed in this paper has a strong practical reference value in the future of the ever expanding household photovoltaic industry However the proposed model assumes that the satisfaction parameters of users in the group are similar This is not the case in the real life where different families have different satisfaction parameters for electricity consumption at varied time period Therefore the study recommends further study on the influence of power consumption satisfaction parameters on cluster economic operation model"}
{"title": "Hyper-Dimensional Computing Challenges and Opportunities for AI Applications", "number": "9354795", "authors": "[{'preferredName': 'Eman Hassan', 'normalizedName': 'E. Hassan', 'firstName': 'Eman', 'lastName': 'Hassan', 'searchablePreferredName': 'Eman Hassan', 'id': 37088966908}, {'preferredName': 'Yasmin Halawani', 'normalizedName': 'Y. Halawani', 'firstName': 'Yasmin', 'lastName': 'Halawani', 'searchablePreferredName': 'Yasmin Halawani', 'id': 37085544769}, {'preferredName': 'Baker Mohammad', 'normalizedName': 'B. Mohammad', 'firstName': 'Baker', 'lastName': 'Mohammad', 'searchablePreferredName': 'Baker Mohammad', 'id': 37574033300}, {'preferredName': 'Hani Saleh', 'normalizedName': 'H. Saleh', 'firstName': 'Hani', 'lastName': 'Saleh', 'searchablePreferredName': 'Hani Saleh', 'id': 37391220900}]", "abstract": "Brain-inspired architectures are gaining increased attention, especially for edge devices to perform cognitive tasks utilizing its limited energy budget and computing resources. Hyperdimensional computing (HDC) paradigm is an emerging framework inspired by an abstract representation of neuronal circuits\u2019 attributes in the human brain. That includes a fully holographic random representation, high-d...", "text": "Brain inspired architectures are gaining increased attention especially for edge devices to perform cognitive tasks utilizing its limited energy budget and computing resources Hyperdimensional computing HDC paradigm is an emerging framework inspired by an abstract representation of neuronal circuits attributes in the human brain That includes a fully holographic random representation high d Advancements in deep learning DL algorithms has outperformed conventional machine learning ML approaches in many applications image classification voice recognition activity recognition and object tracking Convolutional neural networks CNN such as AlexNet provide excellent classification accuracy at the cost of large memory storage memory access and computing complexity ML has traditionally been implemented in the cloud and data centers like Big Blue Google etc The need to move the processing of ML algorithms to edge devices is gaining importance for many reasons Firstly some applications are sensitive to response time such as an autonomous vehicle since they cannot tolerate the network s latency Secondly security and privacy as storing sensitive information on the cloud is vulnerable to hackers and viruses Third moving the data to from the cloud is costly in terms of power and resources To this end many critical domain applications such as health care and autonomous vehicles found the intensive ML algorithms impractical for real time edge devices Therefore it is crucial to design an efficient algorithm to perform the cognitive tasks and specialized hardware to provide high efficiency for edge devices Inspired by the brain s computational abilities with the advancements in memory technologies hyper dimensional HD computing has open new avenues as a potential light weight classifier for resource systems to perform a diversity of cognitive tasks Hyper dimensional computing focuses on dimensional expansion rather than reduction by emulating the neuron s activity However due to the large size of the brain s circuit the neural activity patterns can be modeled as a point in high dimensional space that is with hyper vector For vectors of thousands of dimensions e g 1000 it is called an HD vector This article focuses on reviewing the state of the art SOTA designs exploiting the HDC paradigm for classification tasks Also it highlights the main operations of HDC and compares it to the SOTA computing paradigm for classification tasks CNN The main reasons for conducting such a study are to 1 provide an overview of several methodologies commonly used in HD computing to solve classical and new cognitive tasks 2 describes existing HD architectures and highlight their implication on the system s overall performance 3 the impact of HD vector size and training set size on the system accuracy is analyzed 4 compare the HDC with the competing CNN paradigm in terms of efficiency and accuracy The outline of this paper is as follows the data representation and the fundamental operations in hyper dimensional space are highlighted in Section II Section III presents the core of the HD computing framework for cognitive tasks and some of the algorithms that have been designed to unlock their potential HD hardware architectures and memory design details are drawn in Section IV Section V reviews major current applications that employ HDC frameworks Section VI depicts a reasonable comparison between the SOTA CNN model and the HD model for solving classification tasks The paper concludes with the pending challenges and possible future works in Section VII In this section the roots of HDC used to represent human memory perception and cognitive abilities are presented using the mathematical proprieties of HD space which is built on rich linear algebra Brain inspired computing using HDC is carried on large components with independent identical distribution i i d and could be binary or non binary Hyper dimensional vector HD or symbolic vector architecture are common names for presenting data in high space The main inspiration behind HDC comes from Kanerva s work in to represent entities using several thousand dimensions vectors and manipulate them utilizing the traditional computing style Consider a case of the binary representation of 10 000 dimension vectors in HD space having this staggering number of unique vectors 210000 it is unlikely that any system will require this number of vectors to represents its variables HD vectors are generated independently and randomly with equal probability for zeroes and ones Using this assumption the average Hamming distance between any two vectors in the space is 5000 bits Also most HD space is gathered around 50 bits of standard deviation SD in a binomial distribution SD depends on the vector s size which means that for large size vectors the binomial distribution becomes thinner and the majority of the space is concentrated around 0 5 normalized Hamming distance Furthermore it is important to ascertain the inherent symbolic nature of HD computing One of the first observed extraordinary properties for HDC resides in its ability for analogical and hierarchical reasoning Capitalizing on that HD can be used to build complex data structures such as sequences images and lists In the HD domain three main operations namely multiplication addition and permutation referred to as MAP operations are utilized for vectors modeling Multiplication Binding is used to bind two HD vectors together which is usually done using XOR bitwise operation The output HD vector is orthogonal dissimilar to HD vectors being bound The binding operation is invertible Also binding distributes over addition preserving the distance between vectors Addition Bundling is used to combine different HD vectors into a single HD vector The resulting HD vector is similar to each component used in the bundling The final HD vector is binarized using a bitwise sum threshold of vectors which yields 0 when of bits in the vectors are zero and 1 otherwise That happens when the number of vectors in the set is odd For of being even number of zeros equal number of ones one more random HD vectors added to the set to break the ties Besides the terms threshold sum majority sum and consensus sum are used interchangeably to represent the resultant vector and denoted as where are HD vectors Permutation Shifting is used as an alternative approach to bind the HD vector with a special kind of matrix called permutation matrix It is important for the data sequence where the order is important For example it is often convenient to use a fixed permutation denoted as to bind the item s position in a sequence to an HD vector representing the value of the item in that position Because permutation is merely re order it preserves the distance between vectors A versatile HD machine should handle the variety of input data types and scale with an incremented number of inputs without affecting its fidelity It demands efficient encoding algorithms that affect the system performance and the memory optimization for hardware realization Fig 1 demonstrates the general HD pipeline for supervised classification tasks It consists of three main components related to the essential HD pipeline stages Item Memory IM mapping inputs to high space starts as shown in Fig 1 with assigning unique HD seeds representation for the bases defined over the given application For example Latin letters are considered bases for language recognition Every basis in the set is assigned a randomly HD vector and saved in an item memory IM Seed vectors selection could be orthogonal such as letters in language recognition However representing integers or continuous values in a particular order in the same way is not appropriate Therefore a continuous item memory CIM is used where two close numbers have a small distance in their corresponding HD vectors and are considered semi orthogonal It is important to stress that this representation is kept constant during training and inference phases for the intended classification task Nevertheless the memory storage for seed vectors cannot be avoided as it is required to retain the seeds during training and testing stages Encoding module combines all encoded hyper vectors in the training stage to form one HD vector representing each class The same encoding model is used to map the query HD vector which will be compared later with all other classes in the inference stage Associative Memory AM stores all trained HD vectors to be used later for the inference The essential function of AM is to compare the incoming encoded query HD vector with the stored classes and return the closest HD class vector using the appropriate similarity metrics The two similarity measurements adopted in current HD encoding algorithms are the Hamming distance that utilizes XOR operation and the cosine similarity which uses the inner product Few works used the overlap between coded vectors for measuring the similarity between HD vectors HD general pipeline with main stages including IM CIM where main input s features integers continuous HD vectors are randomly generated and stored Encoding module where all features HD values and features HD position are combined using MAC operations to form an abstracted HD representation for the given input In the training stage all HD vector samplers are combined to generate a class prototype stored in associative memory for inference Show All In HD a universal encoder capable of mapping any arbitrary data type into HD space does not exist Each type of encoder should be able to map application specific data after a proper pre processing to a suitable architecture able to accomplish the cognitive classification tasks The first step towards a general HD architecture is to demonstrate essential stages of HD algorithms in an abstracted manner Moreover in this section we highlight the main encoding architectures that have been discussed in the literature Few simple techniques have been used repeatedly for HD encoding Table 1 summarize these techniques in addition to the number of bundling stages used during encoding Most of the HD applications which will be analyzed in details next section are using the following techniques Multi set of sequence gram is used for text classification and modeling sequences In gram the original data is re modeled as long chunks of sequences After mapping seeds set into a hyper vectors and having a set of sequence they are encoded as Likewise all remaining sequences for a particular input are encoded and bundled to generate the HD representation for the particular sequence Features superposition in this technique feature vectors are extracted and mapped to hyper vector Assume that we have a feature vector with dimension Then for each position in the feature vector a hyper vector is formed Similarly each value in the feature vector is assigned hyper vector To correlate the value with position the binding operation is used In the final step all samples are encoded and super imposed to get the final representation of the feature For classification tasks the final step in the HD encoding module is the bundling of all individual representations Building on that HD algorithms can be divided into two types according to the number of bundling used during HD processing A single stage algorithm where the bundling operation used only once Each term in the bundled vector is formed by binding its inputs and or their permutation In other words where represents the input stream of finite dimension of HD vectors represents the number of terms in the class and the th term in  is represented by Each term depends on certain input value some occurring by only using the binding operation according to their position in the set X and some requires permutation where is positive integers account the permutation A multi stages algorithm uses the output of a single stage algorithm as an input to another single stage HD algorithm Similarly we can construct any multi stage HD architecture by combining smaller single stage algorithms in a hierarchical manner The multi stage algorithm defines the complex non linear relationship between variables such as time position and value It is important to mention that only a few researchers are using the multi stages HD architecture for cognitive classification tasks while the majority are focusing on using the single stage algorithm for its simplicity There are several forms of mapping the data from its original space to HD space and are classified as follows Binary where the value of HD elements are 0 1 Ternary in which HD elements take values of 0 1 1 Non binary HD vector where elements in such vector are represented using fixed point complex or floating point number Dense HD vector in the dense HD vector all HD elements have an equal distribution probability Sparse HD vector in the sparse model the 0 value dominates the HD vector with low presence for 1 or 1 Authors in proposed a sparse random binary HD vector utilizing permutation based binding that operates on a segmented vector which resulted in efficient energy deployment Introducing sparsity in the HD vector reduces the number of multiplications and additions required to encode the single HD vector Holographic Graph Neuron HoloGN for one pass pattern learning was introduced in The sparse code representation of the pattern improves noise resistance in the architecture compared to the original HoloGN abstracted representation leading to a tangible improvement in pattern accuracy In the author tested a synthesized and real world data using two types of mapping projection orthogonal and distance preserving In the first type each symbol is assigned a unique random HD vector that is kept fixed over the system s life For the second type of mapping the features value is quantized to a fixed number of levels m Each unique feature is associated with the corresponding distributed HD vector considering different mapping preserving mechanisms linear mapping approximate linear mapping and non linear approximate mapping The approach proved that the sparse and dense mapping using different mapping mechanisms showed nearly identical performance classification tasks Likewise SparseHD in explored the possibility of sparsity in hyper vectors to improve the HD computing efficiency and reduce the computations required for inference SparseHD which enforces sparsity either dimension wise or class wise takes the conventional trained HD model in non binary representation and feeds it to a Model Sparser MS MS drops S of least significant features in each class s trained HD model Works in proposed a novel encoder that quantizes a continuous range of inputs e g input range 1 1 into M levels and then maps it into HD vector using the linear mapping technique mentioned in and stored in CIM It was demonstrated that the quantization technique associated with CIM improved the encoder accuracy and its implementation was hardware friendly when the range of scalar values is known Besides the retraining step was implemented in the training stage to update the AM and improve the classification accuracy for supervised learning by testing the classifier accuracy over the training set The encoding approach used in the brain inspired classifier BRIC used locality based sparse random projection which is based on Locality Sensitive Hash algorithms The generated HD vector for each feature value is represented by the sign of dot product between the feature vector in the n dimension and the random projected vector in the D dimension using the n gram window Also a pre determined index s in the projection matrix is selected to be non zero which creates a spatial locality pattern that the hardware can take advantage of The AM is incrementally updated using the retraining approach A generic hyper vector manipulator MAN module was demonstrated in with cheaper logical operations to re materialize them later Furthermore it allows the representational space to stay in a binary format by applying back to back bundling which is fundamental for on chip learning FACD encoding module was suggested in for non binary HD representation and includes three main steps model training model refinement and inference In the refinement stage a non linear K mean clustering has been applied on the trained class HD vectors to find the best centroids representing the distribution of the values in each HD class CompHD in reduces the HD dimension intelligently in a way that does not sacrifice the system s accuracy The training HD vectors are divided into segments s each of length d D s after mapped them to Hyperspace The positional information for each segment is preserved using the Hadamard matrix and all segments are added up to form one compressed HV model for each class This method reduces the cost of cosine similarity for associative search by reducing multiplication and addition operations However using many segments would increase the data to noise ratio and affect the model accuracy SemiHD in is an application based model that trade accuracy for efficiency A fully binarized SearcHD algorithm was discussed in to reduce the number of addition operations in the training stage by generating multiple binary HD vectors for each class N SearcHD stochastically sharing the query HD vector elements with each HD class by exploiting bitwise substitution Though this algorithm increases the memory access overhead which is introduced by bitwise substitution it accumulates the training data more intelligently In the AdaptHD the author utilized the author same concept of retaining the HD model used in However a learning rate factor was introduced to speed the model convergence during the training phase Two methods are proposed for adaptive retraining iteration dependent and data dependent However the model sensitivity to a minor change in learning rate would result in a misleading accuracy especially for noisy data Likewise BinHD was proposed in which enables the HD model to be trained and tested in binary fashion using binary accumulators Recently a new technique was proposed in namely QubitHD to eliminate the overhead of floating point representation and reduce the gap between the classification accuracy of binarized and non binarized HD classifiers It exploits the principle that the information needs to be stored in a quantum bit Qubit before its measurement The algorithm is based on QuantHD in However it enables an efficient binarization of the HD model during the retraining stage using quantum measurement techniques An unsupervised learning algorithm HDCluster in was investigated for clustering input data in HD space by fully mapping and processing clusters in memory From the above abstraction it is clear that the HD general processor has two main components 1 the encoder which is the only component that needs to be programmed for a particular application However the AM design can be optimized for more efficient search implementation 2 the Data flow is one directional where all input HD vectors usually flow from item memory to the encoder and end in the associative memory One pass learning named in is also given to the same Data flow An example of all the complete HD processor framework elements is illustrated in Fig 2 for hand digit classification task General HD processor for MNIST dataset classification It consists of IM for storing seeds an encoder to map data to HD space AM for storing HD classes and similarity measurements for the inference stage Show All Data parallel architectures are the potential candidate for HD systems since parallelism exists in HD operations In both multiply and addition operations the vector s resulting element depends only on the corresponding elements of its operands The result vector element depends on the nearby operand element for the permutation operation It is clear that for any fixed width architecture less than the D 1000 dimension it will be ineffectual to process the large intermediate HD vectors stored in the costly internal memory Besides a redundant computation is needed in permutation operation due to intra word dependencies To that end the fundamental operations and inherent robustness in HDC make it a good candidate for data flow based array architecture Besides the manipulation of large patterns stored in the memory makes HDC a potential candidate for an emerging in memory computing paradigm or computational memory based on nanoscale resistive memory or memristor devices Furthermore the HD computing characteristics inimitably matches with the inherent abilities of the FPGA Therefore efficient hardware FPGA implementations were proposed to speed up the HD model during the training inference stage or improve the computational cost To date the evolution of HD computing is still in the research domain and the number of applications using HD computing for solving real time problems is limited However there is a positive tone as the number of research advancing new applications for HD is increasing In this section the main works utilizing HD for cognitive tasks grouped according to the data structure are reviewed We start with 1D data structure applications sequences or vectors such as voice recognition text classification bio signal and time series applications Then we highlight some tasks that pertain to the 2D structure data type and show how the HD computing approach provides a means of encoding visual scenes such as frame based images as well as neuromorphic sensors output Random Indexing RI is one of the oldest and well known applications used to study the relationship between the words in the language The RI method introduced the term index vector which assigns a sparse ternary vector for each document and semantic vector for every word In the original RI only two HD components have been used random representation and bundling operation In permutation operation was added to the encoding stage for semantic representation For more works on RI approaches processing and performance the reader can refer to A promising result is revealed in for identifying language in text documents The statistic of n gram letters in a sentence is encoded using HD permutation binding and bundling operations This method was tested on 21 languages using tri gram letters utilizing Project Gutenberg Hart and Wortschatz Corpora This approach accuracy surpassed the baseline ML learning methods used for the same applications In HD sparse vectors were used to store the symbol s occurrence statistics in a Willshaw associative memory using stochastic counters Such a framework is useful for on line learning applications where the system keeps learning with incoming data Work in proposed a bag of letters mapping scheme that helps identify a valid word in the dictionary given a permuted letters of the word and using Hamming distance as a similarity metric The permuted text of the Cambridge test was used as input The method showed a good potential in reconstructing the original words However depending on the letter frequency in the world a kind of bias is revealed when measuring similarity A novel method for extracting the common sub string in two strings with length and using binary HD vectors was proposed in utilizing the same n gram technique For sequence prediction work in described a prediction model based on HDC exploiting the SDM system The model used k consecutive points to predict the entire sequence The prediction s rate was limited to the memory capacity and the dimension HD vector The work in uses HD s principles for modeling the dependencies between parallel multivariate inputs and proposes an HD based predictor HDCP The approach s objective is to predict the future state of the sequence for the stream given their previous states HDCP was tested on activity recognition tasks using data from different body sensors in the Palantir Context Data Library The system performance outperformed the ML start of the art results and showed its ability to account for differing reliability of different sensor inputs However in designing HDCP a special memory architecture should be considered such as sparse distributed memory SDM and not the linear additive memory A similar approach was used in for sequence prediction But instead of dense bipolar HD vectors this model used the sparse distributed HD coding to represent time dependent structures using ternary values The approach which is called Sparse Distributed Predictor SDP was tested on real time mobile phone users data which used to predict the next application launched next music playback logs and next GPS location of the user VoiceHD was proposed in for speech recognition Their approach focused on transforming a collection of voice sets called the Isolate dataset into the frequency domain using the Mel frequency Cepstral coefficients MFCCs mechanism The proposed encoder applied for N frequency bins which are all bundled to a single hyper vector class representing the intended voice To overcome the capacity issue in HDC the design suggested retraining the AM memory by an incremental update to improve the classification accuracy VoiceHD was five times faster in execution than the conventional DNN during training and testing The fast growth of efficient electronics enables a major enhancement in wearable and portable health care systems Building on that utilizing the HD framework in the biomedical domain represents an active research area Works in developed a full set of HD templates that comprehensively encode different types of bio signals like EMG EEG and ECoG for multi class learning and classification Authors in extended the encoder in to process simultaneous analog bio signal inputs using continuous mapping method used in The proposed approach used dense bipolar HD vectors representation for dense sensors The HD classifier learns 3 times faster than the SVM method with accuracy higher than the SVM using the full training set In authors provided an efficient binarized algorithm for fast classification of human epileptic seizures using EEG signals and the region of the brain that generates them using brain inspired HD computing Though the performance of the algorithms mentioned above surpasses the traditional ML methods they are only limited to short term signal recording They require data processing which would increase hardware complexity Reference proposed a Laelaps algorithm to solve the issue for long term recording signal and operates with end to end binary operations to avoid expensive fixed or floating point arithmetic HD Computing based Multimodality Emotion Recognition HDC MER is explored in from physiological signals The real valued features of GSR ECG and EEG were extracted and mapped to dense HD binary vectors using a random non linear mapping function HDC MER classifier s accuracy surpassed the extreme gradient boosting XGB method using only 1 4 training data Heart rate response during deep breathing DB was analyzed in using the principle of HDC Both Heart rate and respiratory signals were synthesized and recorded from real health patients and modeled using Fourier series analysis to extract the desired features These features are mapped to HD vectors using non linear approximate mapping The proposed method using HDC was able to identify signals with low cardio respiratory synchronization during DB due to arrhythmias or when the evaluation of autonomic function using DB test is considered as a problem HDC is relatively new in DNA profiling and has a great potential to play a significant role in taxonomic identification In the hyper dimensional concept is applied to represent the DNA nucleotides adenine A guanine G cytosine C and thymine T which are existed in the DNA molecules in a particular order for sequences classification The HD algorithm was tested on different data set and showed an outstanding performance compared with conventional ML algorithms such as KNN and SVM Processing visual data in HD space seems to be the newest application domain and the least examined one In HDC was used to represent the structured combination of features detected by a perceptual circuitry from a visual scene The system structure is displayed through functional imitation of the learning concept occurring in honey bees The encoding module A visual question answering VQA system was proposed in in which the machine must infer an answer about the provided image The architecture consists of two parts the first part maps each image in the data set into bipolar 1000 dimension HD vector using two layers of forward fully connected FFW neural network The second part contains the seeds of HD vectors stored in IM and the related five questions described in HD format as well The system was queried on unseen images for the five questions the accuracy was limited to 60 72 for new images that were not available during the training In a holographic reduced representations method was used to convert an input image into an HD vector Interesting property in this model is its ability to perform a continuous mapping from positions to HD vectors to preserve the distance between the HD vectors The examined experiments demonstrated good performance on a simple visuospatial inference task with an accuracy of 95 as well as on a 2D navigation task Cellular Automata CA based HDC was used for an analogy reasoning in The proposed method extracts features from an image using a neural network which is then expanded into binary non random HD vectors using CA 90 rule A similar approach was used in for medical image classification The proposed classifier was assessed using IM AGE CLEF2012 data set and collections from the web Even though the classifier s performance was competing with conventional CNN and BOVW classifiers it had variations due to the random permutation of data fed into the CA grid in each iteration Recent work in bridge the gap between perception and action in robotic applications using HD binary vector HBV as a currency to produce the memories concept of previous events The work target was to find the associative velocity given time image using only memories The time image is mapped to HBV by first constructing an intensity space containing values from 0 255 encoded as a binary HD space with 8k vector length for each intensity Both time image HBV and velocity HBV vector are bounded into a single HD space to create action perception space The proposed method was tested on ego motion estimation in the autonomous vehicle using the MVSEC dataset The results showed comparable performance with the traditional CNN approach by applying less training data However the data density to be represented using HBV could limit the HD performance A concise summary of main works that exploit HDC for various applications its encoding module and main hardware implementations is highlighted in table 2 The next section will compare HDC and CNN in terms of the number of operations and accuracy for 1D and 2D applications This section studied both HD and CNN for popular data set and compared the 2 approaches in terms of accuracy computing complexity and overall performance We implemented both HDC and CNN approaches independently and then analyzed the results to guide the selection based on target needs MNIST dataset was used in this work for HDC based classification MNIST is a well known dataset with wide varieties for digit representation For the HDC model each pixel position in image is assigned a unique HD vector Therefore 784 randomly generated vectors act as seeds stored in IM and fixed over the system s life D is assumed to be equal to 10k To preserve the pixel s location in the image each pixel HD vector bounded with its intensity The gray scale images are converted to black and white BW for simple representation This means that 0 denotes a black pixel intensity and 1 denotes a white pixel intensity for a particular image This procedure is called orthogonal mapping which was inspired from and modified according to the targeted task The mapping procedure for the MNIST dataset is performed using the following steps and highlighted in Fig 2 Initialize the HD Mapper Set the dimensionality D of the HD vectors Flatten input images convert them to binary and set the features number The digit image consists of 784 pixels where each pixel represents a feature Initialize the seeds randomly For every feature i a dense binary random HD vector will be generated 748 dissimilar vectors are stored in the IM Encoding mechanism for each pixel position its corresponding HD vector is bound by the value of that pixel the random HD vector for the pixel position is multiplied by a special matrix called permutation matrix where it performs a 1 bit shift for black and 0 bit shift for white Generate the HD distributed representation of the digit by applying  Training stages combine HD vectors for all similar samples into a single pattern binary representation using the majority sum operation where refers to the majority sum operation Store the binary representation for each digit class prototype in the AM Here we study the effect of the training sets size for the MNIST dataset on classification accuracy In the first experiment we used series of randomly selected 50 images for each digit All the HD vectors of the presented image for the particular digits were combined to form a single HD representation of that digit class Thus by the end of the training phase the associative memory contains 10 HD vectors each mutually representing all digits variations We repeated the experiment for 100 500 1000 2000 3000 4400 and 5200 samples for each digit During the testing phase 1000 new images for all digits were used as input The overall accuracy was measured as the percentage of the correctly classified digits averaged over the test set size illustrates that the obtained accuracy for small sets starts small and then increases in an approximately linear fashion until it hits an upper limit slightly less than 90 accuracy After that the classification accuracy starts to decrease That is due to the majority sum operation which generates the class binary representations of the observed patterns It imposes a limit on the number of HD vectors included in the sum above which robust decoding of the individual operands becomes very difficult This explains accuracy behavior when the number of patterns increased above a specific limit Most HDC research focuses on overall accuracy to measure the system s performance We added other metrics to measure performance such as Precision and Recall for this work Precision is the classifier s ability to identify only the relevant samples in the dataset While Recall refers to the system s ability to identify all pertinent instances of the dataset The results show that both Recall 86 5 and the Precision 85 were achieved in retrieving the HDC distributed representations based method We found that the accuracy precision and recall values are in the same range Further analysis for other metrics such as F1 score 86 MCC Matthews correlation coefficient 0 85 and kappa Cohen s kappa 0 81 reveals the robustness of the classifier and that the system utilizes a balanced dataset LeNet 5 architecture It consisting of 7 layers the input layer two convolution layers followed by pooling ones And two fully connected layers Show All The average accuracy of HD classifier for MNIST dataset as a function of the training dataset size The system was examined using 50 100 500 1000 2200 4400 and 5200 samples from each class The accuracy saturate at 89 between 1000 2200 samples Show All In our HDC simulations we have assumed a 10k vector length However some studies show that using an HD vector with a dimension less than 1000 elements is adequate to represent the system This reduction in vector size would positively reflect on the execution time and the chip area implementation Moreover energy and search time are direct functions of the dimension of the HD vector and the number of classes considered in the search operation Results show that a 1k representation can achieve 90 4 compared to 97 8 classification accuracy when utilizing a 10k HD vector for language recognition dataset For the MNIST dataset employed in this work we examine the accuracy level for different HD vector sizes It is clear from Fig 5 that the HD of 6k dimension is the adequate size where the accuracy hits above 89 level Beyond that dimension the accuracy improves slightly at the cost of time and area Nevertheless a cost reduction will emerge when using the 2k HD vector where the accuracy drops only by 0 2 Hence trade off accuracy for efficiency is application dependent and would greatly impact system performance especially for IoT edge devices MNIST classification average accuracy as a function of the HD vector dimension 1k 2k 4k 6k 8k and 10k HD vector sizes were selected to examine the accuracy level Result confirm that for 8 of training size the need for 8K vector size to reach the maximum accuracy Show All HDC is a promising model for edge devices as it does not include the computationally demanding training step found in the widely used CNN A significant difference between the two computing paradigms is that HDC departs from the dimensionality reduction concept found in machine learning such as neural networks And focuses on dimensionality expansion by emulating the neuron s activity Nonetheless HDC comes with its challenges as encoding alone takes about 80 of the execution time of the training and some encoding algorithms might even increase the size of encoded data by This section compares the two computing paradigms for 1D and 2D applications in terms of accuracy computational complexity and number of utilized parameters It has been shown that HDC outperforms digital neural network DNN in 1D data set applications such as speech recognition They proposed two HDC based designs a single stage HDC architecture VoiceHD and one that is followed by a NN VoiceHD NN to increase the accuracy Their designs were compared to a 3 layer NN implemented using TinyDNN with around 3k neurons Table 3 compares the three designs in terms of full and partial training accuracies training and testing times and testing energy consumption As can be deduced HDC based designs can still maintain high classification accuracy even with 40 only of the training set Although the number of parameters in this example for HDC is more than the DNN the designs could perform a much faster runtime with lower energy consumption HDC has been widely implemented for 1D signals but the complexity increases once it is expanded to 2D In this section we want to quantify the number of operations and parameters required by each computing paradigm MNIST classification was performed using LeNet5 on the Caffe framework LeNet 5 is a simple CNN architecture that consists of 7 layers as shown in Caffe a deep learning framework has been used to simulate and quantify the LeNet for MNIST digit classification Moreover we have used Netscape a CNN analyzer tool by importing the LeNet design into it MNIST digit recognition based on LeNet5 CNN achieves 99 classification accuracy using a learning rate of 0 01 for 10 000 iterations While the HDC classification accuracy for the same MNIST data set achieved 86 classification accuracy with the full training dataset Table 4 below presents the accuracy comparison between the recall results for the HDC model and the reference CNN approach for MNIST dataset The analysis shows that the performance of the HDC is lower than the CNN approach when considering the overall accuracy level However in certain digits for the HDC model the recall is inferior to other digits For example as illustrated in the confusion matrix attached in Fig 6 the accuracy of the digit 4 5 and 8 recognition is persistently lower than other digits Thas is due to its similarity to several other characters In particular for example when recalling 4 the recall scoring is 9 19 1 and scoring 6 4 04 For recalling 5 the inference scoring 3 30 2 3 2 Confusion matrix CM displays the total number of observations in each cell the rows in the CM corresponds to the right class and the column corresponds to the predicted class The diagonal corresponds to the correctly classified classes The row at the bottom of the plot shows the percentages of all samples belonging to each class correctly and incorrectly classified These metrics are often called the recall The column on the far right of the plot shows the percentages of all the samples predicted to belong to each class correctly and incorrectly classified These metrics are often called the precision Show All It is essential to highlight that for HD architecture the Hamming distance acts as a quantitative metric of the similarity via direct comparison of distributed representations without decoding those representations Results in show that the larger number of common elements lead to a more considerable similarity between resulting vectors In MNIST HD representation the high resemblance in some digits for example 4 and 9 makes the number of overlapped elements between their HD vectors relatively high making the difference between analyzable patterns sometimes indistinguishable and prone to error That imposes a limit on the number of overlapped elements between two patterns that can be robustly detected To reduce the error one needs to explore another method for mapping the HD space inputs such as using linear mapping instead of orthogonal mapping Another suggestion is to extract the image s main features using CNN techniques and then map those features to HD space Also utilizing some retraining techniques would enhance the system accuracy and reduce the error To compute the No of parameters required by the HDC based design during the testing phase we assume a An array of size is generated for random seeds and stored in the IM b Encoded classes stored in the associative memory have a size of 10k For computation operations we assume a The No of shifting is assuming that 50 of the seeds array needs to be shifted HD vector level operation b Remove the shifting effect by performing the shifting operation again c Column wise addition operation is 784 1 d XOR operation is between the query and the encoded patterns in the AM e Addition operation following the XOR is 10k 1 f Comparison operations are 9 As can be deduced from Table 4 CNN is still superior in accuracy utilizing full training set and with D 10k for the HDC architecture To further study the impact of the size of the training set on the CNN accuracy the LeNet trained with 10k and 5k samples The reported accuracy was 98 29 compared to 89 5 in HDC and 97 70 86 respectively It is apparent that CNN yet performs better for 2D applications as the accuracy drops slightly by 1 4 which is consistent with the findings in Nonetheless in the HDC design no MAC operations are required positively reflecting on area energy and execution time AI at the edge is important to enable intelligent machines Efficient hardware architectures with low computing complexity and memory requirements to allow low power and small form factor is of paramount importance HDC is still considered a new paradigm and faces challenges requiring further analysis One of these challenges that need to address is the sensitivity of the HDC model which depends on several factors such as the dimension of the vector used to represent the input data the mapping form Binary Ternary integer to high dimension space and the majority sum bundling sensitivity Moreover once the model is implemented into hardware there will be an unavoidable source of variations like devices and temperature which would lead to imprecision Future works direction may include but not limited to 1 exploit HDC intrinsic characteristics for more classification cognitive tasks in different domains like security image processing and real time applications 2 focus on developing an efficient encoding algorithm that handles HDC capacity limitation and would improve data representation for 2D applications 3 develop more hardware friendly metrics for similarity measurement that would enhance system accuracy 4 design a unified HD processor that addresses diverse data types and can trade offs accuracy to efficiency based on the application requirements 5 investigate the dimension of HD vector that store and manipulate different data representations to reduce the memory footprint and enhance system efficiency 6 study various methods for integrating the DL ML techniques with HDC and analyzing system performance effects In addition two computing paradigms are studied and compared Firstly CNN focuses on building decisions based on small features of the target input The second one is the HDC which uses the HD vector to encode all possible input views to match the holographic distributed representation Selecting either one has its implication on accuracy computing complexity and power consumption This paper showed that for 1D data HDC is more efficient and can provide superior overall performance While in 2D applications CNN still achieves higher classification accuracy at the expense of more computations Thus 2D HDC reduced the number of required MAC operations by 140 M which has a direct impact on area and power with 10 accuracy loss Besides to architecture further optimization of the implementation CNN and HDC can benefit from many new approaches such as in memory computing data reuse efficient data flow etc The following Github link can be used for Matlab code https github com emfhasan HDCvsCNN StudyCase Two computing approaches HD and CNN for popular MNIST dataset are studied and compared in terms of accuracy computing complexity and overall performance"}
